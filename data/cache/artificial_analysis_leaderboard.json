[
  {
    "Model": "Claude Opus 4.6 (Adaptive)",
    "ContextWindow": "200k",
    "Creator": "Anthropic",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "53",
    "ArtificialAnalysisOmniscience Index": "11",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "55%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "46%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "92%",
    "AA-LCR(LongContext Reasoning)": "71%",
    "AA-OmniscienceAccuracy(Knowledge)": "44%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "40%",
    "Humanity's LastExam(Reasoning & Knowledge)": "37%",
    "GPQA Diamond(ScientificReasoning)": "90%",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "52%",
    "IFBench(InstructionFollowing)": "53%",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "13%",
    "MMMU Pro(VisualReasoning)": "75%",
    "BlendedUSD/1M Tokens": "$10.00",
    "InputPriceUSD/1M Tokens": "$5.00",
    "OutputPriceUSD/1M Tokens": "$25.00",
    "MedianTokens/s": "70",
    "P5Tokens/s": "57",
    "P25Tokens/s": "61",
    "P75Tokens/s": "82",
    "P95Tokens/s": "90",
    "LatencyFirst Answer Chunk (s)": "1.74",
    "FirstAnswerToken (s)": "30.48",
    "P5First Chunk (s)": "1.06",
    "P25First Chunk (s)": "1.55",
    "P75First Chunk (s)": "1.88",
    "P95First Chunk (s)": "2.20",
    "TotalResponse (s)": "37.66",
    "ReasoningTime (s)": "28.74",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "GPT-5.2 (xhigh)",
    "ContextWindow": "400k",
    "Creator": "OpenAI",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "51",
    "ArtificialAnalysisOmniscience Index": "\u22124",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "48%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "47%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "85%",
    "AA-LCR(LongContext Reasoning)": "73%",
    "AA-OmniscienceAccuracy(Knowledge)": "41%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "22%",
    "Humanity's LastExam(Reasoning & Knowledge)": "35%",
    "GPQA Diamond(ScientificReasoning)": "90%",
    "LiveCodeBench(Coding)": "89%",
    "SciCode(Coding)": "52%",
    "IFBench(InstructionFollowing)": "75%",
    "AIME 2025(CompetitionMath)": "99%",
    "CritPt(PhysicsReasoning)": "12%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$4.81",
    "InputPriceUSD/1M Tokens": "$1.75",
    "OutputPriceUSD/1M Tokens": "$14.00",
    "MedianTokens/s": "97",
    "P5Tokens/s": "68",
    "P25Tokens/s": "84",
    "P75Tokens/s": "102",
    "P95Tokens/s": "104",
    "LatencyFirst Answer Chunk (s)": "43.29",
    "FirstAnswerToken (s)": "43.29",
    "P5First Chunk (s)": "0.61",
    "P25First Chunk (s)": "35.91",
    "P75First Chunk (s)": "70.77",
    "P95First Chunk (s)": "82.59",
    "TotalResponse (s)": "48.44",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Claude Opus 4.5",
    "ContextWindow": "200k",
    "Creator": "Anthropic",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "50",
    "ArtificialAnalysisOmniscience Index": "10",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "45%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "47%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "90%",
    "AA-LCR(LongContext Reasoning)": "74%",
    "AA-OmniscienceAccuracy(Knowledge)": "43%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "42%",
    "Humanity's LastExam(Reasoning & Knowledge)": "28%",
    "GPQA Diamond(ScientificReasoning)": "87%",
    "LiveCodeBench(Coding)": "87%",
    "SciCode(Coding)": "50%",
    "IFBench(InstructionFollowing)": "58%",
    "AIME 2025(CompetitionMath)": "91%",
    "CritPt(PhysicsReasoning)": "5%",
    "MMMU Pro(VisualReasoning)": "74%",
    "BlendedUSD/1M Tokens": "$10.00",
    "InputPriceUSD/1M Tokens": "$5.00",
    "OutputPriceUSD/1M Tokens": "$25.00",
    "MedianTokens/s": "85",
    "P5Tokens/s": "60",
    "P25Tokens/s": "77",
    "P75Tokens/s": "93",
    "P95Tokens/s": "99",
    "LatencyFirst Answer Chunk (s)": "1.52",
    "FirstAnswerToken (s)": "25.06",
    "P5First Chunk (s)": "1.36",
    "P25First Chunk (s)": "1.42",
    "P75First Chunk (s)": "1.86",
    "P95First Chunk (s)": "2.12",
    "TotalResponse (s)": "30.94",
    "ReasoningTime (s)": "23.54",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "GLM-5",
    "ContextWindow": "200k",
    "Creator": "Z AI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "50",
    "ArtificialAnalysisOmniscience Index": "\u22121",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "46%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "43%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "98%",
    "AA-LCR(LongContext Reasoning)": "63%",
    "AA-OmniscienceAccuracy(Knowledge)": "25%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "66%",
    "Humanity's LastExam(Reasoning & Knowledge)": "27%",
    "GPQA Diamond(ScientificReasoning)": "82%",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "46%",
    "IFBench(InstructionFollowing)": "72%",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "2%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$1.55",
    "InputPriceUSD/1M Tokens": "$1.00",
    "OutputPriceUSD/1M Tokens": "$3.20",
    "MedianTokens/s": "78",
    "P5Tokens/s": "36",
    "P25Tokens/s": "47",
    "P75Tokens/s": "118",
    "P95Tokens/s": "270",
    "LatencyFirst Answer Chunk (s)": "1.46",
    "FirstAnswerToken (s)": "26.94",
    "P5First Chunk (s)": "0.26",
    "P25First Chunk (s)": "1.06",
    "P75First Chunk (s)": "2.27",
    "P95First Chunk (s)": "3.92",
    "TotalResponse (s)": "33.31",
    "ReasoningTime (s)": "25.48",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "GPT-5.2 Codex (xhigh)",
    "ContextWindow": "400k",
    "Creator": "OpenAI",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "49",
    "ArtificialAnalysisOmniscience Index": "\u22126",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "40%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "37%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "92%",
    "AA-LCR(LongContext Reasoning)": "76%",
    "AA-OmniscienceAccuracy(Knowledge)": "38%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "29%",
    "Humanity's LastExam(Reasoning & Knowledge)": "34%",
    "GPQA Diamond(ScientificReasoning)": "90%",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "55%",
    "IFBench(InstructionFollowing)": "78%",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "9%",
    "MMMU Pro(VisualReasoning)": "76%",
    "BlendedUSD/1M Tokens": "$4.81",
    "InputPriceUSD/1M Tokens": "$1.75",
    "OutputPriceUSD/1M Tokens": "$14.00",
    "MedianTokens/s": "91",
    "P5Tokens/s": "74",
    "P25Tokens/s": "80",
    "P75Tokens/s": "102",
    "P95Tokens/s": "108",
    "LatencyFirst Answer Chunk (s)": "23.63",
    "FirstAnswerToken (s)": "23.63",
    "P5First Chunk (s)": "0.86",
    "P25First Chunk (s)": "1.28",
    "P75First Chunk (s)": "28.24",
    "P95First Chunk (s)": "48.34",
    "TotalResponse (s)": "29.12",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Gemini 3 Pro Preview (high)",
    "ContextWindow": "1m",
    "Creator": "Google",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "48",
    "ArtificialAnalysisOmniscience Index": "13",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "35%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "42%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "87%",
    "AA-LCR(LongContext Reasoning)": "71%",
    "AA-OmniscienceAccuracy(Knowledge)": "54%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "12%",
    "Humanity's LastExam(Reasoning & Knowledge)": "37%",
    "GPQA Diamond(ScientificReasoning)": "91%",
    "LiveCodeBench(Coding)": "92%",
    "SciCode(Coding)": "56%",
    "IFBench(InstructionFollowing)": "70%",
    "AIME 2025(CompetitionMath)": "96%",
    "CritPt(PhysicsReasoning)": "9%",
    "MMMU Pro(VisualReasoning)": "80%",
    "BlendedUSD/1M Tokens": "$4.50",
    "InputPriceUSD/1M Tokens": "$2.00",
    "OutputPriceUSD/1M Tokens": "$12.00",
    "MedianTokens/s": "132",
    "P5Tokens/s": "121",
    "P25Tokens/s": "124",
    "P75Tokens/s": "134",
    "P95Tokens/s": "142",
    "LatencyFirst Answer Chunk (s)": "30.32",
    "FirstAnswerToken (s)": "30.32",
    "P5First Chunk (s)": "25.35",
    "P25First Chunk (s)": "28.23",
    "P75First Chunk (s)": "32.53",
    "P95First Chunk (s)": "37.15",
    "TotalResponse (s)": "34.10",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Kimi K2.5",
    "ContextWindow": "256k",
    "Creator": "Kimi",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "47",
    "ArtificialAnalysisOmniscience Index": "\u221211",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "39%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "35%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "96%",
    "AA-LCR(LongContext Reasoning)": "65%",
    "AA-OmniscienceAccuracy(Knowledge)": "33%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "36%",
    "Humanity's LastExam(Reasoning & Knowledge)": "29%",
    "GPQA Diamond(ScientificReasoning)": "88%",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "49%",
    "IFBench(InstructionFollowing)": "70%",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "3%",
    "MMMU Pro(VisualReasoning)": "75%",
    "BlendedUSD/1M Tokens": "$1.20",
    "InputPriceUSD/1M Tokens": "$0.60",
    "OutputPriceUSD/1M Tokens": "$3.00",
    "MedianTokens/s": "47",
    "P5Tokens/s": "39",
    "P25Tokens/s": "43",
    "P75Tokens/s": "48",
    "P95Tokens/s": "50",
    "LatencyFirst Answer Chunk (s)": "1.20",
    "FirstAnswerToken (s)": "44.19",
    "P5First Chunk (s)": "1.10",
    "P25First Chunk (s)": "1.14",
    "P75First Chunk (s)": "1.31",
    "P95First Chunk (s)": "1.53",
    "TotalResponse (s)": "54.94",
    "ReasoningTime (s)": "43.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "GPT-5.2 (medium)",
    "ContextWindow": "400k",
    "Creator": "OpenAI",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "47",
    "ArtificialAnalysisOmniscience Index": "\u22123",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "46%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "43%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "74%",
    "AA-LCR(LongContext Reasoning)": "63%",
    "AA-OmniscienceAccuracy(Knowledge)": "36%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "41%",
    "Humanity's LastExam(Reasoning & Knowledge)": "25%",
    "GPQA Diamond(ScientificReasoning)": "86%",
    "LiveCodeBench(Coding)": "89%",
    "SciCode(Coding)": "46%",
    "IFBench(InstructionFollowing)": "65%",
    "AIME 2025(CompetitionMath)": "97%",
    "CritPt(PhysicsReasoning)": "8%",
    "MMMU Pro(VisualReasoning)": "75%",
    "BlendedUSD/1M Tokens": "$4.81",
    "InputPriceUSD/1M Tokens": "$1.75",
    "OutputPriceUSD/1M Tokens": "$14.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Gemini 3 Flash",
    "ContextWindow": "1m",
    "Creator": "Google",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "46",
    "ArtificialAnalysisOmniscience Index": "8",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "35%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "39%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "80%",
    "AA-LCR(LongContext Reasoning)": "66%",
    "AA-OmniscienceAccuracy(Knowledge)": "52%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "9%",
    "Humanity's LastExam(Reasoning & Knowledge)": "35%",
    "GPQA Diamond(ScientificReasoning)": "90%",
    "LiveCodeBench(Coding)": "91%",
    "SciCode(Coding)": "51%",
    "IFBench(InstructionFollowing)": "78%",
    "AIME 2025(CompetitionMath)": "97%",
    "CritPt(PhysicsReasoning)": "9%",
    "MMMU Pro(VisualReasoning)": "80%",
    "BlendedUSD/1M Tokens": "$1.13",
    "InputPriceUSD/1M Tokens": "$0.50",
    "OutputPriceUSD/1M Tokens": "$3.00",
    "MedianTokens/s": "207",
    "P5Tokens/s": "197",
    "P25Tokens/s": "200",
    "P75Tokens/s": "217",
    "P95Tokens/s": "223",
    "LatencyFirst Answer Chunk (s)": "12.05",
    "FirstAnswerToken (s)": "12.05",
    "P5First Chunk (s)": "10.02",
    "P25First Chunk (s)": "11.18",
    "P75First Chunk (s)": "13.40",
    "P95First Chunk (s)": "16.14",
    "TotalResponse (s)": "14.46",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Claude Opus 4.6",
    "ContextWindow": "200k",
    "Creator": "Anthropic",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "46",
    "ArtificialAnalysisOmniscience Index": "1",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "54%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "49%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "85%",
    "AA-LCR(LongContext Reasoning)": "58%",
    "AA-OmniscienceAccuracy(Knowledge)": "44%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "24%",
    "Humanity's LastExam(Reasoning & Knowledge)": "19%",
    "GPQA Diamond(ScientificReasoning)": "84%",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "46%",
    "IFBench(InstructionFollowing)": "45%",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "3%",
    "MMMU Pro(VisualReasoning)": "73%",
    "BlendedUSD/1M Tokens": "$10.00",
    "InputPriceUSD/1M Tokens": "$5.00",
    "OutputPriceUSD/1M Tokens": "$25.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Claude Opus 4.5",
    "ContextWindow": "200k",
    "Creator": "Anthropic",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "43",
    "ArtificialAnalysisOmniscience Index": "\u22126",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "46%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "41%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "86%",
    "AA-LCR(LongContext Reasoning)": "65%",
    "AA-OmniscienceAccuracy(Knowledge)": "39%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "26%",
    "Humanity's LastExam(Reasoning & Knowledge)": "13%",
    "GPQA Diamond(ScientificReasoning)": "81%",
    "LiveCodeBench(Coding)": "74%",
    "SciCode(Coding)": "47%",
    "IFBench(InstructionFollowing)": "43%",
    "AIME 2025(CompetitionMath)": "63%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "71%",
    "BlendedUSD/1M Tokens": "$10.00",
    "InputPriceUSD/1M Tokens": "$5.00",
    "OutputPriceUSD/1M Tokens": "$25.00",
    "MedianTokens/s": "74",
    "P5Tokens/s": "59",
    "P25Tokens/s": "70",
    "P75Tokens/s": "76",
    "P95Tokens/s": "80",
    "LatencyFirst Answer Chunk (s)": "1.52",
    "FirstAnswerToken (s)": "1.52",
    "P5First Chunk (s)": "1.41",
    "P25First Chunk (s)": "1.43",
    "P75First Chunk (s)": "1.70",
    "P95First Chunk (s)": "2.17",
    "TotalResponse (s)": "8.30",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Claude 4.5 Sonnet",
    "ContextWindow": "1m",
    "Creator": "Anthropic",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "43",
    "ArtificialAnalysisOmniscience Index": "\u22122",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "39%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "36%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "78%",
    "AA-LCR(LongContext Reasoning)": "66%",
    "AA-OmniscienceAccuracy(Knowledge)": "31%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "52%",
    "Humanity's LastExam(Reasoning & Knowledge)": "17%",
    "GPQA Diamond(ScientificReasoning)": "83%",
    "LiveCodeBench(Coding)": "71%",
    "SciCode(Coding)": "45%",
    "IFBench(InstructionFollowing)": "57%",
    "AIME 2025(CompetitionMath)": "88%",
    "CritPt(PhysicsReasoning)": "1%",
    "MMMU Pro(VisualReasoning)": "69%",
    "BlendedUSD/1M Tokens": "$6.00",
    "InputPriceUSD/1M Tokens": "$3.00",
    "OutputPriceUSD/1M Tokens": "$15.00",
    "MedianTokens/s": "80",
    "P5Tokens/s": "67",
    "P25Tokens/s": "77",
    "P75Tokens/s": "89",
    "P95Tokens/s": "97",
    "LatencyFirst Answer Chunk (s)": "1.24",
    "FirstAnswerToken (s)": "26.38",
    "P5First Chunk (s)": "1.00",
    "P25First Chunk (s)": "1.15",
    "P75First Chunk (s)": "1.61",
    "P95First Chunk (s)": "2.64",
    "TotalResponse (s)": "32.67",
    "ReasoningTime (s)": "25.14",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "GPT-5.1 Codex (high)",
    "ContextWindow": "400k",
    "Creator": "OpenAI",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "42",
    "ArtificialAnalysisOmniscience Index": "\u22127",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "35%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "35%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "83%",
    "AA-LCR(LongContext Reasoning)": "67%",
    "AA-OmniscienceAccuracy(Knowledge)": "23%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "27%",
    "Humanity's LastExam(Reasoning & Knowledge)": "23%",
    "GPQA Diamond(ScientificReasoning)": "86%",
    "LiveCodeBench(Coding)": "85%",
    "SciCode(Coding)": "40%",
    "IFBench(InstructionFollowing)": "70%",
    "AIME 2025(CompetitionMath)": "96%",
    "CritPt(PhysicsReasoning)": "6%",
    "MMMU Pro(VisualReasoning)": "73%",
    "BlendedUSD/1M Tokens": "$3.44",
    "InputPriceUSD/1M Tokens": "$1.25",
    "OutputPriceUSD/1M Tokens": "$10.00",
    "MedianTokens/s": "191",
    "P5Tokens/s": "107",
    "P25Tokens/s": "158",
    "P75Tokens/s": "238",
    "P95Tokens/s": "277",
    "LatencyFirst Answer Chunk (s)": "16.51",
    "FirstAnswerToken (s)": "16.51",
    "P5First Chunk (s)": "7.97",
    "P25First Chunk (s)": "10.38",
    "P75First Chunk (s)": "24.39",
    "P95First Chunk (s)": "38.45",
    "TotalResponse (s)": "19.13",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "GLM-4.7",
    "ContextWindow": "200k",
    "Creator": "Z AI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "42",
    "ArtificialAnalysisOmniscience Index": "\u221236",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "35%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "32%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "96%",
    "AA-LCR(LongContext Reasoning)": "64%",
    "AA-OmniscienceAccuracy(Knowledge)": "28%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "10%",
    "Humanity's LastExam(Reasoning & Knowledge)": "25%",
    "GPQA Diamond(ScientificReasoning)": "86%",
    "LiveCodeBench(Coding)": "89%",
    "SciCode(Coding)": "45%",
    "IFBench(InstructionFollowing)": "68%",
    "AIME 2025(CompetitionMath)": "95%",
    "CritPt(PhysicsReasoning)": "2%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.94",
    "InputPriceUSD/1M Tokens": "$0.55",
    "OutputPriceUSD/1M Tokens": "$2.15",
    "MedianTokens/s": "148",
    "P5Tokens/s": "43",
    "P25Tokens/s": "82",
    "P75Tokens/s": "311",
    "P95Tokens/s": "1,185",
    "LatencyFirst Answer Chunk (s)": "0.65",
    "FirstAnswerToken (s)": "14.14",
    "P5First Chunk (s)": "0.21",
    "P25First Chunk (s)": "0.39",
    "P75First Chunk (s)": "1.03",
    "P95First Chunk (s)": "16.53",
    "TotalResponse (s)": "17.51",
    "ReasoningTime (s)": "13.49",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "MiniMax-M2.5",
    "ContextWindow": "205k",
    "Creator": "MiniMax",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "42",
    "ArtificialAnalysisOmniscience Index": "\u221241",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "36%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "35%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "95%",
    "AA-LCR(LongContext Reasoning)": "66%",
    "AA-OmniscienceAccuracy(Knowledge)": "25%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "12%",
    "Humanity's LastExam(Reasoning & Knowledge)": "19%",
    "GPQA Diamond(ScientificReasoning)": "85%",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "43%",
    "IFBench(InstructionFollowing)": "72%",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "1%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.53",
    "InputPriceUSD/1M Tokens": "$0.30",
    "OutputPriceUSD/1M Tokens": "$1.20",
    "MedianTokens/s": "57",
    "P5Tokens/s": "38",
    "P25Tokens/s": "50",
    "P75Tokens/s": "66",
    "P95Tokens/s": "73",
    "LatencyFirst Answer Chunk (s)": "2.17",
    "FirstAnswerToken (s)": "37.04",
    "P5First Chunk (s)": "1.26",
    "P25First Chunk (s)": "1.77",
    "P75First Chunk (s)": "3.63",
    "P95First Chunk (s)": "8.13",
    "TotalResponse (s)": "45.76",
    "ReasoningTime (s)": "34.88",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "DeepSeek V3.2",
    "ContextWindow": "128k",
    "Creator": "DeepSeek",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "42",
    "ArtificialAnalysisOmniscience Index": "\u221223",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "35%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "36%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "91%",
    "AA-LCR(LongContext Reasoning)": "65%",
    "AA-OmniscienceAccuracy(Knowledge)": "32%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "18%",
    "Humanity's LastExam(Reasoning & Knowledge)": "22%",
    "GPQA Diamond(ScientificReasoning)": "84%",
    "LiveCodeBench(Coding)": "86%",
    "SciCode(Coding)": "39%",
    "IFBench(InstructionFollowing)": "61%",
    "AIME 2025(CompetitionMath)": "92%",
    "CritPt(PhysicsReasoning)": "3%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.32",
    "InputPriceUSD/1M Tokens": "$0.28",
    "OutputPriceUSD/1M Tokens": "$0.42",
    "MedianTokens/s": "49",
    "P5Tokens/s": "46",
    "P25Tokens/s": "47",
    "P75Tokens/s": "52",
    "P95Tokens/s": "55",
    "LatencyFirst Answer Chunk (s)": "1.17",
    "FirstAnswerToken (s)": "41.86",
    "P5First Chunk (s)": "0.90",
    "P25First Chunk (s)": "1.01",
    "P75First Chunk (s)": "1.36",
    "P95First Chunk (s)": "1.61",
    "TotalResponse (s)": "52.04",
    "ReasoningTime (s)": "40.69",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Grok 4",
    "ContextWindow": "256k",
    "Creator": "xAI",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "41",
    "ArtificialAnalysisOmniscience Index": "1",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "25%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "38%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "75%",
    "AA-LCR(LongContext Reasoning)": "68%",
    "AA-OmniscienceAccuracy(Knowledge)": "40%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "36%",
    "Humanity's LastExam(Reasoning & Knowledge)": "24%",
    "GPQA Diamond(ScientificReasoning)": "88%",
    "LiveCodeBench(Coding)": "82%",
    "SciCode(Coding)": "46%",
    "IFBench(InstructionFollowing)": "54%",
    "AIME 2025(CompetitionMath)": "93%",
    "CritPt(PhysicsReasoning)": "2%",
    "MMMU Pro(VisualReasoning)": "69%",
    "BlendedUSD/1M Tokens": "$6.00",
    "InputPriceUSD/1M Tokens": "$3.00",
    "OutputPriceUSD/1M Tokens": "$15.00",
    "MedianTokens/s": "39",
    "P5Tokens/s": "32",
    "P25Tokens/s": "35",
    "P75Tokens/s": "43",
    "P95Tokens/s": "46",
    "LatencyFirst Answer Chunk (s)": "10.20",
    "FirstAnswerToken (s)": "10.20",
    "P5First Chunk (s)": "5.53",
    "P25First Chunk (s)": "7.21",
    "P75First Chunk (s)": "16.97",
    "P95First Chunk (s)": "22.12",
    "TotalResponse (s)": "22.86",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "MiMo-V2-Flash (Feb 2026)",
    "ContextWindow": "256k",
    "Creator": "Xiaomi",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "41",
    "ArtificialAnalysisOmniscience Index": "\u221220",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "27%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "31%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "93%",
    "AA-LCR(LongContext Reasoning)": "64%",
    "AA-OmniscienceAccuracy(Knowledge)": "19%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "52%",
    "Humanity's LastExam(Reasoning & Knowledge)": "20%",
    "GPQA Diamond(ScientificReasoning)": "84%",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "38%",
    "IFBench(InstructionFollowing)": "72%",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "3%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.15",
    "InputPriceUSD/1M Tokens": "$0.10",
    "OutputPriceUSD/1M Tokens": "$0.30",
    "MedianTokens/s": "166",
    "P5Tokens/s": "144",
    "P25Tokens/s": "156",
    "P75Tokens/s": "174",
    "P95Tokens/s": "190",
    "LatencyFirst Answer Chunk (s)": "1.34",
    "FirstAnswerToken (s)": "13.42",
    "P5First Chunk (s)": "1.11",
    "P25First Chunk (s)": "1.23",
    "P75First Chunk (s)": "1.40",
    "P95First Chunk (s)": "1.73",
    "TotalResponse (s)": "16.44",
    "ReasoningTime (s)": "12.08",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Gemini 3 Pro Preview (low)",
    "ContextWindow": "1m",
    "Creator": "Google",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "41",
    "ArtificialAnalysisOmniscience Index": "\u22121",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "34%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "34%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "68%",
    "AA-LCR(LongContext Reasoning)": "67%",
    "AA-OmniscienceAccuracy(Knowledge)": "46%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "13%",
    "Humanity's LastExam(Reasoning & Knowledge)": "28%",
    "GPQA Diamond(ScientificReasoning)": "89%",
    "LiveCodeBench(Coding)": "86%",
    "SciCode(Coding)": "50%",
    "IFBench(InstructionFollowing)": "50%",
    "AIME 2025(CompetitionMath)": "87%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$4.50",
    "InputPriceUSD/1M Tokens": "$2.00",
    "OutputPriceUSD/1M Tokens": "$12.00",
    "MedianTokens/s": "132",
    "P5Tokens/s": "120",
    "P25Tokens/s": "129",
    "P75Tokens/s": "137",
    "P95Tokens/s": "146",
    "LatencyFirst Answer Chunk (s)": "3.75",
    "FirstAnswerToken (s)": "3.75",
    "P5First Chunk (s)": "3.42",
    "P25First Chunk (s)": "3.67",
    "P75First Chunk (s)": "3.89",
    "P95First Chunk (s)": "4.61",
    "TotalResponse (s)": "7.53",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "GPT-5 mini (high)",
    "ContextWindow": "400k",
    "Creator": "OpenAI",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "41",
    "ArtificialAnalysisOmniscience Index": "\u221220",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "35%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "33%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "68%",
    "AA-LCR(LongContext Reasoning)": "68%",
    "AA-OmniscienceAccuracy(Knowledge)": "23%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "45%",
    "Humanity's LastExam(Reasoning & Knowledge)": "20%",
    "GPQA Diamond(ScientificReasoning)": "83%",
    "LiveCodeBench(Coding)": "84%",
    "SciCode(Coding)": "39%",
    "IFBench(InstructionFollowing)": "75%",
    "AIME 2025(CompetitionMath)": "91%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "70%",
    "BlendedUSD/1M Tokens": "$0.69",
    "InputPriceUSD/1M Tokens": "$0.25",
    "OutputPriceUSD/1M Tokens": "$2.00",
    "MedianTokens/s": "131",
    "P5Tokens/s": "77",
    "P25Tokens/s": "123",
    "P75Tokens/s": "142",
    "P95Tokens/s": "150",
    "LatencyFirst Answer Chunk (s)": "65.66",
    "FirstAnswerToken (s)": "65.66",
    "P5First Chunk (s)": "40.65",
    "P25First Chunk (s)": "59.24",
    "P75First Chunk (s)": "79.64",
    "P95First Chunk (s)": "89.43",
    "TotalResponse (s)": "69.48",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "GLM-5",
    "ContextWindow": "200k",
    "Creator": "Z AI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "40",
    "ArtificialAnalysisOmniscience Index": "\u221213",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "42%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "39%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "97%",
    "AA-LCR(LongContext Reasoning)": "37%",
    "AA-OmniscienceAccuracy(Knowledge)": "22%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "55%",
    "Humanity's LastExam(Reasoning & Knowledge)": "7%",
    "GPQA Diamond(ScientificReasoning)": "67%",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "38%",
    "IFBench(InstructionFollowing)": "55%",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$1.55",
    "InputPriceUSD/1M Tokens": "$1.00",
    "OutputPriceUSD/1M Tokens": "$3.20",
    "MedianTokens/s": "36",
    "P5Tokens/s": "30",
    "P25Tokens/s": "34",
    "P75Tokens/s": "37",
    "P95Tokens/s": "40",
    "LatencyFirst Answer Chunk (s)": "1.87",
    "FirstAnswerToken (s)": "1.87",
    "P5First Chunk (s)": "0.95",
    "P25First Chunk (s)": "1.25",
    "P75First Chunk (s)": "3.17",
    "P95First Chunk (s)": "4.58",
    "TotalResponse (s)": "15.85",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 Max Thinking",
    "ContextWindow": "256k",
    "Creator": "Alibaba",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "40",
    "ArtificialAnalysisOmniscience Index": "\u221238",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "33%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "24%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "84%",
    "AA-LCR(LongContext Reasoning)": "66%",
    "AA-OmniscienceAccuracy(Knowledge)": "29%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "7%",
    "Humanity's LastExam(Reasoning & Knowledge)": "26%",
    "GPQA Diamond(ScientificReasoning)": "86%",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "43%",
    "IFBench(InstructionFollowing)": "71%",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "2%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$2.40",
    "InputPriceUSD/1M Tokens": "$1.20",
    "OutputPriceUSD/1M Tokens": "$6.00",
    "MedianTokens/s": "36",
    "P5Tokens/s": "31",
    "P25Tokens/s": "34",
    "P75Tokens/s": "38",
    "P95Tokens/s": "42",
    "LatencyFirst Answer Chunk (s)": "1.74",
    "FirstAnswerToken (s)": "57.45",
    "P5First Chunk (s)": "1.60",
    "P25First Chunk (s)": "1.62",
    "P75First Chunk (s)": "1.88",
    "P95First Chunk (s)": "16.84",
    "TotalResponse (s)": "71.37",
    "ReasoningTime (s)": "55.71",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "MiniMax-M2.1",
    "ContextWindow": "205k",
    "Creator": "MiniMax",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "40",
    "ArtificialAnalysisOmniscience Index": "\u221230",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "29%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "29%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "85%",
    "AA-LCR(LongContext Reasoning)": "59%",
    "AA-OmniscienceAccuracy(Knowledge)": "22%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "33%",
    "Humanity's LastExam(Reasoning & Knowledge)": "22%",
    "GPQA Diamond(ScientificReasoning)": "83%",
    "LiveCodeBench(Coding)": "81%",
    "SciCode(Coding)": "41%",
    "IFBench(InstructionFollowing)": "70%",
    "AIME 2025(CompetitionMath)": "83%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.53",
    "InputPriceUSD/1M Tokens": "$0.30",
    "OutputPriceUSD/1M Tokens": "$1.20",
    "MedianTokens/s": "54",
    "P5Tokens/s": "34",
    "P25Tokens/s": "43",
    "P75Tokens/s": "74",
    "P95Tokens/s": "90",
    "LatencyFirst Answer Chunk (s)": "1.29",
    "FirstAnswerToken (s)": "38.41",
    "P5First Chunk (s)": "0.94",
    "P25First Chunk (s)": "1.17",
    "P75First Chunk (s)": "1.46",
    "P95First Chunk (s)": "1.74",
    "TotalResponse (s)": "47.69",
    "ReasoningTime (s)": "37.12",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "MiMo-V2-Flash",
    "ContextWindow": "256k",
    "Creator": "Xiaomi",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "39",
    "ArtificialAnalysisOmniscience Index": "\u221242",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "31%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "28%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "95%",
    "AA-LCR(LongContext Reasoning)": "63%",
    "AA-OmniscienceAccuracy(Knowledge)": "26%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "7%",
    "Humanity's LastExam(Reasoning & Knowledge)": "21%",
    "GPQA Diamond(ScientificReasoning)": "85%",
    "LiveCodeBench(Coding)": "87%",
    "SciCode(Coding)": "39%",
    "IFBench(InstructionFollowing)": "64%",
    "AIME 2025(CompetitionMath)": "96%",
    "CritPt(PhysicsReasoning)": "4%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.15",
    "InputPriceUSD/1M Tokens": "$0.10",
    "OutputPriceUSD/1M Tokens": "$0.30",
    "MedianTokens/s": "171",
    "P5Tokens/s": "144",
    "P25Tokens/s": "152",
    "P75Tokens/s": "182",
    "P95Tokens/s": "198",
    "LatencyFirst Answer Chunk (s)": "1.28",
    "FirstAnswerToken (s)": "13.01",
    "P5First Chunk (s)": "1.02",
    "P25First Chunk (s)": "1.10",
    "P75First Chunk (s)": "1.53",
    "P95First Chunk (s)": "2.06",
    "TotalResponse (s)": "15.94",
    "ReasoningTime (s)": "11.73",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "GPT-5.1 Codex mini (high)",
    "ContextWindow": "400k",
    "Creator": "OpenAI",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "39",
    "ArtificialAnalysisOmniscience Index": "\u221218",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "27%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "33%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "63%",
    "AA-LCR(LongContext Reasoning)": "63%",
    "AA-OmniscienceAccuracy(Knowledge)": "22%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "49%",
    "Humanity's LastExam(Reasoning & Knowledge)": "17%",
    "GPQA Diamond(ScientificReasoning)": "81%",
    "LiveCodeBench(Coding)": "84%",
    "SciCode(Coding)": "43%",
    "IFBench(InstructionFollowing)": "68%",
    "AIME 2025(CompetitionMath)": "92%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "69%",
    "BlendedUSD/1M Tokens": "$0.69",
    "InputPriceUSD/1M Tokens": "$0.25",
    "OutputPriceUSD/1M Tokens": "$2.00",
    "MedianTokens/s": "153",
    "P5Tokens/s": "123",
    "P25Tokens/s": "141",
    "P75Tokens/s": "184",
    "P95Tokens/s": "203",
    "LatencyFirst Answer Chunk (s)": "9.50",
    "FirstAnswerToken (s)": "9.50",
    "P5First Chunk (s)": "1.00",
    "P25First Chunk (s)": "7.80",
    "P75First Chunk (s)": "13.37",
    "P95First Chunk (s)": "17.95",
    "TotalResponse (s)": "12.77",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Grok 4.1 Fast",
    "ContextWindow": "2m",
    "Creator": "xAI",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "39",
    "ArtificialAnalysisOmniscience Index": "\u221231",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "28%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "24%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "93%",
    "AA-LCR(LongContext Reasoning)": "68%",
    "AA-OmniscienceAccuracy(Knowledge)": "24%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "28%",
    "Humanity's LastExam(Reasoning & Knowledge)": "18%",
    "GPQA Diamond(ScientificReasoning)": "85%",
    "LiveCodeBench(Coding)": "82%",
    "SciCode(Coding)": "44%",
    "IFBench(InstructionFollowing)": "53%",
    "AIME 2025(CompetitionMath)": "89%",
    "CritPt(PhysicsReasoning)": "3%",
    "MMMU Pro(VisualReasoning)": "63%",
    "BlendedUSD/1M Tokens": "$0.28",
    "InputPriceUSD/1M Tokens": "$0.20",
    "OutputPriceUSD/1M Tokens": "$0.50",
    "MedianTokens/s": "242",
    "P5Tokens/s": "131",
    "P25Tokens/s": "168",
    "P75Tokens/s": "282",
    "P95Tokens/s": "347",
    "LatencyFirst Answer Chunk (s)": "8.48",
    "FirstAnswerToken (s)": "8.48",
    "P5First Chunk (s)": "3.47",
    "P25First Chunk (s)": "4.27",
    "P75First Chunk (s)": "10.89",
    "P95First Chunk (s)": "13.88",
    "TotalResponse (s)": "10.54",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "o3",
    "ContextWindow": "200k",
    "Creator": "OpenAI",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "38",
    "ArtificialAnalysisOmniscience Index": "\u221217",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "13%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "37%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "81%",
    "AA-LCR(LongContext Reasoning)": "69%",
    "AA-OmniscienceAccuracy(Knowledge)": "37%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "13%",
    "Humanity's LastExam(Reasoning & Knowledge)": "20%",
    "GPQA Diamond(ScientificReasoning)": "83%",
    "LiveCodeBench(Coding)": "81%",
    "SciCode(Coding)": "41%",
    "IFBench(InstructionFollowing)": "71%",
    "AIME 2025(CompetitionMath)": "88%",
    "CritPt(PhysicsReasoning)": "1%",
    "MMMU Pro(VisualReasoning)": "70%",
    "BlendedUSD/1M Tokens": "$3.50",
    "InputPriceUSD/1M Tokens": "$2.00",
    "OutputPriceUSD/1M Tokens": "$8.00",
    "MedianTokens/s": "135",
    "P5Tokens/s": "59",
    "P25Tokens/s": "91",
    "P75Tokens/s": "160",
    "P95Tokens/s": "199",
    "LatencyFirst Answer Chunk (s)": "16.37",
    "FirstAnswerToken (s)": "16.37",
    "P5First Chunk (s)": "8.33",
    "P25First Chunk (s)": "12.95",
    "P75First Chunk (s)": "22.03",
    "P95First Chunk (s)": "30.65",
    "TotalResponse (s)": "20.08",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Kimi K2.5",
    "ContextWindow": "256k",
    "Creator": "Kimi",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "37",
    "ArtificialAnalysisOmniscience Index": "\u221216",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "39%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "19%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "81%",
    "AA-LCR(LongContext Reasoning)": "59%",
    "AA-OmniscienceAccuracy(Knowledge)": "22%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "51%",
    "Humanity's LastExam(Reasoning & Knowledge)": "12%",
    "GPQA Diamond(ScientificReasoning)": "79%",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "40%",
    "IFBench(InstructionFollowing)": "44%",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "1%",
    "MMMU Pro(VisualReasoning)": "73%",
    "BlendedUSD/1M Tokens": "$1.20",
    "InputPriceUSD/1M Tokens": "$0.60",
    "OutputPriceUSD/1M Tokens": "$3.00",
    "MedianTokens/s": "40",
    "P5Tokens/s": "32",
    "P25Tokens/s": "35",
    "P75Tokens/s": "43",
    "P95Tokens/s": "48",
    "LatencyFirst Answer Chunk (s)": "1.25",
    "FirstAnswerToken (s)": "1.25",
    "P5First Chunk (s)": "1.10",
    "P25First Chunk (s)": "1.17",
    "P75First Chunk (s)": "1.35",
    "P95First Chunk (s)": "1.45",
    "TotalResponse (s)": "13.64",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Claude 4.5 Sonnet",
    "ContextWindow": "1m",
    "Creator": "Anthropic",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "37",
    "ArtificialAnalysisOmniscience Index": "\u221211",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "41%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "29%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "71%",
    "AA-LCR(LongContext Reasoning)": "51%",
    "AA-OmniscienceAccuracy(Knowledge)": "27%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "49%",
    "Humanity's LastExam(Reasoning & Knowledge)": "7%",
    "GPQA Diamond(ScientificReasoning)": "73%",
    "LiveCodeBench(Coding)": "59%",
    "SciCode(Coding)": "43%",
    "IFBench(InstructionFollowing)": "43%",
    "AIME 2025(CompetitionMath)": "37%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "65%",
    "BlendedUSD/1M Tokens": "$6.00",
    "InputPriceUSD/1M Tokens": "$3.00",
    "OutputPriceUSD/1M Tokens": "$15.00",
    "MedianTokens/s": "71",
    "P5Tokens/s": "49",
    "P25Tokens/s": "63",
    "P75Tokens/s": "74",
    "P95Tokens/s": "75",
    "LatencyFirst Answer Chunk (s)": "1.17",
    "FirstAnswerToken (s)": "1.17",
    "P5First Chunk (s)": "0.89",
    "P25First Chunk (s)": "1.08",
    "P75First Chunk (s)": "1.39",
    "P95First Chunk (s)": "2.01",
    "TotalResponse (s)": "8.21",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Claude 4.5 Haiku",
    "ContextWindow": "200k",
    "Creator": "Anthropic",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "37",
    "ArtificialAnalysisOmniscience Index": "\u22126",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "33%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "27%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "55%",
    "AA-LCR(LongContext Reasoning)": "70%",
    "AA-OmniscienceAccuracy(Knowledge)": "16%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "74%",
    "Humanity's LastExam(Reasoning & Knowledge)": "10%",
    "GPQA Diamond(ScientificReasoning)": "67%",
    "LiveCodeBench(Coding)": "62%",
    "SciCode(Coding)": "43%",
    "IFBench(InstructionFollowing)": "54%",
    "AIME 2025(CompetitionMath)": "84%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "58%",
    "BlendedUSD/1M Tokens": "$2.00",
    "InputPriceUSD/1M Tokens": "$1.00",
    "OutputPriceUSD/1M Tokens": "$5.00",
    "MedianTokens/s": "112",
    "P5Tokens/s": "83",
    "P25Tokens/s": "100",
    "P75Tokens/s": "139",
    "P95Tokens/s": "154",
    "LatencyFirst Answer Chunk (s)": "0.48",
    "FirstAnswerToken (s)": "18.29",
    "P5First Chunk (s)": "0.39",
    "P25First Chunk (s)": "0.46",
    "P75First Chunk (s)": "0.83",
    "P95First Chunk (s)": "2.07",
    "TotalResponse (s)": "22.74",
    "ReasoningTime (s)": "17.80",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "KAT-Coder-Pro V1",
    "ContextWindow": "256k",
    "Creator": "KwaiKAT",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "36",
    "ArtificialAnalysisOmniscience Index": "\u221236",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "18%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "9%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "89%",
    "AA-LCR(LongContext Reasoning)": "74%",
    "AA-OmniscienceAccuracy(Knowledge)": "18%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "34%",
    "Humanity's LastExam(Reasoning & Knowledge)": "33%",
    "GPQA Diamond(ScientificReasoning)": "76%",
    "LiveCodeBench(Coding)": "75%",
    "SciCode(Coding)": "37%",
    "IFBench(InstructionFollowing)": "68%",
    "AIME 2025(CompetitionMath)": "95%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.53",
    "InputPriceUSD/1M Tokens": "$0.30",
    "OutputPriceUSD/1M Tokens": "$1.20",
    "MedianTokens/s": "57",
    "P5Tokens/s": "43",
    "P25Tokens/s": "53",
    "P75Tokens/s": "69",
    "P95Tokens/s": "76",
    "LatencyFirst Answer Chunk (s)": "1.67",
    "FirstAnswerToken (s)": "1.67",
    "P5First Chunk (s)": "1.04",
    "P25First Chunk (s)": "1.43",
    "P75First Chunk (s)": "2.11",
    "P95First Chunk (s)": "3.44",
    "TotalResponse (s)": "10.39",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Nova 2.0 Pro Preview (medium)",
    "ContextWindow": "256k",
    "Creator": "Amazon",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "36",
    "ArtificialAnalysisOmniscience Index": "\u221250",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "24%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "24%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "93%",
    "AA-LCR(LongContext Reasoning)": "54%",
    "AA-OmniscienceAccuracy(Knowledge)": "21%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "10%",
    "Humanity's LastExam(Reasoning & Knowledge)": "9%",
    "GPQA Diamond(ScientificReasoning)": "79%",
    "LiveCodeBench(Coding)": "73%",
    "SciCode(Coding)": "43%",
    "IFBench(InstructionFollowing)": "79%",
    "AIME 2025(CompetitionMath)": "89%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "65%",
    "BlendedUSD/1M Tokens": "$3.44",
    "InputPriceUSD/1M Tokens": "$1.25",
    "OutputPriceUSD/1M Tokens": "$10.00",
    "MedianTokens/s": "131",
    "P5Tokens/s": "120",
    "P25Tokens/s": "125",
    "P75Tokens/s": "146",
    "P95Tokens/s": "166",
    "LatencyFirst Answer Chunk (s)": "22.08",
    "FirstAnswerToken (s)": "37.39",
    "P5First Chunk (s)": "0.84",
    "P25First Chunk (s)": "7.62",
    "P75First Chunk (s)": "26.63",
    "P95First Chunk (s)": "31.74",
    "TotalResponse (s)": "41.22",
    "ReasoningTime (s)": "15.31",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Gemini 3 Flash",
    "ContextWindow": "1m",
    "Creator": "Google",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "35",
    "ArtificialAnalysisOmniscience Index": "\u22121",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "31%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "32%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "43%",
    "AA-LCR(LongContext Reasoning)": "48%",
    "AA-OmniscienceAccuracy(Knowledge)": "47%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "9%",
    "Humanity's LastExam(Reasoning & Knowledge)": "14%",
    "GPQA Diamond(ScientificReasoning)": "81%",
    "LiveCodeBench(Coding)": "80%",
    "SciCode(Coding)": "50%",
    "IFBench(InstructionFollowing)": "55%",
    "AIME 2025(CompetitionMath)": "56%",
    "CritPt(PhysicsReasoning)": "1%",
    "MMMU Pro(VisualReasoning)": "79%",
    "BlendedUSD/1M Tokens": "$1.13",
    "InputPriceUSD/1M Tokens": "$0.50",
    "OutputPriceUSD/1M Tokens": "$3.00",
    "MedianTokens/s": "172",
    "P5Tokens/s": "42",
    "P25Tokens/s": "158",
    "P75Tokens/s": "182",
    "P95Tokens/s": "202",
    "LatencyFirst Answer Chunk (s)": "1.46",
    "FirstAnswerToken (s)": "1.46",
    "P5First Chunk (s)": "0.72",
    "P25First Chunk (s)": "0.85",
    "P75First Chunk (s)": "14.36",
    "P95First Chunk (s)": "52.27",
    "TotalResponse (s)": "4.37",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Gemini 2.5 Pro",
    "ContextWindow": "1m",
    "Creator": "Google",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "34",
    "ArtificialAnalysisOmniscience Index": "\u221218",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "22%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "27%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "54%",
    "AA-LCR(LongContext Reasoning)": "66%",
    "AA-OmniscienceAccuracy(Knowledge)": "37%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "11%",
    "Humanity's LastExam(Reasoning & Knowledge)": "21%",
    "GPQA Diamond(ScientificReasoning)": "84%",
    "LiveCodeBench(Coding)": "80%",
    "SciCode(Coding)": "43%",
    "IFBench(InstructionFollowing)": "49%",
    "AIME 2025(CompetitionMath)": "88%",
    "CritPt(PhysicsReasoning)": "3%",
    "MMMU Pro(VisualReasoning)": "75%",
    "BlendedUSD/1M Tokens": "$3.44",
    "InputPriceUSD/1M Tokens": "$1.25",
    "OutputPriceUSD/1M Tokens": "$10.00",
    "MedianTokens/s": "153",
    "P5Tokens/s": "144",
    "P25Tokens/s": "152",
    "P75Tokens/s": "161",
    "P95Tokens/s": "168",
    "LatencyFirst Answer Chunk (s)": "37.54",
    "FirstAnswerToken (s)": "37.54",
    "P5First Chunk (s)": "29.28",
    "P25First Chunk (s)": "32.14",
    "P75First Chunk (s)": "40.99",
    "P95First Chunk (s)": "48.08",
    "TotalResponse (s)": "40.80",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "GLM-4.7",
    "ContextWindow": "200k",
    "Creator": "Z AI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "34",
    "ArtificialAnalysisOmniscience Index": "\u221248",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "35%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "30%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "94%",
    "AA-LCR(LongContext Reasoning)": "36%",
    "AA-OmniscienceAccuracy(Knowledge)": "23%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "8%",
    "Humanity's LastExam(Reasoning & Knowledge)": "6%",
    "GPQA Diamond(ScientificReasoning)": "66%",
    "LiveCodeBench(Coding)": "56%",
    "SciCode(Coding)": "35%",
    "IFBench(InstructionFollowing)": "55%",
    "AIME 2025(CompetitionMath)": "48%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$1.00",
    "InputPriceUSD/1M Tokens": "$0.60",
    "OutputPriceUSD/1M Tokens": "$2.20",
    "MedianTokens/s": "144",
    "P5Tokens/s": "27",
    "P25Tokens/s": "85",
    "P75Tokens/s": "303",
    "P95Tokens/s": "705",
    "LatencyFirst Answer Chunk (s)": "0.50",
    "FirstAnswerToken (s)": "0.50",
    "P5First Chunk (s)": "0.19",
    "P25First Chunk (s)": "0.37",
    "P75First Chunk (s)": "0.85",
    "P95First Chunk (s)": "74.28",
    "TotalResponse (s)": "3.98",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "DeepSeek V3.2 Speciale",
    "ContextWindow": "128k",
    "Creator": "DeepSeek",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "34E",
    "ArtificialAnalysisOmniscience Index": "\u221219",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "35%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "0%",
    "AA-LCR(LongContext Reasoning)": "59%",
    "AA-OmniscienceAccuracy(Knowledge)": "37%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "11%",
    "Humanity's LastExam(Reasoning & Knowledge)": "26%",
    "GPQA Diamond(ScientificReasoning)": "87%",
    "LiveCodeBench(Coding)": "90%",
    "SciCode(Coding)": "44%",
    "IFBench(InstructionFollowing)": "64%",
    "AIME 2025(CompetitionMath)": "97%",
    "CritPt(PhysicsReasoning)": "7%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.42",
    "InputPriceUSD/1M Tokens": "$0.40",
    "OutputPriceUSD/1M Tokens": "$0.50",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "GPT-5.2",
    "ContextWindow": "400k",
    "Creator": "OpenAI",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "34",
    "ArtificialAnalysisOmniscience Index": "\u221215",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "37%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "32%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "47%",
    "AA-LCR(LongContext Reasoning)": "38%",
    "AA-OmniscienceAccuracy(Knowledge)": "28%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "40%",
    "Humanity's LastExam(Reasoning & Knowledge)": "7%",
    "GPQA Diamond(ScientificReasoning)": "71%",
    "LiveCodeBench(Coding)": "67%",
    "SciCode(Coding)": "40%",
    "IFBench(InstructionFollowing)": "47%",
    "AIME 2025(CompetitionMath)": "51%",
    "CritPt(PhysicsReasoning)": "1%",
    "MMMU Pro(VisualReasoning)": "66%",
    "BlendedUSD/1M Tokens": "$4.81",
    "InputPriceUSD/1M Tokens": "$1.75",
    "OutputPriceUSD/1M Tokens": "$14.00",
    "MedianTokens/s": "79",
    "P5Tokens/s": "68",
    "P25Tokens/s": "75",
    "P75Tokens/s": "83",
    "P95Tokens/s": "86",
    "LatencyFirst Answer Chunk (s)": "0.61",
    "FirstAnswerToken (s)": "0.61",
    "P5First Chunk (s)": "0.47",
    "P25First Chunk (s)": "0.57",
    "P75First Chunk (s)": "0.75",
    "P95First Chunk (s)": "1.21",
    "TotalResponse (s)": "6.92",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Doubao Seed Code",
    "ContextWindow": "256k",
    "Creator": "ByteDance Seed",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "34",
    "ArtificialAnalysisOmniscience Index": "\u221236",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "26%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "27%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "58%",
    "AA-LCR(LongContext Reasoning)": "65%",
    "AA-OmniscienceAccuracy(Knowledge)": "24%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "21%",
    "Humanity's LastExam(Reasoning & Knowledge)": "13%",
    "GPQA Diamond(ScientificReasoning)": "76%",
    "LiveCodeBench(Coding)": "77%",
    "SciCode(Coding)": "41%",
    "IFBench(InstructionFollowing)": "51%",
    "AIME 2025(CompetitionMath)": "79%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "68%",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "gpt-oss-120B (high)",
    "ContextWindow": "131k",
    "Creator": "OpenAI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "33",
    "ArtificialAnalysisOmniscience Index": "\u221252",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "24%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "24%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "66%",
    "AA-LCR(LongContext Reasoning)": "51%",
    "AA-OmniscienceAccuracy(Knowledge)": "20%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "10%",
    "Humanity's LastExam(Reasoning & Knowledge)": "19%",
    "GPQA Diamond(ScientificReasoning)": "78%",
    "LiveCodeBench(Coding)": "88%",
    "SciCode(Coding)": "39%",
    "IFBench(InstructionFollowing)": "69%",
    "AIME 2025(CompetitionMath)": "93%",
    "CritPt(PhysicsReasoning)": "1%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.26",
    "InputPriceUSD/1M Tokens": "$0.15",
    "OutputPriceUSD/1M Tokens": "$0.60",
    "MedianTokens/s": "320",
    "P5Tokens/s": "65",
    "P25Tokens/s": "210",
    "P75Tokens/s": "731",
    "P95Tokens/s": "1,020",
    "LatencyFirst Answer Chunk (s)": "0.47",
    "FirstAnswerToken (s)": "6.72",
    "P5First Chunk (s)": "0.14",
    "P25First Chunk (s)": "0.28",
    "P75First Chunk (s)": "0.61",
    "P95First Chunk (s)": "1.79",
    "TotalResponse (s)": "8.29",
    "ReasoningTime (s)": "6.25",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 Max Thinking (Preview)",
    "ContextWindow": "262k",
    "Creator": "Alibaba",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "32",
    "ArtificialAnalysisOmniscience Index": "\u221240",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "23%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "17%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "84%",
    "AA-LCR(LongContext Reasoning)": "58%",
    "AA-OmniscienceAccuracy(Knowledge)": "27%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "10%",
    "Humanity's LastExam(Reasoning & Knowledge)": "12%",
    "GPQA Diamond(ScientificReasoning)": "78%",
    "LiveCodeBench(Coding)": "54%",
    "SciCode(Coding)": "39%",
    "IFBench(InstructionFollowing)": "54%",
    "AIME 2025(CompetitionMath)": "82%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$2.40",
    "InputPriceUSD/1M Tokens": "$1.20",
    "OutputPriceUSD/1M Tokens": "$6.00",
    "MedianTokens/s": "59",
    "P5Tokens/s": "48",
    "P25Tokens/s": "55",
    "P75Tokens/s": "64",
    "P95Tokens/s": "71",
    "LatencyFirst Answer Chunk (s)": "1.84",
    "FirstAnswerToken (s)": "35.66",
    "P5First Chunk (s)": "1.68",
    "P25First Chunk (s)": "1.75",
    "P75First Chunk (s)": "1.99",
    "P95First Chunk (s)": "2.96",
    "TotalResponse (s)": "44.11",
    "ReasoningTime (s)": "33.82",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "K-EXAONE",
    "ContextWindow": "256k",
    "Creator": "LG AI Research",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "32",
    "ArtificialAnalysisOmniscience Index": "\u221259",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "20%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "23%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "74%",
    "AA-LCR(LongContext Reasoning)": "56%",
    "AA-OmniscienceAccuracy(Knowledge)": "16%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "12%",
    "Humanity's LastExam(Reasoning & Knowledge)": "13%",
    "GPQA Diamond(ScientificReasoning)": "78%",
    "LiveCodeBench(Coding)": "77%",
    "SciCode(Coding)": "36%",
    "IFBench(InstructionFollowing)": "65%",
    "AIME 2025(CompetitionMath)": "90%",
    "CritPt(PhysicsReasoning)": "1%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "116",
    "P5Tokens/s": "101",
    "P25Tokens/s": "109",
    "P75Tokens/s": "133",
    "P95Tokens/s": "154",
    "LatencyFirst Answer Chunk (s)": "0.29",
    "FirstAnswerToken (s)": "17.49",
    "P5First Chunk (s)": "0.24",
    "P25First Chunk (s)": "0.26",
    "P75First Chunk (s)": "0.31",
    "P95First Chunk (s)": "0.33",
    "TotalResponse (s)": "21.80",
    "ReasoningTime (s)": "17.21",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "DeepSeek V3.2",
    "ContextWindow": "128k",
    "Creator": "DeepSeek",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "32",
    "ArtificialAnalysisOmniscience Index": "\u221249",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "20%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "33%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "79%",
    "AA-LCR(LongContext Reasoning)": "39%",
    "AA-OmniscienceAccuracy(Knowledge)": "23%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "7%",
    "Humanity's LastExam(Reasoning & Knowledge)": "11%",
    "GPQA Diamond(ScientificReasoning)": "75%",
    "LiveCodeBench(Coding)": "59%",
    "SciCode(Coding)": "39%",
    "IFBench(InstructionFollowing)": "49%",
    "AIME 2025(CompetitionMath)": "59%",
    "CritPt(PhysicsReasoning)": "1%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.32",
    "InputPriceUSD/1M Tokens": "$0.28",
    "OutputPriceUSD/1M Tokens": "$0.42",
    "MedianTokens/s": "49",
    "P5Tokens/s": "46",
    "P25Tokens/s": "48",
    "P75Tokens/s": "52",
    "P95Tokens/s": "54",
    "LatencyFirst Answer Chunk (s)": "1.21",
    "FirstAnswerToken (s)": "1.21",
    "P5First Chunk (s)": "0.92",
    "P25First Chunk (s)": "1.05",
    "P75First Chunk (s)": "1.36",
    "P95First Chunk (s)": "1.62",
    "TotalResponse (s)": "11.33",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Grok 3 mini Reasoning (high)",
    "ContextWindow": "1m",
    "Creator": "xAI",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "32",
    "ArtificialAnalysisOmniscience Index": "\u22127",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "17%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "90%",
    "AA-LCR(LongContext Reasoning)": "50%",
    "AA-OmniscienceAccuracy(Knowledge)": "14%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "75%",
    "Humanity's LastExam(Reasoning & Knowledge)": "11%",
    "GPQA Diamond(ScientificReasoning)": "79%",
    "LiveCodeBench(Coding)": "70%",
    "SciCode(Coding)": "41%",
    "IFBench(InstructionFollowing)": "46%",
    "AIME 2025(CompetitionMath)": "85%",
    "CritPt(PhysicsReasoning)": "1%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.35",
    "InputPriceUSD/1M Tokens": "$0.30",
    "OutputPriceUSD/1M Tokens": "$0.50",
    "MedianTokens/s": "194",
    "P5Tokens/s": "148",
    "P25Tokens/s": "193",
    "P75Tokens/s": "196",
    "P95Tokens/s": "208",
    "LatencyFirst Answer Chunk (s)": "0.78",
    "FirstAnswerToken (s)": "11.07",
    "P5First Chunk (s)": "0.63",
    "P25First Chunk (s)": "0.71",
    "P75First Chunk (s)": "0.88",
    "P95First Chunk (s)": "4.39",
    "TotalResponse (s)": "13.65",
    "ReasoningTime (s)": "10.30",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Nova 2.0 Pro Preview (low)",
    "ContextWindow": "256k",
    "Creator": "Amazon",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "32",
    "ArtificialAnalysisOmniscience Index": "\u221248",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "11%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "17%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "91%",
    "AA-LCR(LongContext Reasoning)": "62%",
    "AA-OmniscienceAccuracy(Knowledge)": "21%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "13%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "75%",
    "LiveCodeBench(Coding)": "64%",
    "SciCode(Coding)": "39%",
    "IFBench(InstructionFollowing)": "80%",
    "AIME 2025(CompetitionMath)": "63%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "63%",
    "BlendedUSD/1M Tokens": "$3.44",
    "InputPriceUSD/1M Tokens": "$1.25",
    "OutputPriceUSD/1M Tokens": "$10.00",
    "MedianTokens/s": "132",
    "P5Tokens/s": "120",
    "P25Tokens/s": "127",
    "P75Tokens/s": "143",
    "P95Tokens/s": "160",
    "LatencyFirst Answer Chunk (s)": "12.16",
    "FirstAnswerToken (s)": "27.29",
    "P5First Chunk (s)": "3.53",
    "P25First Chunk (s)": "8.80",
    "P75First Chunk (s)": "13.62",
    "P95First Chunk (s)": "23.01",
    "TotalResponse (s)": "31.07",
    "ReasoningTime (s)": "15.13",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 Max",
    "ContextWindow": "262k",
    "Creator": "Alibaba",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "31",
    "ArtificialAnalysisOmniscience Index": "\u221245",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "28%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "21%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "74%",
    "AA-LCR(LongContext Reasoning)": "47%",
    "AA-OmniscienceAccuracy(Knowledge)": "23%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "11%",
    "Humanity's LastExam(Reasoning & Knowledge)": "11%",
    "GPQA Diamond(ScientificReasoning)": "76%",
    "LiveCodeBench(Coding)": "77%",
    "SciCode(Coding)": "38%",
    "IFBench(InstructionFollowing)": "44%",
    "AIME 2025(CompetitionMath)": "81%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$2.40",
    "InputPriceUSD/1M Tokens": "$1.20",
    "OutputPriceUSD/1M Tokens": "$6.00",
    "MedianTokens/s": "28",
    "P5Tokens/s": "26",
    "P25Tokens/s": "26",
    "P75Tokens/s": "29",
    "P95Tokens/s": "29",
    "LatencyFirst Answer Chunk (s)": "2.30",
    "FirstAnswerToken (s)": "2.30",
    "P5First Chunk (s)": "1.79",
    "P25First Chunk (s)": "2.06",
    "P75First Chunk (s)": "2.41",
    "P95First Chunk (s)": "2.92",
    "TotalResponse (s)": "19.90",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Claude 4.5 Haiku",
    "ContextWindow": "200k",
    "Creator": "Anthropic",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "31",
    "ArtificialAnalysisOmniscience Index": "\u22128",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "33%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "27%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "33%",
    "AA-LCR(LongContext Reasoning)": "44%",
    "AA-OmniscienceAccuracy(Knowledge)": "13%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "75%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "65%",
    "LiveCodeBench(Coding)": "51%",
    "SciCode(Coding)": "34%",
    "IFBench(InstructionFollowing)": "42%",
    "AIME 2025(CompetitionMath)": "39%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "55%",
    "BlendedUSD/1M Tokens": "$2.00",
    "InputPriceUSD/1M Tokens": "$1.00",
    "OutputPriceUSD/1M Tokens": "$5.00",
    "MedianTokens/s": "107",
    "P5Tokens/s": "91",
    "P25Tokens/s": "98",
    "P75Tokens/s": "113",
    "P95Tokens/s": "127",
    "LatencyFirst Answer Chunk (s)": "0.45",
    "FirstAnswerToken (s)": "0.45",
    "P5First Chunk (s)": "0.39",
    "P25First Chunk (s)": "0.42",
    "P75First Chunk (s)": "0.52",
    "P95First Chunk (s)": "0.80",
    "TotalResponse (s)": "5.15",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "MiMo-V2-Flash",
    "ContextWindow": "256k",
    "Creator": "Xiaomi",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "31",
    "ArtificialAnalysisOmniscience Index": "\u221245",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "30%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "26%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "84%",
    "AA-LCR(LongContext Reasoning)": "31%",
    "AA-OmniscienceAccuracy(Knowledge)": "16%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "27%",
    "Humanity's LastExam(Reasoning & Knowledge)": "8%",
    "GPQA Diamond(ScientificReasoning)": "66%",
    "LiveCodeBench(Coding)": "40%",
    "SciCode(Coding)": "26%",
    "IFBench(InstructionFollowing)": "40%",
    "AIME 2025(CompetitionMath)": "68%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.15",
    "InputPriceUSD/1M Tokens": "$0.10",
    "OutputPriceUSD/1M Tokens": "$0.30",
    "MedianTokens/s": "142",
    "P5Tokens/s": "123",
    "P25Tokens/s": "135",
    "P75Tokens/s": "157",
    "P95Tokens/s": "169",
    "LatencyFirst Answer Chunk (s)": "1.22",
    "FirstAnswerToken (s)": "1.22",
    "P5First Chunk (s)": "0.96",
    "P25First Chunk (s)": "1.03",
    "P75First Chunk (s)": "1.44",
    "P95First Chunk (s)": "2.01",
    "TotalResponse (s)": "4.74",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "GLM-4.7-Flash",
    "ContextWindow": "200k",
    "Creator": "Z AI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "30",
    "ArtificialAnalysisOmniscience Index": "\u221260",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "19%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "22%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "99%",
    "AA-LCR(LongContext Reasoning)": "35%",
    "AA-OmniscienceAccuracy(Knowledge)": "15%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "11%",
    "Humanity's LastExam(Reasoning & Knowledge)": "7%",
    "GPQA Diamond(ScientificReasoning)": "58%",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "34%",
    "IFBench(InstructionFollowing)": "61%",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.15",
    "InputPriceUSD/1M Tokens": "$0.07",
    "OutputPriceUSD/1M Tokens": "$0.40",
    "MedianTokens/s": "59",
    "P5Tokens/s": "31",
    "P25Tokens/s": "43",
    "P75Tokens/s": "76",
    "P95Tokens/s": "104",
    "LatencyFirst Answer Chunk (s)": "0.71",
    "FirstAnswerToken (s)": "34.75",
    "P5First Chunk (s)": "0.21",
    "P25First Chunk (s)": "0.27",
    "P75First Chunk (s)": "0.80",
    "P95First Chunk (s)": "7.81",
    "TotalResponse (s)": "43.26",
    "ReasoningTime (s)": "34.04",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Nova 2.0 Lite (medium)",
    "ContextWindow": "1m",
    "Creator": "Amazon",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "30",
    "ArtificialAnalysisOmniscience Index": "\u221258",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "10%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "17%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "76%",
    "AA-LCR(LongContext Reasoning)": "58%",
    "AA-OmniscienceAccuracy(Knowledge)": "17%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "9%",
    "Humanity's LastExam(Reasoning & Knowledge)": "9%",
    "GPQA Diamond(ScientificReasoning)": "77%",
    "LiveCodeBench(Coding)": "66%",
    "SciCode(Coding)": "37%",
    "IFBench(InstructionFollowing)": "69%",
    "AIME 2025(CompetitionMath)": "89%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "63%",
    "BlendedUSD/1M Tokens": "$0.85",
    "InputPriceUSD/1M Tokens": "$0.30",
    "OutputPriceUSD/1M Tokens": "$2.50",
    "MedianTokens/s": "236",
    "P5Tokens/s": "213",
    "P25Tokens/s": "227",
    "P75Tokens/s": "251",
    "P95Tokens/s": "253",
    "LatencyFirst Answer Chunk (s)": "18.55",
    "FirstAnswerToken (s)": "27.01",
    "P5First Chunk (s)": "9.11",
    "P25First Chunk (s)": "14.64",
    "P75First Chunk (s)": "19.33",
    "P95First Chunk (s)": "20.16",
    "TotalResponse (s)": "29.13",
    "ReasoningTime (s)": "8.46",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 235B A22B 2507",
    "ContextWindow": "256k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "29",
    "ArtificialAnalysisOmniscience Index": "\u221248",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "18%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "14%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "53%",
    "AA-LCR(LongContext Reasoning)": "67%",
    "AA-OmniscienceAccuracy(Knowledge)": "22%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "10%",
    "Humanity's LastExam(Reasoning & Knowledge)": "15%",
    "GPQA Diamond(ScientificReasoning)": "79%",
    "LiveCodeBench(Coding)": "79%",
    "SciCode(Coding)": "42%",
    "IFBench(InstructionFollowing)": "51%",
    "AIME 2025(CompetitionMath)": "91%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$2.63",
    "InputPriceUSD/1M Tokens": "$0.70",
    "OutputPriceUSD/1M Tokens": "$8.40",
    "MedianTokens/s": "49",
    "P5Tokens/s": "36",
    "P25Tokens/s": "44",
    "P75Tokens/s": "55",
    "P95Tokens/s": "62",
    "LatencyFirst Answer Chunk (s)": "1.31",
    "FirstAnswerToken (s)": "41.73",
    "P5First Chunk (s)": "1.17",
    "P25First Chunk (s)": "1.22",
    "P75First Chunk (s)": "1.38",
    "P95First Chunk (s)": "1.58",
    "TotalResponse (s)": "51.84",
    "ReasoningTime (s)": "40.42",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "ERNIE 5.0 Thinking Preview",
    "ContextWindow": "128k",
    "Creator": "Baidu",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "29",
    "ArtificialAnalysisOmniscience Index": "\u221242",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "17%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "25%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "84%",
    "AA-LCR(LongContext Reasoning)": "7%",
    "AA-OmniscienceAccuracy(Knowledge)": "23%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "15%",
    "Humanity's LastExam(Reasoning & Knowledge)": "13%",
    "GPQA Diamond(ScientificReasoning)": "78%",
    "LiveCodeBench(Coding)": "81%",
    "SciCode(Coding)": "38%",
    "IFBench(InstructionFollowing)": "41%",
    "AIME 2025(CompetitionMath)": "85%",
    "CritPt(PhysicsReasoning)": "1%",
    "MMMU Pro(VisualReasoning)": "65%",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Grok Code Fast 1",
    "ContextWindow": "256k",
    "Creator": "xAI",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "29",
    "ArtificialAnalysisOmniscience Index": "\u221238",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "15%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "17%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "76%",
    "AA-LCR(LongContext Reasoning)": "48%",
    "AA-OmniscienceAccuracy(Knowledge)": "23%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "21%",
    "Humanity's LastExam(Reasoning & Knowledge)": "8%",
    "GPQA Diamond(ScientificReasoning)": "73%",
    "LiveCodeBench(Coding)": "66%",
    "SciCode(Coding)": "36%",
    "IFBench(InstructionFollowing)": "41%",
    "AIME 2025(CompetitionMath)": "43%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.53",
    "InputPriceUSD/1M Tokens": "$0.20",
    "OutputPriceUSD/1M Tokens": "$1.50",
    "MedianTokens/s": "335",
    "P5Tokens/s": "192",
    "P25Tokens/s": "273",
    "P75Tokens/s": "381",
    "P95Tokens/s": "440",
    "LatencyFirst Answer Chunk (s)": "7.93",
    "FirstAnswerToken (s)": "7.93",
    "P5First Chunk (s)": "3.44",
    "P25First Chunk (s)": "6.24",
    "P75First Chunk (s)": "9.25",
    "P95First Chunk (s)": "10.86",
    "TotalResponse (s)": "9.43",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3-Coder-Next",
    "ContextWindow": "256k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "28",
    "ArtificialAnalysisOmniscience Index": "\u221263",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "23%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "18%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "80%",
    "AA-LCR(LongContext Reasoning)": "40%",
    "AA-OmniscienceAccuracy(Knowledge)": "15%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "7%",
    "Humanity's LastExam(Reasoning & Knowledge)": "9%",
    "GPQA Diamond(ScientificReasoning)": "74%",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "32%",
    "IFBench(InstructionFollowing)": "35%",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.53",
    "InputPriceUSD/1M Tokens": "$0.20",
    "OutputPriceUSD/1M Tokens": "$1.20",
    "MedianTokens/s": "133",
    "P5Tokens/s": "55",
    "P25Tokens/s": "95",
    "P75Tokens/s": "147",
    "P95Tokens/s": "155",
    "LatencyFirst Answer Chunk (s)": "0.83",
    "FirstAnswerToken (s)": "0.83",
    "P5First Chunk (s)": "0.33",
    "P25First Chunk (s)": "0.45",
    "P75First Chunk (s)": "1.05",
    "P95First Chunk (s)": "1.66",
    "TotalResponse (s)": "4.58",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Nova 2.0 Omni (medium)",
    "ContextWindow": "1m",
    "Creator": "Amazon",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "28",
    "ArtificialAnalysisOmniscience Index": "\u221260",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "17%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "5%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "80%",
    "AA-LCR(LongContext Reasoning)": "54%",
    "AA-OmniscienceAccuracy(Knowledge)": "17%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "7%",
    "Humanity's LastExam(Reasoning & Knowledge)": "7%",
    "GPQA Diamond(ScientificReasoning)": "76%",
    "LiveCodeBench(Coding)": "66%",
    "SciCode(Coding)": "36%",
    "IFBench(InstructionFollowing)": "66%",
    "AIME 2025(CompetitionMath)": "90%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "62%",
    "BlendedUSD/1M Tokens": "$0.85",
    "InputPriceUSD/1M Tokens": "$0.30",
    "OutputPriceUSD/1M Tokens": "$2.50",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Apriel-v1.6-15B-Thinker",
    "ContextWindow": "128k",
    "Creator": "ServiceNow",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "28",
    "ArtificialAnalysisOmniscience Index": "\u221260",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "7%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "14%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "69%",
    "AA-LCR(LongContext Reasoning)": "50%",
    "AA-OmniscienceAccuracy(Knowledge)": "17%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "8%",
    "Humanity's LastExam(Reasoning & Knowledge)": "10%",
    "GPQA Diamond(ScientificReasoning)": "73%",
    "LiveCodeBench(Coding)": "81%",
    "SciCode(Coding)": "37%",
    "IFBench(InstructionFollowing)": "69%",
    "AIME 2025(CompetitionMath)": "88%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "143",
    "P5Tokens/s": "133",
    "P25Tokens/s": "139",
    "P75Tokens/s": "147",
    "P95Tokens/s": "151",
    "LatencyFirst Answer Chunk (s)": "0.23",
    "FirstAnswerToken (s)": "14.26",
    "P5First Chunk (s)": "0.18",
    "P25First Chunk (s)": "0.20",
    "P75First Chunk (s)": "0.30",
    "P95First Chunk (s)": "0.49",
    "TotalResponse (s)": "17.77",
    "ReasoningTime (s)": "14.03",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 VL 235B A22B",
    "ContextWindow": "262k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "28",
    "ArtificialAnalysisOmniscience Index": "\u221247",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "13%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "11%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "54%",
    "AA-LCR(LongContext Reasoning)": "59%",
    "AA-OmniscienceAccuracy(Knowledge)": "20%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "16%",
    "Humanity's LastExam(Reasoning & Knowledge)": "10%",
    "GPQA Diamond(ScientificReasoning)": "77%",
    "LiveCodeBench(Coding)": "65%",
    "SciCode(Coding)": "40%",
    "IFBench(InstructionFollowing)": "56%",
    "AIME 2025(CompetitionMath)": "88%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "69%",
    "BlendedUSD/1M Tokens": "$2.63",
    "InputPriceUSD/1M Tokens": "$0.70",
    "OutputPriceUSD/1M Tokens": "$8.40",
    "MedianTokens/s": "47",
    "P5Tokens/s": "43",
    "P25Tokens/s": "45",
    "P75Tokens/s": "49",
    "P95Tokens/s": "50",
    "LatencyFirst Answer Chunk (s)": "1.21",
    "FirstAnswerToken (s)": "43.45",
    "P5First Chunk (s)": "1.12",
    "P25First Chunk (s)": "1.17",
    "P75First Chunk (s)": "1.34",
    "P95First Chunk (s)": "1.44",
    "TotalResponse (s)": "54.01",
    "ReasoningTime (s)": "42.24",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Magistral Medium 1.2",
    "ContextWindow": "128k",
    "Creator": "Mistral",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "27",
    "ArtificialAnalysisOmniscience Index": "\u221228",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "10%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "13%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "52%",
    "AA-LCR(LongContext Reasoning)": "51%",
    "AA-OmniscienceAccuracy(Knowledge)": "20%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "40%",
    "Humanity's LastExam(Reasoning & Knowledge)": "10%",
    "GPQA Diamond(ScientificReasoning)": "74%",
    "LiveCodeBench(Coding)": "75%",
    "SciCode(Coding)": "39%",
    "IFBench(InstructionFollowing)": "43%",
    "AIME 2025(CompetitionMath)": "82%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "60%",
    "BlendedUSD/1M Tokens": "$2.75",
    "InputPriceUSD/1M Tokens": "$2.00",
    "OutputPriceUSD/1M Tokens": "$5.00",
    "MedianTokens/s": "35",
    "P5Tokens/s": "23",
    "P25Tokens/s": "32",
    "P75Tokens/s": "40",
    "P95Tokens/s": "103",
    "LatencyFirst Answer Chunk (s)": "0.49",
    "FirstAnswerToken (s)": "57.85",
    "P5First Chunk (s)": "0.44",
    "P25First Chunk (s)": "0.45",
    "P75First Chunk (s)": "0.55",
    "P95First Chunk (s)": "0.85",
    "TotalResponse (s)": "72.19",
    "ReasoningTime (s)": "57.35",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "DeepSeek R1 0528",
    "ContextWindow": "128k",
    "Creator": "DeepSeek",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "27",
    "ArtificialAnalysisOmniscience Index": "\u221230",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "12%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "16%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "37%",
    "AA-LCR(LongContext Reasoning)": "55%",
    "AA-OmniscienceAccuracy(Knowledge)": "29%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "17%",
    "Humanity's LastExam(Reasoning & Knowledge)": "15%",
    "GPQA Diamond(ScientificReasoning)": "81%",
    "LiveCodeBench(Coding)": "77%",
    "SciCode(Coding)": "40%",
    "IFBench(InstructionFollowing)": "40%",
    "AIME 2025(CompetitionMath)": "76%",
    "CritPt(PhysicsReasoning)": "1%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$2.36",
    "InputPriceUSD/1M Tokens": "$1.35",
    "OutputPriceUSD/1M Tokens": "$4.20",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "GPT-5 nano (high)",
    "ContextWindow": "400k",
    "Creator": "OpenAI",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "27",
    "ArtificialAnalysisOmniscience Index": "\u221230",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "16%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "12%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "37%",
    "AA-LCR(LongContext Reasoning)": "42%",
    "AA-OmniscienceAccuracy(Knowledge)": "18%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "41%",
    "Humanity's LastExam(Reasoning & Knowledge)": "8%",
    "GPQA Diamond(ScientificReasoning)": "68%",
    "LiveCodeBench(Coding)": "79%",
    "SciCode(Coding)": "37%",
    "IFBench(InstructionFollowing)": "68%",
    "AIME 2025(CompetitionMath)": "84%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "61%",
    "BlendedUSD/1M Tokens": "$0.14",
    "InputPriceUSD/1M Tokens": "$0.05",
    "OutputPriceUSD/1M Tokens": "$0.40",
    "MedianTokens/s": "145",
    "P5Tokens/s": "114",
    "P25Tokens/s": "122",
    "P75Tokens/s": "151",
    "P95Tokens/s": "185",
    "LatencyFirst Answer Chunk (s)": "104.85",
    "FirstAnswerToken (s)": "104.85",
    "P5First Chunk (s)": "76.99",
    "P25First Chunk (s)": "81.62",
    "P75First Chunk (s)": "126.24",
    "P95First Chunk (s)": "165.16",
    "TotalResponse (s)": "108.31",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 Next 80B A3B",
    "ContextWindow": "262k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "26",
    "ArtificialAnalysisOmniscience Index": "\u221253",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "14%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "10%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "42%",
    "AA-LCR(LongContext Reasoning)": "60%",
    "AA-OmniscienceAccuracy(Knowledge)": "18%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "13%",
    "Humanity's LastExam(Reasoning & Knowledge)": "12%",
    "GPQA Diamond(ScientificReasoning)": "76%",
    "LiveCodeBench(Coding)": "78%",
    "SciCode(Coding)": "39%",
    "IFBench(InstructionFollowing)": "61%",
    "AIME 2025(CompetitionMath)": "84%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$1.88",
    "InputPriceUSD/1M Tokens": "$0.50",
    "OutputPriceUSD/1M Tokens": "$6.00",
    "MedianTokens/s": "166",
    "P5Tokens/s": "126",
    "P25Tokens/s": "162",
    "P75Tokens/s": "175",
    "P95Tokens/s": "187",
    "LatencyFirst Answer Chunk (s)": "1.06",
    "FirstAnswerToken (s)": "13.10",
    "P5First Chunk (s)": "1.02",
    "P25First Chunk (s)": "1.04",
    "P75First Chunk (s)": "1.25",
    "P95First Chunk (s)": "1.41",
    "TotalResponse (s)": "16.11",
    "ReasoningTime (s)": "12.04",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 235B 2507",
    "ContextWindow": "256k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "25",
    "ArtificialAnalysisOmniscience Index": "\u221245",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "17%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "15%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "33%",
    "AA-LCR(LongContext Reasoning)": "31%",
    "AA-OmniscienceAccuracy(Knowledge)": "18%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "24%",
    "Humanity's LastExam(Reasoning & Knowledge)": "11%",
    "GPQA Diamond(ScientificReasoning)": "75%",
    "LiveCodeBench(Coding)": "52%",
    "SciCode(Coding)": "36%",
    "IFBench(InstructionFollowing)": "46%",
    "AIME 2025(CompetitionMath)": "72%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$1.23",
    "InputPriceUSD/1M Tokens": "$0.70",
    "OutputPriceUSD/1M Tokens": "$2.80",
    "MedianTokens/s": "47",
    "P5Tokens/s": "42",
    "P25Tokens/s": "43",
    "P75Tokens/s": "54",
    "P95Tokens/s": "63",
    "LatencyFirst Answer Chunk (s)": "1.21",
    "FirstAnswerToken (s)": "1.21",
    "P5First Chunk (s)": "1.09",
    "P25First Chunk (s)": "1.13",
    "P75First Chunk (s)": "1.40",
    "P95First Chunk (s)": "1.49",
    "TotalResponse (s)": "11.89",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 Coder 480B",
    "ContextWindow": "262k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "25",
    "ArtificialAnalysisOmniscience Index": "\u221224",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "4%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "19%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "44%",
    "AA-LCR(LongContext Reasoning)": "42%",
    "AA-OmniscienceAccuracy(Knowledge)": "14%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "55%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "62%",
    "LiveCodeBench(Coding)": "59%",
    "SciCode(Coding)": "36%",
    "IFBench(InstructionFollowing)": "41%",
    "AIME 2025(CompetitionMath)": "39%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$3.00",
    "InputPriceUSD/1M Tokens": "$1.50",
    "OutputPriceUSD/1M Tokens": "$7.50",
    "MedianTokens/s": "56",
    "P5Tokens/s": "43",
    "P25Tokens/s": "50",
    "P75Tokens/s": "64",
    "P95Tokens/s": "67",
    "LatencyFirst Answer Chunk (s)": "1.74",
    "FirstAnswerToken (s)": "1.74",
    "P5First Chunk (s)": "1.52",
    "P25First Chunk (s)": "1.61",
    "P75First Chunk (s)": "1.81",
    "P95First Chunk (s)": "2.01",
    "TotalResponse (s)": "10.61",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "K2 Think V2",
    "ContextWindow": "262k",
    "Creator": "MBZUAI Institute of Foundation Models",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "25",
    "ArtificialAnalysisOmniscience Index": "\u221229",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "9%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "7%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "25%",
    "AA-LCR(LongContext Reasoning)": "53%",
    "AA-OmniscienceAccuracy(Knowledge)": "15%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "48%",
    "Humanity's LastExam(Reasoning & Knowledge)": "10%",
    "GPQA Diamond(ScientificReasoning)": "71%",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "33%",
    "IFBench(InstructionFollowing)": "63%",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 VL 32B",
    "ContextWindow": "256k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "25",
    "ArtificialAnalysisOmniscience Index": "\u221253",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "12%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "8%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "46%",
    "AA-LCR(LongContext Reasoning)": "55%",
    "AA-OmniscienceAccuracy(Knowledge)": "16%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "17%",
    "Humanity's LastExam(Reasoning & Knowledge)": "10%",
    "GPQA Diamond(ScientificReasoning)": "73%",
    "LiveCodeBench(Coding)": "74%",
    "SciCode(Coding)": "28%",
    "IFBench(InstructionFollowing)": "59%",
    "AIME 2025(CompetitionMath)": "85%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "63%",
    "BlendedUSD/1M Tokens": "$2.63",
    "InputPriceUSD/1M Tokens": "$0.70",
    "OutputPriceUSD/1M Tokens": "$8.40",
    "MedianTokens/s": "85",
    "P5Tokens/s": "77",
    "P25Tokens/s": "84",
    "P75Tokens/s": "85",
    "P95Tokens/s": "87",
    "LatencyFirst Answer Chunk (s)": "1.33",
    "FirstAnswerToken (s)": "24.99",
    "P5First Chunk (s)": "1.08",
    "P25First Chunk (s)": "1.14",
    "P75First Chunk (s)": "1.43",
    "P95First Chunk (s)": "1.54",
    "TotalResponse (s)": "30.90",
    "ReasoningTime (s)": "23.66",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "gpt-oss-20B (high)",
    "ContextWindow": "131k",
    "Creator": "OpenAI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "24",
    "ArtificialAnalysisOmniscience Index": "\u221265",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "11%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "11%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "60%",
    "AA-LCR(LongContext Reasoning)": "31%",
    "AA-OmniscienceAccuracy(Knowledge)": "15%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "7%",
    "Humanity's LastExam(Reasoning & Knowledge)": "10%",
    "GPQA Diamond(ScientificReasoning)": "69%",
    "LiveCodeBench(Coding)": "78%",
    "SciCode(Coding)": "34%",
    "IFBench(InstructionFollowing)": "65%",
    "AIME 2025(CompetitionMath)": "89%",
    "CritPt(PhysicsReasoning)": "1%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.10",
    "InputPriceUSD/1M Tokens": "$0.07",
    "OutputPriceUSD/1M Tokens": "$0.20",
    "MedianTokens/s": "309",
    "P5Tokens/s": "121",
    "P25Tokens/s": "172",
    "P75Tokens/s": "492",
    "P95Tokens/s": "988",
    "LatencyFirst Answer Chunk (s)": "0.51",
    "FirstAnswerToken (s)": "6.99",
    "P5First Chunk (s)": "0.16",
    "P25First Chunk (s)": "0.31",
    "P75First Chunk (s)": "0.86",
    "P95First Chunk (s)": "25.27",
    "TotalResponse (s)": "8.61",
    "ReasoningTime (s)": "6.48",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "NVIDIA Nemotron 3 Nano",
    "ContextWindow": "1m",
    "Creator": "NVIDIA",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "24",
    "ArtificialAnalysisOmniscience Index": "\u221252",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "7%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "14%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "41%",
    "AA-LCR(LongContext Reasoning)": "34%",
    "AA-OmniscienceAccuracy(Knowledge)": "17%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "17%",
    "Humanity's LastExam(Reasoning & Knowledge)": "10%",
    "GPQA Diamond(ScientificReasoning)": "76%",
    "LiveCodeBench(Coding)": "74%",
    "SciCode(Coding)": "30%",
    "IFBench(InstructionFollowing)": "71%",
    "AIME 2025(CompetitionMath)": "91%",
    "CritPt(PhysicsReasoning)": "1%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.10",
    "InputPriceUSD/1M Tokens": "$0.06",
    "OutputPriceUSD/1M Tokens": "$0.24",
    "MedianTokens/s": "195",
    "P5Tokens/s": "91",
    "P25Tokens/s": "135",
    "P75Tokens/s": "206",
    "P95Tokens/s": "296",
    "LatencyFirst Answer Chunk (s)": "0.48",
    "FirstAnswerToken (s)": "10.72",
    "P5First Chunk (s)": "0.19",
    "P25First Chunk (s)": "0.22",
    "P75First Chunk (s)": "0.59",
    "P95First Chunk (s)": "1.15",
    "TotalResponse (s)": "13.28",
    "ReasoningTime (s)": "10.24",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Nova 2.0 Lite (low)",
    "ContextWindow": "1m",
    "Creator": "Amazon",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "24",
    "ArtificialAnalysisOmniscience Index": "\u221255",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "4%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "4%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "72%",
    "AA-LCR(LongContext Reasoning)": "52%",
    "AA-OmniscienceAccuracy(Knowledge)": "17%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "14%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "70%",
    "LiveCodeBench(Coding)": "47%",
    "SciCode(Coding)": "33%",
    "IFBench(InstructionFollowing)": "61%",
    "AIME 2025(CompetitionMath)": "47%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "58%",
    "BlendedUSD/1M Tokens": "$0.85",
    "InputPriceUSD/1M Tokens": "$0.30",
    "OutputPriceUSD/1M Tokens": "$2.50",
    "MedianTokens/s": "228",
    "P5Tokens/s": "203",
    "P25Tokens/s": "217",
    "P75Tokens/s": "246",
    "P95Tokens/s": "259",
    "LatencyFirst Answer Chunk (s)": "5.78",
    "FirstAnswerToken (s)": "14.56",
    "P5First Chunk (s)": "2.82",
    "P25First Chunk (s)": "3.62",
    "P75First Chunk (s)": "8.09",
    "P95First Chunk (s)": "11.32",
    "TotalResponse (s)": "16.75",
    "ReasoningTime (s)": "8.78",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "gpt-oss-120B (low)",
    "ContextWindow": "131k",
    "Creator": "OpenAI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "24",
    "ArtificialAnalysisOmniscience Index": "\u221256",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "19%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "5%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "45%",
    "AA-LCR(LongContext Reasoning)": "44%",
    "AA-OmniscienceAccuracy(Knowledge)": "18%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "9%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "67%",
    "LiveCodeBench(Coding)": "71%",
    "SciCode(Coding)": "36%",
    "IFBench(InstructionFollowing)": "58%",
    "AIME 2025(CompetitionMath)": "67%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.26",
    "InputPriceUSD/1M Tokens": "$0.15",
    "OutputPriceUSD/1M Tokens": "$0.59",
    "MedianTokens/s": "324",
    "P5Tokens/s": "56",
    "P25Tokens/s": "197",
    "P75Tokens/s": "765",
    "P95Tokens/s": "990",
    "LatencyFirst Answer Chunk (s)": "0.40",
    "FirstAnswerToken (s)": "6.57",
    "P5First Chunk (s)": "0.14",
    "P25First Chunk (s)": "0.26",
    "P75First Chunk (s)": "0.62",
    "P95First Chunk (s)": "12.92",
    "TotalResponse (s)": "8.11",
    "ReasoningTime (s)": "6.17",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "HyperCLOVA X SEED Think (32B)",
    "ContextWindow": "128k",
    "Creator": "Naver",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "24",
    "ArtificialAnalysisOmniscience Index": "\u221252",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "13%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "12%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "87%",
    "AA-LCR(LongContext Reasoning)": "12%",
    "AA-OmniscienceAccuracy(Knowledge)": "16%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "20%",
    "Humanity's LastExam(Reasoning & Knowledge)": "6%",
    "GPQA Diamond(ScientificReasoning)": "62%",
    "LiveCodeBench(Coding)": "63%",
    "SciCode(Coding)": "28%",
    "IFBench(InstructionFollowing)": "38%",
    "AIME 2025(CompetitionMath)": "59%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Grok 4.1 Fast",
    "ContextWindow": "2m",
    "Creator": "xAI",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "24",
    "ArtificialAnalysisOmniscience Index": "\u221252",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "17%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "14%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "64%",
    "AA-LCR(LongContext Reasoning)": "22%",
    "AA-OmniscienceAccuracy(Knowledge)": "16%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "19%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "64%",
    "LiveCodeBench(Coding)": "40%",
    "SciCode(Coding)": "30%",
    "IFBench(InstructionFollowing)": "37%",
    "AIME 2025(CompetitionMath)": "34%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "48%",
    "BlendedUSD/1M Tokens": "$0.28",
    "InputPriceUSD/1M Tokens": "$0.20",
    "OutputPriceUSD/1M Tokens": "$0.50",
    "MedianTokens/s": "171",
    "P5Tokens/s": "113",
    "P25Tokens/s": "136",
    "P75Tokens/s": "184",
    "P95Tokens/s": "214",
    "LatencyFirst Answer Chunk (s)": "0.66",
    "FirstAnswerToken (s)": "0.66",
    "P5First Chunk (s)": "0.55",
    "P25First Chunk (s)": "0.61",
    "P75First Chunk (s)": "0.73",
    "P95First Chunk (s)": "0.90",
    "TotalResponse (s)": "3.58",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "GLM-4.6V",
    "ContextWindow": "128k",
    "Creator": "Z AI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "23",
    "ArtificialAnalysisOmniscience Index": "\u221226",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "9%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "14%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "32%",
    "AA-LCR(LongContext Reasoning)": "40%",
    "AA-OmniscienceAccuracy(Knowledge)": "15%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "51%",
    "Humanity's LastExam(Reasoning & Knowledge)": "9%",
    "GPQA Diamond(ScientificReasoning)": "72%",
    "LiveCodeBench(Coding)": "16%",
    "SciCode(Coding)": "30%",
    "IFBench(InstructionFollowing)": "30%",
    "AIME 2025(CompetitionMath)": "85%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "49%",
    "BlendedUSD/1M Tokens": "$0.45",
    "InputPriceUSD/1M Tokens": "$0.30",
    "OutputPriceUSD/1M Tokens": "$0.90",
    "MedianTokens/s": "67",
    "P5Tokens/s": "16",
    "P25Tokens/s": "21",
    "P75Tokens/s": "104",
    "P95Tokens/s": "121",
    "LatencyFirst Answer Chunk (s)": "0.71",
    "FirstAnswerToken (s)": "30.60",
    "P5First Chunk (s)": "0.19",
    "P25First Chunk (s)": "0.36",
    "P75First Chunk (s)": "1.24",
    "P95First Chunk (s)": "4.79",
    "TotalResponse (s)": "38.07",
    "ReasoningTime (s)": "29.89",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Nova 2.0 Omni (low)",
    "ContextWindow": "1m",
    "Creator": "Amazon",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "23",
    "ArtificialAnalysisOmniscience Index": "\u221251",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "4%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "68%",
    "AA-LCR(LongContext Reasoning)": "51%",
    "AA-OmniscienceAccuracy(Knowledge)": "18%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "16%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "70%",
    "LiveCodeBench(Coding)": "59%",
    "SciCode(Coding)": "34%",
    "IFBench(InstructionFollowing)": "62%",
    "AIME 2025(CompetitionMath)": "56%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "60%",
    "BlendedUSD/1M Tokens": "$0.85",
    "InputPriceUSD/1M Tokens": "$0.30",
    "OutputPriceUSD/1M Tokens": "$2.50",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "GLM-4.5-Air",
    "ContextWindow": "128k",
    "Creator": "Z AI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "23",
    "ArtificialAnalysisOmniscience Index": "\u221263",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "7%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "21%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "47%",
    "AA-LCR(LongContext Reasoning)": "44%",
    "AA-OmniscienceAccuracy(Knowledge)": "15%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "8%",
    "Humanity's LastExam(Reasoning & Knowledge)": "7%",
    "GPQA Diamond(ScientificReasoning)": "73%",
    "LiveCodeBench(Coding)": "68%",
    "SciCode(Coding)": "31%",
    "IFBench(InstructionFollowing)": "38%",
    "AIME 2025(CompetitionMath)": "81%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.42",
    "InputPriceUSD/1M Tokens": "$0.20",
    "OutputPriceUSD/1M Tokens": "$1.10",
    "MedianTokens/s": "111",
    "P5Tokens/s": "76",
    "P25Tokens/s": "99",
    "P75Tokens/s": "233",
    "P95Tokens/s": "293",
    "LatencyFirst Answer Chunk (s)": "0.65",
    "FirstAnswerToken (s)": "18.67",
    "P5First Chunk (s)": "0.25",
    "P25First Chunk (s)": "0.54",
    "P75First Chunk (s)": "1.12",
    "P95First Chunk (s)": "1.53",
    "TotalResponse (s)": "23.18",
    "ReasoningTime (s)": "18.02",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "K-EXAONE",
    "ContextWindow": "256k",
    "Creator": "LG AI Research",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "23",
    "ArtificialAnalysisOmniscience Index": "\u221266",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "17%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "7%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "59%",
    "AA-LCR(LongContext Reasoning)": "47%",
    "AA-OmniscienceAccuracy(Knowledge)": "13%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "10%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "70%",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "27%",
    "IFBench(InstructionFollowing)": "40%",
    "AIME 2025(CompetitionMath)": "44%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "83",
    "P5Tokens/s": "70",
    "P25Tokens/s": "76",
    "P75Tokens/s": "96",
    "P95Tokens/s": "108",
    "LatencyFirst Answer Chunk (s)": "0.30",
    "FirstAnswerToken (s)": "0.30",
    "P5First Chunk (s)": "0.26",
    "P25First Chunk (s)": "0.28",
    "P75First Chunk (s)": "0.32",
    "P95First Chunk (s)": "0.37",
    "TotalResponse (s)": "6.29",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Mi:dm K 2.5 Pro",
    "ContextWindow": "128k",
    "Creator": "Korea Telecom",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "23",
    "ArtificialAnalysisOmniscience Index": "\u221255",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "11%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "2%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "87%",
    "AA-LCR(LongContext Reasoning)": "9%",
    "AA-OmniscienceAccuracy(Knowledge)": "19%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "8%",
    "Humanity's LastExam(Reasoning & Knowledge)": "8%",
    "GPQA Diamond(ScientificReasoning)": "70%",
    "LiveCodeBench(Coding)": "66%",
    "SciCode(Coding)": "33%",
    "IFBench(InstructionFollowing)": "49%",
    "AIME 2025(CompetitionMath)": "77%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Nova 2.0 Pro Preview",
    "ContextWindow": "256k",
    "Creator": "Amazon",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "23",
    "ArtificialAnalysisOmniscience Index": "\u221250",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "17%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "72%",
    "AA-LCR(LongContext Reasoning)": "28%",
    "AA-OmniscienceAccuracy(Knowledge)": "16%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "21%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "64%",
    "LiveCodeBench(Coding)": "47%",
    "SciCode(Coding)": "28%",
    "IFBench(InstructionFollowing)": "52%",
    "AIME 2025(CompetitionMath)": "31%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$3.44",
    "InputPriceUSD/1M Tokens": "$1.25",
    "OutputPriceUSD/1M Tokens": "$10.00",
    "MedianTokens/s": "161",
    "P5Tokens/s": "145",
    "P25Tokens/s": "150",
    "P75Tokens/s": "166",
    "P95Tokens/s": "177",
    "LatencyFirst Answer Chunk (s)": "0.46",
    "FirstAnswerToken (s)": "0.46",
    "P5First Chunk (s)": "0.44",
    "P25First Chunk (s)": "0.44",
    "P75First Chunk (s)": "0.51",
    "P95First Chunk (s)": "0.55",
    "TotalResponse (s)": "3.57",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Mistral Large 3",
    "ContextWindow": "256k",
    "Creator": "Mistral",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "23",
    "ArtificialAnalysisOmniscience Index": "\u221241",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "20%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "16%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "25%",
    "AA-LCR(LongContext Reasoning)": "35%",
    "AA-OmniscienceAccuracy(Knowledge)": "24%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "15%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "68%",
    "LiveCodeBench(Coding)": "47%",
    "SciCode(Coding)": "36%",
    "IFBench(InstructionFollowing)": "36%",
    "AIME 2025(CompetitionMath)": "38%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "56%",
    "BlendedUSD/1M Tokens": "$0.75",
    "InputPriceUSD/1M Tokens": "$0.50",
    "OutputPriceUSD/1M Tokens": "$1.50",
    "MedianTokens/s": "56",
    "P5Tokens/s": "51",
    "P25Tokens/s": "54",
    "P75Tokens/s": "61",
    "P95Tokens/s": "67",
    "LatencyFirst Answer Chunk (s)": "0.50",
    "FirstAnswerToken (s)": "0.50",
    "P5First Chunk (s)": "0.46",
    "P25First Chunk (s)": "0.48",
    "P75First Chunk (s)": "0.55",
    "P95First Chunk (s)": "0.73",
    "TotalResponse (s)": "9.51",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Ring-1T",
    "ContextWindow": "128k",
    "Creator": "InclusionAI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "23",
    "ArtificialAnalysisOmniscience Index": "\u221251",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "13%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "7%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "26%",
    "AA-LCR(LongContext Reasoning)": "46%",
    "AA-OmniscienceAccuracy(Knowledge)": "19%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "12%",
    "Humanity's LastExam(Reasoning & Knowledge)": "10%",
    "GPQA Diamond(ScientificReasoning)": "77%",
    "LiveCodeBench(Coding)": "64%",
    "SciCode(Coding)": "37%",
    "IFBench(InstructionFollowing)": "45%",
    "AIME 2025(CompetitionMath)": "89%",
    "CritPt(PhysicsReasoning)": "1%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 30B A3B 2507",
    "ContextWindow": "262k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "22",
    "ArtificialAnalysisOmniscience Index": "\u221257",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "12%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "5%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "28%",
    "AA-LCR(LongContext Reasoning)": "59%",
    "AA-OmniscienceAccuracy(Knowledge)": "15%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "14%",
    "Humanity's LastExam(Reasoning & Knowledge)": "10%",
    "GPQA Diamond(ScientificReasoning)": "71%",
    "LiveCodeBench(Coding)": "71%",
    "SciCode(Coding)": "33%",
    "IFBench(InstructionFollowing)": "51%",
    "AIME 2025(CompetitionMath)": "56%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.75",
    "InputPriceUSD/1M Tokens": "$0.20",
    "OutputPriceUSD/1M Tokens": "$2.40",
    "MedianTokens/s": "157",
    "P5Tokens/s": "77",
    "P25Tokens/s": "92",
    "P75Tokens/s": "166",
    "P95Tokens/s": "191",
    "LatencyFirst Answer Chunk (s)": "1.13",
    "FirstAnswerToken (s)": "13.89",
    "P5First Chunk (s)": "0.99",
    "P25First Chunk (s)": "1.00",
    "P75First Chunk (s)": "1.19",
    "P95First Chunk (s)": "1.38",
    "TotalResponse (s)": "17.08",
    "ReasoningTime (s)": "12.76",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "INTELLECT-3",
    "ContextWindow": "131k",
    "Creator": "Prime Intellect",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "22",
    "ArtificialAnalysisOmniscience Index": "\u221252",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "16%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "9%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "27%",
    "AA-LCR(LongContext Reasoning)": "32%",
    "AA-OmniscienceAccuracy(Knowledge)": "18%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "13%",
    "Humanity's LastExam(Reasoning & Knowledge)": "12%",
    "GPQA Diamond(ScientificReasoning)": "76%",
    "LiveCodeBench(Coding)": "78%",
    "SciCode(Coding)": "39%",
    "IFBench(InstructionFollowing)": "34%",
    "AIME 2025(CompetitionMath)": "88%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.42",
    "InputPriceUSD/1M Tokens": "$0.20",
    "OutputPriceUSD/1M Tokens": "$1.10",
    "MedianTokens/s": "87",
    "P5Tokens/s": "76",
    "P25Tokens/s": "86",
    "P75Tokens/s": "88",
    "P95Tokens/s": "92",
    "LatencyFirst Answer Chunk (s)": "0.61",
    "FirstAnswerToken (s)": "23.72",
    "P5First Chunk (s)": "0.56",
    "P25First Chunk (s)": "0.59",
    "P75First Chunk (s)": "0.69",
    "P95First Chunk (s)": "1.17",
    "TotalResponse (s)": "29.49",
    "ReasoningTime (s)": "23.11",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Devstral 2",
    "ContextWindow": "256k",
    "Creator": "Mistral",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "22",
    "ArtificialAnalysisOmniscience Index": "\u221248",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "21%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "19%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "25%",
    "AA-LCR(LongContext Reasoning)": "30%",
    "AA-OmniscienceAccuracy(Knowledge)": "20%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "16%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "59%",
    "LiveCodeBench(Coding)": "45%",
    "SciCode(Coding)": "33%",
    "IFBench(InstructionFollowing)": "38%",
    "AIME 2025(CompetitionMath)": "37%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "74",
    "P5Tokens/s": "33",
    "P25Tokens/s": "73",
    "P75Tokens/s": "78",
    "P95Tokens/s": "79",
    "LatencyFirst Answer Chunk (s)": "0.40",
    "FirstAnswerToken (s)": "0.40",
    "P5First Chunk (s)": "0.37",
    "P25First Chunk (s)": "0.39",
    "P75First Chunk (s)": "0.42",
    "P95First Chunk (s)": "0.52",
    "TotalResponse (s)": "7.12",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Gemini 2.5 Flash-Lite (Sep)",
    "ContextWindow": "1m",
    "Creator": "Google",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "22",
    "ArtificialAnalysisOmniscience Index": "\u221255",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "13%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "31%",
    "AA-LCR(LongContext Reasoning)": "59%",
    "AA-OmniscienceAccuracy(Knowledge)": "17%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "13%",
    "Humanity's LastExam(Reasoning & Knowledge)": "7%",
    "GPQA Diamond(ScientificReasoning)": "71%",
    "LiveCodeBench(Coding)": "69%",
    "SciCode(Coding)": "29%",
    "IFBench(InstructionFollowing)": "53%",
    "AIME 2025(CompetitionMath)": "69%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "65%",
    "BlendedUSD/1M Tokens": "$0.17",
    "InputPriceUSD/1M Tokens": "$0.10",
    "OutputPriceUSD/1M Tokens": "$0.40",
    "MedianTokens/s": "520",
    "P5Tokens/s": "423",
    "P25Tokens/s": "466",
    "P75Tokens/s": "574",
    "P95Tokens/s": "617",
    "LatencyFirst Answer Chunk (s)": "8.45",
    "FirstAnswerToken (s)": "8.45",
    "P5First Chunk (s)": "6.84",
    "P25First Chunk (s)": "7.56",
    "P75First Chunk (s)": "9.34",
    "P95First Chunk (s)": "10.06",
    "TotalResponse (s)": "9.41",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Solar Open 100B",
    "ContextWindow": "128k",
    "Creator": "Upstage",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "22",
    "ArtificialAnalysisOmniscience Index": "\u221256",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "12%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "2%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "48%",
    "AA-LCR(LongContext Reasoning)": "36%",
    "AA-OmniscienceAccuracy(Knowledge)": "17%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "12%",
    "Humanity's LastExam(Reasoning & Knowledge)": "9%",
    "GPQA Diamond(ScientificReasoning)": "66%",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "27%",
    "IFBench(InstructionFollowing)": "58%",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "GLM-4.7-Flash",
    "ContextWindow": "200k",
    "Creator": "Z AI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "21",
    "ArtificialAnalysisOmniscience Index": "\u221270",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "18%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "4%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "92%",
    "AA-LCR(LongContext Reasoning)": "15%",
    "AA-OmniscienceAccuracy(Knowledge)": "12%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "6%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "45%",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "26%",
    "IFBench(InstructionFollowing)": "46%",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.15",
    "InputPriceUSD/1M Tokens": "$0.07",
    "OutputPriceUSD/1M Tokens": "$0.40",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Mistral Medium 3.1",
    "ContextWindow": "128k",
    "Creator": "Mistral",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "21",
    "ArtificialAnalysisOmniscience Index": "\u221248",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "17%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "11%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "41%",
    "AA-LCR(LongContext Reasoning)": "20%",
    "AA-OmniscienceAccuracy(Knowledge)": "19%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "18%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "59%",
    "LiveCodeBench(Coding)": "41%",
    "SciCode(Coding)": "34%",
    "IFBench(InstructionFollowing)": "40%",
    "AIME 2025(CompetitionMath)": "38%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "54%",
    "BlendedUSD/1M Tokens": "$0.80",
    "InputPriceUSD/1M Tokens": "$0.40",
    "OutputPriceUSD/1M Tokens": "$2.00",
    "MedianTokens/s": "100",
    "P5Tokens/s": "71",
    "P25Tokens/s": "83",
    "P75Tokens/s": "108",
    "P95Tokens/s": "128",
    "LatencyFirst Answer Chunk (s)": "0.36",
    "FirstAnswerToken (s)": "0.36",
    "P5First Chunk (s)": "0.34",
    "P25First Chunk (s)": "0.35",
    "P75First Chunk (s)": "0.40",
    "P95First Chunk (s)": "0.48",
    "TotalResponse (s)": "5.38",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "gpt-oss-20B (low)",
    "ContextWindow": "131k",
    "Creator": "OpenAI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "21",
    "ArtificialAnalysisOmniscience Index": "\u221261",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "7%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "5%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "50%",
    "AA-LCR(LongContext Reasoning)": "31%",
    "AA-OmniscienceAccuracy(Knowledge)": "14%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "14%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "61%",
    "LiveCodeBench(Coding)": "65%",
    "SciCode(Coding)": "34%",
    "IFBench(InstructionFollowing)": "58%",
    "AIME 2025(CompetitionMath)": "62%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.10",
    "InputPriceUSD/1M Tokens": "$0.07",
    "OutputPriceUSD/1M Tokens": "$0.20",
    "MedianTokens/s": "309",
    "P5Tokens/s": "120",
    "P25Tokens/s": "171",
    "P75Tokens/s": "447",
    "P95Tokens/s": "984",
    "LatencyFirst Answer Chunk (s)": "0.43",
    "FirstAnswerToken (s)": "6.91",
    "P5First Chunk (s)": "0.16",
    "P25First Chunk (s)": "0.31",
    "P75First Chunk (s)": "0.88",
    "P95First Chunk (s)": "6.90",
    "TotalResponse (s)": "8.53",
    "ReasoningTime (s)": "6.47",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "K2-V2 (high)",
    "ContextWindow": "512k",
    "Creator": "MBZUAI Institute of Foundation Models",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "21",
    "ArtificialAnalysisOmniscience Index": "\u221257",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "6%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "10%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "28%",
    "AA-LCR(LongContext Reasoning)": "33%",
    "AA-OmniscienceAccuracy(Knowledge)": "17%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "11%",
    "Humanity's LastExam(Reasoning & Knowledge)": "10%",
    "GPQA Diamond(ScientificReasoning)": "68%",
    "LiveCodeBench(Coding)": "69%",
    "SciCode(Coding)": "29%",
    "IFBench(InstructionFollowing)": "60%",
    "AIME 2025(CompetitionMath)": "78%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 VL 235B A22B",
    "ContextWindow": "262k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "21",
    "ArtificialAnalysisOmniscience Index": "\u221254",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "10%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "7%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "35%",
    "AA-LCR(LongContext Reasoning)": "32%",
    "AA-OmniscienceAccuracy(Knowledge)": "19%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "10%",
    "Humanity's LastExam(Reasoning & Knowledge)": "6%",
    "GPQA Diamond(ScientificReasoning)": "71%",
    "LiveCodeBench(Coding)": "59%",
    "SciCode(Coding)": "36%",
    "IFBench(InstructionFollowing)": "43%",
    "AIME 2025(CompetitionMath)": "71%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "68%",
    "BlendedUSD/1M Tokens": "$1.23",
    "InputPriceUSD/1M Tokens": "$0.70",
    "OutputPriceUSD/1M Tokens": "$2.80",
    "MedianTokens/s": "45",
    "P5Tokens/s": "32",
    "P25Tokens/s": "41",
    "P75Tokens/s": "46",
    "P95Tokens/s": "49",
    "LatencyFirst Answer Chunk (s)": "1.20",
    "FirstAnswerToken (s)": "1.20",
    "P5First Chunk (s)": "1.02",
    "P25First Chunk (s)": "1.10",
    "P75First Chunk (s)": "1.28",
    "P95First Chunk (s)": "1.44",
    "TotalResponse (s)": "12.43",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 Next 80B A3B",
    "ContextWindow": "262k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "20",
    "ArtificialAnalysisOmniscience Index": "\u221260",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "9%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "8%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "22%",
    "AA-LCR(LongContext Reasoning)": "51%",
    "AA-OmniscienceAccuracy(Knowledge)": "17%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "7%",
    "Humanity's LastExam(Reasoning & Knowledge)": "7%",
    "GPQA Diamond(ScientificReasoning)": "74%",
    "LiveCodeBench(Coding)": "68%",
    "SciCode(Coding)": "31%",
    "IFBench(InstructionFollowing)": "40%",
    "AIME 2025(CompetitionMath)": "66%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.88",
    "InputPriceUSD/1M Tokens": "$0.50",
    "OutputPriceUSD/1M Tokens": "$2.00",
    "MedianTokens/s": "152",
    "P5Tokens/s": "106",
    "P25Tokens/s": "130",
    "P75Tokens/s": "172",
    "P95Tokens/s": "187",
    "LatencyFirst Answer Chunk (s)": "1.17",
    "FirstAnswerToken (s)": "1.17",
    "P5First Chunk (s)": "1.06",
    "P25First Chunk (s)": "1.08",
    "P75First Chunk (s)": "1.33",
    "P95First Chunk (s)": "1.53",
    "TotalResponse (s)": "4.45",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 Coder 30B A3B",
    "ContextWindow": "262k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "20",
    "ArtificialAnalysisOmniscience Index": "\u221252",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "14%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "15%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "35%",
    "AA-LCR(LongContext Reasoning)": "29%",
    "AA-OmniscienceAccuracy(Knowledge)": "15%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "21%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "52%",
    "LiveCodeBench(Coding)": "40%",
    "SciCode(Coding)": "28%",
    "IFBench(InstructionFollowing)": "33%",
    "AIME 2025(CompetitionMath)": "29%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.90",
    "InputPriceUSD/1M Tokens": "$0.45",
    "OutputPriceUSD/1M Tokens": "$2.25",
    "MedianTokens/s": "22",
    "P5Tokens/s": "21",
    "P25Tokens/s": "22",
    "P75Tokens/s": "23",
    "P95Tokens/s": "25",
    "LatencyFirst Answer Chunk (s)": "1.63",
    "FirstAnswerToken (s)": "1.63",
    "P5First Chunk (s)": "1.35",
    "P25First Chunk (s)": "1.43",
    "P75First Chunk (s)": "1.71",
    "P95First Chunk (s)": "1.87",
    "TotalResponse (s)": "24.06",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 VL 30B A3B",
    "ContextWindow": "256k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "20",
    "ArtificialAnalysisOmniscience Index": "\u221259",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "11%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "5%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "20%",
    "AA-LCR(LongContext Reasoning)": "41%",
    "AA-OmniscienceAccuracy(Knowledge)": "16%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "11%",
    "Humanity's LastExam(Reasoning & Knowledge)": "9%",
    "GPQA Diamond(ScientificReasoning)": "72%",
    "LiveCodeBench(Coding)": "70%",
    "SciCode(Coding)": "29%",
    "IFBench(InstructionFollowing)": "45%",
    "AIME 2025(CompetitionMath)": "82%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "62%",
    "BlendedUSD/1M Tokens": "$0.75",
    "InputPriceUSD/1M Tokens": "$0.20",
    "OutputPriceUSD/1M Tokens": "$2.40",
    "MedianTokens/s": "116",
    "P5Tokens/s": "91",
    "P25Tokens/s": "109",
    "P75Tokens/s": "121",
    "P95Tokens/s": "122",
    "LatencyFirst Answer Chunk (s)": "1.02",
    "FirstAnswerToken (s)": "18.30",
    "P5First Chunk (s)": "0.98",
    "P25First Chunk (s)": "0.99",
    "P75First Chunk (s)": "1.12",
    "P95First Chunk (s)": "1.38",
    "TotalResponse (s)": "22.61",
    "ReasoningTime (s)": "17.27",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Gemini 2.5 Flash-Lite (Sep)",
    "ContextWindow": "1m",
    "Creator": "Google",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "19",
    "ArtificialAnalysisOmniscience Index": "\u221244",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "8%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "30%",
    "AA-LCR(LongContext Reasoning)": "48%",
    "AA-OmniscienceAccuracy(Knowledge)": "13%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "34%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "65%",
    "LiveCodeBench(Coding)": "64%",
    "SciCode(Coding)": "28%",
    "IFBench(InstructionFollowing)": "42%",
    "AIME 2025(CompetitionMath)": "47%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "63%",
    "BlendedUSD/1M Tokens": "$0.17",
    "InputPriceUSD/1M Tokens": "$0.10",
    "OutputPriceUSD/1M Tokens": "$0.40",
    "MedianTokens/s": "348",
    "P5Tokens/s": "325",
    "P25Tokens/s": "335",
    "P75Tokens/s": "358",
    "P95Tokens/s": "366",
    "LatencyFirst Answer Chunk (s)": "0.56",
    "FirstAnswerToken (s)": "0.56",
    "P5First Chunk (s)": "0.38",
    "P25First Chunk (s)": "0.46",
    "P75First Chunk (s)": "0.59",
    "P95First Chunk (s)": "0.61",
    "TotalResponse (s)": "2.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Devstral Small 2",
    "ContextWindow": "256k",
    "Creator": "Mistral",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "19",
    "ArtificialAnalysisOmniscience Index": "\u221259",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "19%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "17%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "23%",
    "AA-LCR(LongContext Reasoning)": "24%",
    "AA-OmniscienceAccuracy(Knowledge)": "15%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "13%",
    "Humanity's LastExam(Reasoning & Knowledge)": "3%",
    "GPQA Diamond(ScientificReasoning)": "53%",
    "LiveCodeBench(Coding)": "35%",
    "SciCode(Coding)": "29%",
    "IFBench(InstructionFollowing)": "31%",
    "AIME 2025(CompetitionMath)": "34%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "45%",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "201",
    "P5Tokens/s": "185",
    "P25Tokens/s": "195",
    "P75Tokens/s": "209",
    "P95Tokens/s": "213",
    "LatencyFirst Answer Chunk (s)": "0.35",
    "FirstAnswerToken (s)": "0.35",
    "P5First Chunk (s)": "0.32",
    "P25First Chunk (s)": "0.34",
    "P75First Chunk (s)": "0.36",
    "P95First Chunk (s)": "0.38",
    "TotalResponse (s)": "2.84",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Motif-2-12.7B",
    "ContextWindow": "128k",
    "Creator": "Motif Technologies",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "19",
    "ArtificialAnalysisOmniscience Index": "\u221262",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "3%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "4%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "47%",
    "AA-LCR(LongContext Reasoning)": "13%",
    "AA-OmniscienceAccuracy(Knowledge)": "14%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "11%",
    "Humanity's LastExam(Reasoning & Knowledge)": "8%",
    "GPQA Diamond(ScientificReasoning)": "70%",
    "LiveCodeBench(Coding)": "65%",
    "SciCode(Coding)": "28%",
    "IFBench(InstructionFollowing)": "57%",
    "AIME 2025(CompetitionMath)": "80%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Ling-1T",
    "ContextWindow": "128k",
    "Creator": "InclusionAI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "19",
    "ArtificialAnalysisOmniscience Index": "\u221257",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "11%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "33%",
    "AA-LCR(LongContext Reasoning)": "35%",
    "AA-OmniscienceAccuracy(Knowledge)": "19%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "7%",
    "Humanity's LastExam(Reasoning & Knowledge)": "7%",
    "GPQA Diamond(ScientificReasoning)": "72%",
    "LiveCodeBench(Coding)": "68%",
    "SciCode(Coding)": "35%",
    "IFBench(InstructionFollowing)": "35%",
    "AIME 2025(CompetitionMath)": "71%",
    "CritPt(PhysicsReasoning)": "1%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Nova Premier",
    "ContextWindow": "1m",
    "Creator": "Amazon",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "19",
    "ArtificialAnalysisOmniscience Index": "\u221238",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "4%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "7%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "38%",
    "AA-LCR(LongContext Reasoning)": "30%",
    "AA-OmniscienceAccuracy(Knowledge)": "19%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "29%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "57%",
    "LiveCodeBench(Coding)": "32%",
    "SciCode(Coding)": "28%",
    "IFBench(InstructionFollowing)": "36%",
    "AIME 2025(CompetitionMath)": "17%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$5.00",
    "InputPriceUSD/1M Tokens": "$2.50",
    "OutputPriceUSD/1M Tokens": "$12.50",
    "MedianTokens/s": "79",
    "P5Tokens/s": "68",
    "P25Tokens/s": "76",
    "P75Tokens/s": "81",
    "P95Tokens/s": "82",
    "LatencyFirst Answer Chunk (s)": "0.82",
    "FirstAnswerToken (s)": "0.82",
    "P5First Chunk (s)": "0.77",
    "P25First Chunk (s)": "0.79",
    "P75First Chunk (s)": "0.86",
    "P95First Chunk (s)": "1.37",
    "TotalResponse (s)": "7.15",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "K2-V2 (medium)",
    "ContextWindow": "512k",
    "Creator": "MBZUAI Institute of Foundation Models",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "19",
    "ArtificialAnalysisOmniscience Index": "\u221251",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "7%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "8%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "25%",
    "AA-LCR(LongContext Reasoning)": "28%",
    "AA-OmniscienceAccuracy(Knowledge)": "17%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "19%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "60%",
    "LiveCodeBench(Coding)": "54%",
    "SciCode(Coding)": "25%",
    "IFBench(InstructionFollowing)": "55%",
    "AIME 2025(CompetitionMath)": "65%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Llama Nemotron Super 49B v1.5",
    "ContextWindow": "128k",
    "Creator": "NVIDIA",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "19",
    "ArtificialAnalysisOmniscience Index": "\u221247",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "5%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "28%",
    "AA-LCR(LongContext Reasoning)": "34%",
    "AA-OmniscienceAccuracy(Knowledge)": "16%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "24%",
    "Humanity's LastExam(Reasoning & Knowledge)": "7%",
    "GPQA Diamond(ScientificReasoning)": "75%",
    "LiveCodeBench(Coding)": "74%",
    "SciCode(Coding)": "35%",
    "IFBench(InstructionFollowing)": "37%",
    "AIME 2025(CompetitionMath)": "77%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.17",
    "InputPriceUSD/1M Tokens": "$0.10",
    "OutputPriceUSD/1M Tokens": "$0.40",
    "MedianTokens/s": "75",
    "P5Tokens/s": "60",
    "P25Tokens/s": "69",
    "P75Tokens/s": "76",
    "P95Tokens/s": "80",
    "LatencyFirst Answer Chunk (s)": "0.28",
    "FirstAnswerToken (s)": "27.09",
    "P5First Chunk (s)": "0.19",
    "P25First Chunk (s)": "0.22",
    "P75First Chunk (s)": "0.46",
    "P95First Chunk (s)": "0.60",
    "TotalResponse (s)": "33.79",
    "ReasoningTime (s)": "26.81",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 4B 2507",
    "ContextWindow": "262k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "19",
    "ArtificialAnalysisOmniscience Index": "\u221255",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "8%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "2%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "25%",
    "AA-LCR(LongContext Reasoning)": "38%",
    "AA-OmniscienceAccuracy(Knowledge)": "13%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "22%",
    "Humanity's LastExam(Reasoning & Knowledge)": "6%",
    "GPQA Diamond(ScientificReasoning)": "67%",
    "LiveCodeBench(Coding)": "64%",
    "SciCode(Coding)": "26%",
    "IFBench(InstructionFollowing)": "50%",
    "AIME 2025(CompetitionMath)": "83%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Hermes 4 405B",
    "ContextWindow": "128k",
    "Creator": "Nous Research",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "19",
    "ArtificialAnalysisOmniscience Index": "\u221237",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "7%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "11%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "22%",
    "AA-LCR(LongContext Reasoning)": "21%",
    "AA-OmniscienceAccuracy(Knowledge)": "29%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "6%",
    "Humanity's LastExam(Reasoning & Knowledge)": "10%",
    "GPQA Diamond(ScientificReasoning)": "73%",
    "LiveCodeBench(Coding)": "69%",
    "SciCode(Coding)": "25%",
    "IFBench(InstructionFollowing)": "33%",
    "AIME 2025(CompetitionMath)": "70%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$1.50",
    "InputPriceUSD/1M Tokens": "$1.00",
    "OutputPriceUSD/1M Tokens": "$3.00",
    "MedianTokens/s": "36",
    "P5Tokens/s": "28",
    "P25Tokens/s": "35",
    "P75Tokens/s": "36",
    "P95Tokens/s": "38",
    "LatencyFirst Answer Chunk (s)": "0.79",
    "FirstAnswerToken (s)": "56.86",
    "P5First Chunk (s)": "0.69",
    "P25First Chunk (s)": "0.72",
    "P75First Chunk (s)": "1.06",
    "P95First Chunk (s)": "1.17",
    "TotalResponse (s)": "70.87",
    "ReasoningTime (s)": "56.07",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Llama 3.3 Nemotron Super 49B",
    "ContextWindow": "128k",
    "Creator": "NVIDIA",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "18E",
    "ArtificialAnalysisOmniscience Index": "\u221259",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "--",
    "AA-LCR(LongContext Reasoning)": "17%",
    "AA-OmniscienceAccuracy(Knowledge)": "16%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "9%",
    "Humanity's LastExam(Reasoning & Knowledge)": "7%",
    "GPQA Diamond(ScientificReasoning)": "64%",
    "LiveCodeBench(Coding)": "28%",
    "SciCode(Coding)": "28%",
    "IFBench(InstructionFollowing)": "38%",
    "AIME 2025(CompetitionMath)": "55%",
    "CritPt(PhysicsReasoning)": "--",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Llama 4 Maverick",
    "ContextWindow": "1m",
    "Creator": "Meta",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "18",
    "ArtificialAnalysisOmniscience Index": "\u221243",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "7%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "18%",
    "AA-LCR(LongContext Reasoning)": "46%",
    "AA-OmniscienceAccuracy(Knowledge)": "24%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "12%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "67%",
    "LiveCodeBench(Coding)": "40%",
    "SciCode(Coding)": "33%",
    "IFBench(InstructionFollowing)": "43%",
    "AIME 2025(CompetitionMath)": "19%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "62%",
    "BlendedUSD/1M Tokens": "$0.46",
    "InputPriceUSD/1M Tokens": "$0.31",
    "OutputPriceUSD/1M Tokens": "$0.85",
    "MedianTokens/s": "129",
    "P5Tokens/s": "46",
    "P25Tokens/s": "91",
    "P75Tokens/s": "204",
    "P95Tokens/s": "691",
    "LatencyFirst Answer Chunk (s)": "0.44",
    "FirstAnswerToken (s)": "0.44",
    "P5First Chunk (s)": "0.21",
    "P25First Chunk (s)": "0.34",
    "P75First Chunk (s)": "0.59",
    "P95First Chunk (s)": "1.72",
    "TotalResponse (s)": "4.32",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Magistral Small 1.2",
    "ContextWindow": "128k",
    "Creator": "Mistral",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "18",
    "ArtificialAnalysisOmniscience Index": "\u221266",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "12%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "5%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "28%",
    "AA-LCR(LongContext Reasoning)": "16%",
    "AA-OmniscienceAccuracy(Knowledge)": "13%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "9%",
    "Humanity's LastExam(Reasoning & Knowledge)": "6%",
    "GPQA Diamond(ScientificReasoning)": "66%",
    "LiveCodeBench(Coding)": "72%",
    "SciCode(Coding)": "35%",
    "IFBench(InstructionFollowing)": "44%",
    "AIME 2025(CompetitionMath)": "80%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "56%",
    "BlendedUSD/1M Tokens": "$0.75",
    "InputPriceUSD/1M Tokens": "$0.50",
    "OutputPriceUSD/1M Tokens": "$1.50",
    "MedianTokens/s": "163",
    "P5Tokens/s": "73",
    "P25Tokens/s": "84",
    "P75Tokens/s": "200",
    "P95Tokens/s": "219",
    "LatencyFirst Answer Chunk (s)": "0.35",
    "FirstAnswerToken (s)": "12.66",
    "P5First Chunk (s)": "0.30",
    "P25First Chunk (s)": "0.31",
    "P75First Chunk (s)": "0.40",
    "P95First Chunk (s)": "1.53",
    "TotalResponse (s)": "15.74",
    "ReasoningTime (s)": "12.30",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Nova 2.0 Lite",
    "ContextWindow": "1m",
    "Creator": "Amazon",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "18",
    "ArtificialAnalysisOmniscience Index": "\u221260",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "7%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "62%",
    "AA-LCR(LongContext Reasoning)": "18%",
    "AA-OmniscienceAccuracy(Knowledge)": "13%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "15%",
    "Humanity's LastExam(Reasoning & Knowledge)": "3%",
    "GPQA Diamond(ScientificReasoning)": "60%",
    "LiveCodeBench(Coding)": "35%",
    "SciCode(Coding)": "24%",
    "IFBench(InstructionFollowing)": "41%",
    "AIME 2025(CompetitionMath)": "34%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "49%",
    "BlendedUSD/1M Tokens": "$0.85",
    "InputPriceUSD/1M Tokens": "$0.30",
    "OutputPriceUSD/1M Tokens": "$2.50",
    "MedianTokens/s": "201",
    "P5Tokens/s": "147",
    "P25Tokens/s": "190",
    "P75Tokens/s": "211",
    "P95Tokens/s": "222",
    "LatencyFirst Answer Chunk (s)": "0.52",
    "FirstAnswerToken (s)": "0.52",
    "P5First Chunk (s)": "0.48",
    "P25First Chunk (s)": "0.49",
    "P75First Chunk (s)": "0.55",
    "P95First Chunk (s)": "0.59",
    "TotalResponse (s)": "3.01",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Hermes 4 405B",
    "ContextWindow": "128k",
    "Creator": "Nous Research",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "18",
    "ArtificialAnalysisOmniscience Index": "\u221235",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "4%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "10%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "27%",
    "AA-LCR(LongContext Reasoning)": "20%",
    "AA-OmniscienceAccuracy(Knowledge)": "25%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "21%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "54%",
    "LiveCodeBench(Coding)": "55%",
    "SciCode(Coding)": "35%",
    "IFBench(InstructionFollowing)": "35%",
    "AIME 2025(CompetitionMath)": "15%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$1.50",
    "InputPriceUSD/1M Tokens": "$1.00",
    "OutputPriceUSD/1M Tokens": "$3.00",
    "MedianTokens/s": "32",
    "P5Tokens/s": "24",
    "P25Tokens/s": "30",
    "P75Tokens/s": "33",
    "P95Tokens/s": "34",
    "LatencyFirst Answer Chunk (s)": "0.72",
    "FirstAnswerToken (s)": "0.72",
    "P5First Chunk (s)": "0.66",
    "P25First Chunk (s)": "0.68",
    "P75First Chunk (s)": "0.79",
    "P95First Chunk (s)": "1.71",
    "TotalResponse (s)": "16.26",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 VL 32B",
    "ContextWindow": "256k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "17",
    "ArtificialAnalysisOmniscience Index": "\u221264",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "8%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "29%",
    "AA-LCR(LongContext Reasoning)": "31%",
    "AA-OmniscienceAccuracy(Knowledge)": "14%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "9%",
    "Humanity's LastExam(Reasoning & Knowledge)": "6%",
    "GPQA Diamond(ScientificReasoning)": "67%",
    "LiveCodeBench(Coding)": "51%",
    "SciCode(Coding)": "30%",
    "IFBench(InstructionFollowing)": "39%",
    "AIME 2025(CompetitionMath)": "68%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "64%",
    "BlendedUSD/1M Tokens": "$1.23",
    "InputPriceUSD/1M Tokens": "$0.70",
    "OutputPriceUSD/1M Tokens": "$2.80",
    "MedianTokens/s": "63",
    "P5Tokens/s": "56",
    "P25Tokens/s": "60",
    "P75Tokens/s": "64",
    "P95Tokens/s": "70",
    "LatencyFirst Answer Chunk (s)": "1.07",
    "FirstAnswerToken (s)": "1.07",
    "P5First Chunk (s)": "1.03",
    "P25First Chunk (s)": "1.06",
    "P75First Chunk (s)": "1.32",
    "P95First Chunk (s)": "1.57",
    "TotalResponse (s)": "9.03",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "GLM-4.6V",
    "ContextWindow": "128k",
    "Creator": "Z AI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "17",
    "ArtificialAnalysisOmniscience Index": "\u221239",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "12%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "3%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "31%",
    "AA-LCR(LongContext Reasoning)": "12%",
    "AA-OmniscienceAccuracy(Knowledge)": "17%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "34%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "57%",
    "LiveCodeBench(Coding)": "41%",
    "SciCode(Coding)": "27%",
    "IFBench(InstructionFollowing)": "28%",
    "AIME 2025(CompetitionMath)": "26%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "42%",
    "BlendedUSD/1M Tokens": "$0.45",
    "InputPriceUSD/1M Tokens": "$0.30",
    "OutputPriceUSD/1M Tokens": "$0.90",
    "MedianTokens/s": "29",
    "P5Tokens/s": "14",
    "P25Tokens/s": "19",
    "P75Tokens/s": "94",
    "P95Tokens/s": "103",
    "LatencyFirst Answer Chunk (s)": "0.91",
    "FirstAnswerToken (s)": "0.91",
    "P5First Chunk (s)": "0.38",
    "P25First Chunk (s)": "0.50",
    "P75First Chunk (s)": "63.64",
    "P95First Chunk (s)": "135.42",
    "TotalResponse (s)": "18.04",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "EXAONE 4.0 32B",
    "ContextWindow": "131k",
    "Creator": "LG AI Research",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "17",
    "ArtificialAnalysisOmniscience Index": "\u221261",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "4%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "4%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "17%",
    "AA-LCR(LongContext Reasoning)": "14%",
    "AA-OmniscienceAccuracy(Knowledge)": "13%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "14%",
    "Humanity's LastExam(Reasoning & Knowledge)": "11%",
    "GPQA Diamond(ScientificReasoning)": "74%",
    "LiveCodeBench(Coding)": "75%",
    "SciCode(Coding)": "34%",
    "IFBench(InstructionFollowing)": "36%",
    "AIME 2025(CompetitionMath)": "80%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.70",
    "InputPriceUSD/1M Tokens": "$0.60",
    "OutputPriceUSD/1M Tokens": "$1.00",
    "MedianTokens/s": "97",
    "P5Tokens/s": "75",
    "P25Tokens/s": "95",
    "P75Tokens/s": "100",
    "P95Tokens/s": "122",
    "LatencyFirst Answer Chunk (s)": "0.30",
    "FirstAnswerToken (s)": "20.88",
    "P5First Chunk (s)": "0.25",
    "P25First Chunk (s)": "0.27",
    "P75First Chunk (s)": "0.34",
    "P95First Chunk (s)": "0.38",
    "TotalResponse (s)": "26.03",
    "ReasoningTime (s)": "20.58",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 VL 8B",
    "ContextWindow": "256k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "17",
    "ArtificialAnalysisOmniscience Index": "\u221254",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "11%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "4%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "23%",
    "AA-LCR(LongContext Reasoning)": "31%",
    "AA-OmniscienceAccuracy(Knowledge)": "19%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "10%",
    "Humanity's LastExam(Reasoning & Knowledge)": "3%",
    "GPQA Diamond(ScientificReasoning)": "58%",
    "LiveCodeBench(Coding)": "35%",
    "SciCode(Coding)": "22%",
    "IFBench(InstructionFollowing)": "40%",
    "AIME 2025(CompetitionMath)": "31%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "57%",
    "BlendedUSD/1M Tokens": "$0.66",
    "InputPriceUSD/1M Tokens": "$0.18",
    "OutputPriceUSD/1M Tokens": "$2.10",
    "MedianTokens/s": "122",
    "P5Tokens/s": "109",
    "P25Tokens/s": "121",
    "P75Tokens/s": "123",
    "P95Tokens/s": "128",
    "LatencyFirst Answer Chunk (s)": "1.07",
    "FirstAnswerToken (s)": "17.42",
    "P5First Chunk (s)": "1.00",
    "P25First Chunk (s)": "1.03",
    "P75First Chunk (s)": "1.22",
    "P95First Chunk (s)": "1.42",
    "TotalResponse (s)": "21.51",
    "ReasoningTime (s)": "16.36",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Nova 2.0 Omni",
    "ContextWindow": "1m",
    "Creator": "Amazon",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "17",
    "ArtificialAnalysisOmniscience Index": "\u221265",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "7%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "45%",
    "AA-LCR(LongContext Reasoning)": "22%",
    "AA-OmniscienceAccuracy(Knowledge)": "12%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "11%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "56%",
    "LiveCodeBench(Coding)": "31%",
    "SciCode(Coding)": "28%",
    "IFBench(InstructionFollowing)": "41%",
    "AIME 2025(CompetitionMath)": "37%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "50%",
    "BlendedUSD/1M Tokens": "$0.85",
    "InputPriceUSD/1M Tokens": "$0.30",
    "OutputPriceUSD/1M Tokens": "$2.50",
    "MedianTokens/s": "206",
    "P5Tokens/s": "195",
    "P25Tokens/s": "200",
    "P75Tokens/s": "217",
    "P95Tokens/s": "223",
    "LatencyFirst Answer Chunk (s)": "0.65",
    "FirstAnswerToken (s)": "0.65",
    "P5First Chunk (s)": "0.53",
    "P25First Chunk (s)": "0.56",
    "P75First Chunk (s)": "0.70",
    "P95First Chunk (s)": "0.79",
    "TotalResponse (s)": "3.08",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "DeepSeek R1 0528 Qwen3 8B",
    "ContextWindow": "33k",
    "Creator": "DeepSeek",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "16E",
    "ArtificialAnalysisOmniscience Index": "\u221265",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "2%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "0%",
    "AA-LCR(LongContext Reasoning)": "13%",
    "AA-OmniscienceAccuracy(Knowledge)": "11%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "14%",
    "Humanity's LastExam(Reasoning & Knowledge)": "6%",
    "GPQA Diamond(ScientificReasoning)": "61%",
    "LiveCodeBench(Coding)": "51%",
    "SciCode(Coding)": "20%",
    "IFBench(InstructionFollowing)": "20%",
    "AIME 2025(CompetitionMath)": "64%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 VL 30B A3B",
    "ContextWindow": "256k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "16",
    "ArtificialAnalysisOmniscience Index": "\u221264",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "3%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "6%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "19%",
    "AA-LCR(LongContext Reasoning)": "24%",
    "AA-OmniscienceAccuracy(Knowledge)": "15%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "8%",
    "Humanity's LastExam(Reasoning & Knowledge)": "6%",
    "GPQA Diamond(ScientificReasoning)": "70%",
    "LiveCodeBench(Coding)": "48%",
    "SciCode(Coding)": "31%",
    "IFBench(InstructionFollowing)": "33%",
    "AIME 2025(CompetitionMath)": "72%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "62%",
    "BlendedUSD/1M Tokens": "$0.35",
    "InputPriceUSD/1M Tokens": "$0.20",
    "OutputPriceUSD/1M Tokens": "$0.80",
    "MedianTokens/s": "104",
    "P5Tokens/s": "100",
    "P25Tokens/s": "103",
    "P75Tokens/s": "107",
    "P95Tokens/s": "110",
    "LatencyFirst Answer Chunk (s)": "1.03",
    "FirstAnswerToken (s)": "1.03",
    "P5First Chunk (s)": "0.96",
    "P25First Chunk (s)": "0.99",
    "P75First Chunk (s)": "1.25",
    "P95First Chunk (s)": "1.30",
    "TotalResponse (s)": "5.83",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Hermes 4 70B",
    "ContextWindow": "128k",
    "Creator": "Nous Research",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "16",
    "ArtificialAnalysisOmniscience Index": "\u221251",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "5%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "5%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "23%",
    "AA-LCR(LongContext Reasoning)": "7%",
    "AA-OmniscienceAccuracy(Knowledge)": "22%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "6%",
    "Humanity's LastExam(Reasoning & Knowledge)": "8%",
    "GPQA Diamond(ScientificReasoning)": "70%",
    "LiveCodeBench(Coding)": "65%",
    "SciCode(Coding)": "34%",
    "IFBench(InstructionFollowing)": "31%",
    "AIME 2025(CompetitionMath)": "69%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.20",
    "InputPriceUSD/1M Tokens": "$0.13",
    "OutputPriceUSD/1M Tokens": "$0.40",
    "MedianTokens/s": "83",
    "P5Tokens/s": "80",
    "P25Tokens/s": "81",
    "P75Tokens/s": "85",
    "P95Tokens/s": "86",
    "LatencyFirst Answer Chunk (s)": "0.63",
    "FirstAnswerToken (s)": "24.77",
    "P5First Chunk (s)": "0.57",
    "P25First Chunk (s)": "0.60",
    "P75First Chunk (s)": "0.67",
    "P95First Chunk (s)": "1.92",
    "TotalResponse (s)": "30.80",
    "ReasoningTime (s)": "24.14",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Ministral 3 14B",
    "ContextWindow": "256k",
    "Creator": "Mistral",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "16",
    "ArtificialAnalysisOmniscience Index": "\u221267",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "11%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "5%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "27%",
    "AA-LCR(LongContext Reasoning)": "22%",
    "AA-OmniscienceAccuracy(Knowledge)": "12%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "10%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "57%",
    "LiveCodeBench(Coding)": "35%",
    "SciCode(Coding)": "24%",
    "IFBench(InstructionFollowing)": "32%",
    "AIME 2025(CompetitionMath)": "30%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "50%",
    "BlendedUSD/1M Tokens": "$0.20",
    "InputPriceUSD/1M Tokens": "$0.20",
    "OutputPriceUSD/1M Tokens": "$0.20",
    "MedianTokens/s": "140",
    "P5Tokens/s": "107",
    "P25Tokens/s": "127",
    "P75Tokens/s": "148",
    "P95Tokens/s": "153",
    "LatencyFirst Answer Chunk (s)": "0.28",
    "FirstAnswerToken (s)": "0.28",
    "P5First Chunk (s)": "0.26",
    "P25First Chunk (s)": "0.27",
    "P75First Chunk (s)": "0.31",
    "P95First Chunk (s)": "0.45",
    "TotalResponse (s)": "3.86",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "DeepSeek R1 Distill Llama 70B",
    "ContextWindow": "128k",
    "Creator": "DeepSeek",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "16E",
    "ArtificialAnalysisOmniscience Index": "\u221247",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "2%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "22%",
    "AA-LCR(LongContext Reasoning)": "11%",
    "AA-OmniscienceAccuracy(Knowledge)": "19%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "19%",
    "Humanity's LastExam(Reasoning & Knowledge)": "6%",
    "GPQA Diamond(ScientificReasoning)": "40%",
    "LiveCodeBench(Coding)": "27%",
    "SciCode(Coding)": "31%",
    "IFBench(InstructionFollowing)": "28%",
    "AIME 2025(CompetitionMath)": "54%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.88",
    "InputPriceUSD/1M Tokens": "$0.70",
    "OutputPriceUSD/1M Tokens": "$1.05",
    "MedianTokens/s": "56",
    "P5Tokens/s": "23",
    "P25Tokens/s": "24",
    "P75Tokens/s": "297",
    "P95Tokens/s": "381",
    "LatencyFirst Answer Chunk (s)": "0.91",
    "FirstAnswerToken (s)": "36.94",
    "P5First Chunk (s)": "0.44",
    "P25First Chunk (s)": "0.52",
    "P75First Chunk (s)": "1.10",
    "P95First Chunk (s)": "3.28",
    "TotalResponse (s)": "45.95",
    "ReasoningTime (s)": "36.03",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Falcon-H1R-7B",
    "ContextWindow": "256k",
    "Creator": "TII UAE",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "16",
    "ArtificialAnalysisOmniscience Index": "\u221262",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "2%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "28%",
    "AA-LCR(LongContext Reasoning)": "9%",
    "AA-OmniscienceAccuracy(Knowledge)": "14%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "13%",
    "Humanity's LastExam(Reasoning & Knowledge)": "11%",
    "GPQA Diamond(ScientificReasoning)": "66%",
    "LiveCodeBench(Coding)": "72%",
    "SciCode(Coding)": "25%",
    "IFBench(InstructionFollowing)": "54%",
    "AIME 2025(CompetitionMath)": "80%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 Omni 30B A3B",
    "ContextWindow": "66k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "16",
    "ArtificialAnalysisOmniscience Index": "\u221262",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "3%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "4%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "21%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "14%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "12%",
    "Humanity's LastExam(Reasoning & Knowledge)": "7%",
    "GPQA Diamond(ScientificReasoning)": "73%",
    "LiveCodeBench(Coding)": "68%",
    "SciCode(Coding)": "31%",
    "IFBench(InstructionFollowing)": "43%",
    "AIME 2025(CompetitionMath)": "74%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "60%",
    "BlendedUSD/1M Tokens": "$0.43",
    "InputPriceUSD/1M Tokens": "$0.25",
    "OutputPriceUSD/1M Tokens": "$0.97",
    "MedianTokens/s": "97",
    "P5Tokens/s": "61",
    "P25Tokens/s": "85",
    "P75Tokens/s": "99",
    "P95Tokens/s": "102",
    "LatencyFirst Answer Chunk (s)": "1.05",
    "FirstAnswerToken (s)": "21.77",
    "P5First Chunk (s)": "0.94",
    "P25First Chunk (s)": "0.97",
    "P75First Chunk (s)": "1.16",
    "P95First Chunk (s)": "1.31",
    "TotalResponse (s)": "26.94",
    "ReasoningTime (s)": "20.71",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Ling-flash-2.0",
    "ContextWindow": "128k",
    "Creator": "InclusionAI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "15",
    "ArtificialAnalysisOmniscience Index": "\u221267",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "11%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "21%",
    "AA-LCR(LongContext Reasoning)": "15%",
    "AA-OmniscienceAccuracy(Knowledge)": "14%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "6%",
    "Humanity's LastExam(Reasoning & Knowledge)": "6%",
    "GPQA Diamond(ScientificReasoning)": "66%",
    "LiveCodeBench(Coding)": "59%",
    "SciCode(Coding)": "29%",
    "IFBench(InstructionFollowing)": "34%",
    "AIME 2025(CompetitionMath)": "65%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.25",
    "InputPriceUSD/1M Tokens": "$0.14",
    "OutputPriceUSD/1M Tokens": "$0.57",
    "MedianTokens/s": "58",
    "P5Tokens/s": "53",
    "P25Tokens/s": "55",
    "P75Tokens/s": "60",
    "P95Tokens/s": "63",
    "LatencyFirst Answer Chunk (s)": "1.45",
    "FirstAnswerToken (s)": "1.45",
    "P5First Chunk (s)": "0.73",
    "P25First Chunk (s)": "1.41",
    "P75First Chunk (s)": "1.56",
    "P95First Chunk (s)": "3.70",
    "TotalResponse (s)": "10.13",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Step3 VL 10B",
    "ContextWindow": "66k",
    "Creator": "StepFun",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "15",
    "ArtificialAnalysisOmniscience Index": "\u221260",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "5%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "16%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "12%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "18%",
    "Humanity's LastExam(Reasoning & Knowledge)": "10%",
    "GPQA Diamond(ScientificReasoning)": "69%",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "31%",
    "IFBench(InstructionFollowing)": "50%",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "64%",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Mistral Small 3.2",
    "ContextWindow": "128k",
    "Creator": "Mistral",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "15",
    "ArtificialAnalysisOmniscience Index": "\u221251",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "7%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "30%",
    "AA-LCR(LongContext Reasoning)": "17%",
    "AA-OmniscienceAccuracy(Knowledge)": "14%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "24%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "51%",
    "LiveCodeBench(Coding)": "28%",
    "SciCode(Coding)": "26%",
    "IFBench(InstructionFollowing)": "34%",
    "AIME 2025(CompetitionMath)": "27%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "48%",
    "BlendedUSD/1M Tokens": "$0.15",
    "InputPriceUSD/1M Tokens": "$0.10",
    "OutputPriceUSD/1M Tokens": "$0.30",
    "MedianTokens/s": "118",
    "P5Tokens/s": "84",
    "P25Tokens/s": "113",
    "P75Tokens/s": "127",
    "P95Tokens/s": "169",
    "LatencyFirst Answer Chunk (s)": "0.29",
    "FirstAnswerToken (s)": "0.29",
    "P5First Chunk (s)": "0.25",
    "P25First Chunk (s)": "0.27",
    "P75First Chunk (s)": "0.33",
    "P95First Chunk (s)": "0.51",
    "TotalResponse (s)": "4.52",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Llama Nemotron Ultra",
    "ContextWindow": "128k",
    "Creator": "NVIDIA",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "15",
    "ArtificialAnalysisOmniscience Index": "\u221246",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "2%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "11%",
    "AA-LCR(LongContext Reasoning)": "7%",
    "AA-OmniscienceAccuracy(Knowledge)": "19%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "19%",
    "Humanity's LastExam(Reasoning & Knowledge)": "8%",
    "GPQA Diamond(ScientificReasoning)": "73%",
    "LiveCodeBench(Coding)": "64%",
    "SciCode(Coding)": "35%",
    "IFBench(InstructionFollowing)": "38%",
    "AIME 2025(CompetitionMath)": "64%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.90",
    "InputPriceUSD/1M Tokens": "$0.60",
    "OutputPriceUSD/1M Tokens": "$1.80",
    "MedianTokens/s": "37",
    "P5Tokens/s": "34",
    "P25Tokens/s": "36",
    "P75Tokens/s": "38",
    "P95Tokens/s": "38",
    "LatencyFirst Answer Chunk (s)": "0.71",
    "FirstAnswerToken (s)": "54.67",
    "P5First Chunk (s)": "0.66",
    "P25First Chunk (s)": "0.68",
    "P75First Chunk (s)": "0.74",
    "P95First Chunk (s)": "0.93",
    "TotalResponse (s)": "68.15",
    "ReasoningTime (s)": "53.96",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 30B A3B 2507",
    "ContextWindow": "262k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "15",
    "ArtificialAnalysisOmniscience Index": "\u221267",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "4%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "6%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "10%",
    "AA-LCR(LongContext Reasoning)": "23%",
    "AA-OmniscienceAccuracy(Knowledge)": "14%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "5%",
    "Humanity's LastExam(Reasoning & Knowledge)": "7%",
    "GPQA Diamond(ScientificReasoning)": "66%",
    "LiveCodeBench(Coding)": "52%",
    "SciCode(Coding)": "30%",
    "IFBench(InstructionFollowing)": "33%",
    "AIME 2025(CompetitionMath)": "66%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.35",
    "InputPriceUSD/1M Tokens": "$0.20",
    "OutputPriceUSD/1M Tokens": "$0.80",
    "MedianTokens/s": "62",
    "P5Tokens/s": "18",
    "P25Tokens/s": "44",
    "P75Tokens/s": "66",
    "P95Tokens/s": "81",
    "LatencyFirst Answer Chunk (s)": "1.05",
    "FirstAnswerToken (s)": "1.05",
    "P5First Chunk (s)": "0.98",
    "P25First Chunk (s)": "1.02",
    "P75First Chunk (s)": "1.18",
    "P95First Chunk (s)": "3.28",
    "TotalResponse (s)": "9.12",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "ERNIE 4.5 300B A47B",
    "ContextWindow": "131k",
    "Creator": "Baidu",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "15",
    "ArtificialAnalysisOmniscience Index": "\u221237",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "6%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "0%",
    "AA-LCR(LongContext Reasoning)": "2%",
    "AA-OmniscienceAccuracy(Knowledge)": "18%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "33%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "81%",
    "LiveCodeBench(Coding)": "47%",
    "SciCode(Coding)": "32%",
    "IFBench(InstructionFollowing)": "39%",
    "AIME 2025(CompetitionMath)": "41%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.48",
    "InputPriceUSD/1M Tokens": "$0.28",
    "OutputPriceUSD/1M Tokens": "$1.10",
    "MedianTokens/s": "28",
    "P5Tokens/s": "20",
    "P25Tokens/s": "23",
    "P75Tokens/s": "37",
    "P95Tokens/s": "40",
    "LatencyFirst Answer Chunk (s)": "1.78",
    "FirstAnswerToken (s)": "1.78",
    "P5First Chunk (s)": "1.30",
    "P25First Chunk (s)": "1.48",
    "P75First Chunk (s)": "2.06",
    "P95First Chunk (s)": "3.00",
    "TotalResponse (s)": "19.71",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Solar Pro 2",
    "ContextWindow": "66k",
    "Creator": "Upstage",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "15",
    "ArtificialAnalysisOmniscience Index": "\u221258",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "1%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "3%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "28%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "19%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "7%",
    "Humanity's LastExam(Reasoning & Knowledge)": "7%",
    "GPQA Diamond(ScientificReasoning)": "69%",
    "LiveCodeBench(Coding)": "62%",
    "SciCode(Coding)": "30%",
    "IFBench(InstructionFollowing)": "37%",
    "AIME 2025(CompetitionMath)": "61%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "NVIDIA Nemotron Nano 12B v2 VL",
    "ContextWindow": "128k",
    "Creator": "NVIDIA",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "15",
    "ArtificialAnalysisOmniscience Index": "\u221266",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "5%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "21%",
    "AA-LCR(LongContext Reasoning)": "40%",
    "AA-OmniscienceAccuracy(Knowledge)": "13%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "8%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "57%",
    "LiveCodeBench(Coding)": "69%",
    "SciCode(Coding)": "26%",
    "IFBench(InstructionFollowing)": "32%",
    "AIME 2025(CompetitionMath)": "75%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "53%",
    "BlendedUSD/1M Tokens": "$0.30",
    "InputPriceUSD/1M Tokens": "$0.20",
    "OutputPriceUSD/1M Tokens": "$0.60",
    "MedianTokens/s": "128",
    "P5Tokens/s": "124",
    "P25Tokens/s": "126",
    "P75Tokens/s": "129",
    "P95Tokens/s": "131",
    "LatencyFirst Answer Chunk (s)": "0.23",
    "FirstAnswerToken (s)": "15.87",
    "P5First Chunk (s)": "0.18",
    "P25First Chunk (s)": "0.21",
    "P75First Chunk (s)": "0.37",
    "P95First Chunk (s)": "0.46",
    "TotalResponse (s)": "19.79",
    "ReasoningTime (s)": "15.65",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "NVIDIA Nemotron Nano 9B V2",
    "ContextWindow": "131k",
    "Creator": "NVIDIA",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "15",
    "ArtificialAnalysisOmniscience Index": "\u221243",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "1%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "2%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "22%",
    "AA-LCR(LongContext Reasoning)": "21%",
    "AA-OmniscienceAccuracy(Knowledge)": "11%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "40%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "57%",
    "LiveCodeBench(Coding)": "72%",
    "SciCode(Coding)": "22%",
    "IFBench(InstructionFollowing)": "28%",
    "AIME 2025(CompetitionMath)": "70%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.07",
    "InputPriceUSD/1M Tokens": "$0.04",
    "OutputPriceUSD/1M Tokens": "$0.16",
    "MedianTokens/s": "106",
    "P5Tokens/s": "75",
    "P25Tokens/s": "97",
    "P75Tokens/s": "123",
    "P95Tokens/s": "125",
    "LatencyFirst Answer Chunk (s)": "0.22",
    "FirstAnswerToken (s)": "19.10",
    "P5First Chunk (s)": "0.16",
    "P25First Chunk (s)": "0.18",
    "P75First Chunk (s)": "0.23",
    "P95First Chunk (s)": "0.30",
    "TotalResponse (s)": "23.82",
    "ReasoningTime (s)": "18.88",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Ministral 3 8B",
    "ContextWindow": "256k",
    "Creator": "Mistral",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "15",
    "ArtificialAnalysisOmniscience Index": "\u221270",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "11%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "5%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "27%",
    "AA-LCR(LongContext Reasoning)": "24%",
    "AA-OmniscienceAccuracy(Knowledge)": "12%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "7%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "47%",
    "LiveCodeBench(Coding)": "30%",
    "SciCode(Coding)": "21%",
    "IFBench(InstructionFollowing)": "29%",
    "AIME 2025(CompetitionMath)": "32%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "46%",
    "BlendedUSD/1M Tokens": "$0.15",
    "InputPriceUSD/1M Tokens": "$0.15",
    "OutputPriceUSD/1M Tokens": "$0.15",
    "MedianTokens/s": "186",
    "P5Tokens/s": "98",
    "P25Tokens/s": "162",
    "P75Tokens/s": "200",
    "P95Tokens/s": "207",
    "LatencyFirst Answer Chunk (s)": "0.27",
    "FirstAnswerToken (s)": "0.27",
    "P5First Chunk (s)": "0.25",
    "P25First Chunk (s)": "0.26",
    "P75First Chunk (s)": "0.33",
    "P95First Chunk (s)": "1.06",
    "TotalResponse (s)": "2.96",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Llama Nemotron Super 49B v1.5",
    "ContextWindow": "128k",
    "Creator": "NVIDIA",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "15",
    "ArtificialAnalysisOmniscience Index": "\u221247",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "4%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "25%",
    "AA-LCR(LongContext Reasoning)": "22%",
    "AA-OmniscienceAccuracy(Knowledge)": "12%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "34%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "48%",
    "LiveCodeBench(Coding)": "29%",
    "SciCode(Coding)": "24%",
    "IFBench(InstructionFollowing)": "33%",
    "AIME 2025(CompetitionMath)": "8%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.17",
    "InputPriceUSD/1M Tokens": "$0.10",
    "OutputPriceUSD/1M Tokens": "$0.40",
    "MedianTokens/s": "69",
    "P5Tokens/s": "65",
    "P25Tokens/s": "67",
    "P75Tokens/s": "72",
    "P95Tokens/s": "75",
    "LatencyFirst Answer Chunk (s)": "0.24",
    "FirstAnswerToken (s)": "0.24",
    "P5First Chunk (s)": "0.19",
    "P25First Chunk (s)": "0.20",
    "P75First Chunk (s)": "0.43",
    "P95First Chunk (s)": "0.47",
    "TotalResponse (s)": "7.51",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "K2-V2 (low)",
    "ContextWindow": "512k",
    "Creator": "MBZUAI Institute of Foundation Models",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "14",
    "ArtificialAnalysisOmniscience Index": "\u221249",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "5%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "21%",
    "AA-LCR(LongContext Reasoning)": "19%",
    "AA-OmniscienceAccuracy(Knowledge)": "15%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "24%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "54%",
    "LiveCodeBench(Coding)": "39%",
    "SciCode(Coding)": "22%",
    "IFBench(InstructionFollowing)": "41%",
    "AIME 2025(CompetitionMath)": "35%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Llama 3.1 Nemotron Nano 4B v1.1",
    "ContextWindow": "128k",
    "Creator": "NVIDIA",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "14E",
    "ArtificialAnalysisOmniscience Index": "--",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "--",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "12%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "--",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "--",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "41%",
    "LiveCodeBench(Coding)": "49%",
    "SciCode(Coding)": "10%",
    "IFBench(InstructionFollowing)": "26%",
    "AIME 2025(CompetitionMath)": "50%",
    "CritPt(PhysicsReasoning)": "--",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Kimi Linear 48B A3B Instruct",
    "ContextWindow": "1m",
    "Creator": "Kimi",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "14E",
    "ArtificialAnalysisOmniscience Index": "--",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "11%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "0%",
    "AA-LCR(LongContext Reasoning)": "26%",
    "AA-OmniscienceAccuracy(Knowledge)": "--",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "--",
    "Humanity's LastExam(Reasoning & Knowledge)": "3%",
    "GPQA Diamond(ScientificReasoning)": "41%",
    "LiveCodeBench(Coding)": "38%",
    "SciCode(Coding)": "20%",
    "IFBench(InstructionFollowing)": "28%",
    "AIME 2025(CompetitionMath)": "36%",
    "CritPt(PhysicsReasoning)": "--",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Llama 3.3 Nemotron Super 49B",
    "ContextWindow": "128k",
    "Creator": "NVIDIA",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "14E",
    "ArtificialAnalysisOmniscience Index": "\u221251",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "--",
    "AA-LCR(LongContext Reasoning)": "11%",
    "AA-OmniscienceAccuracy(Knowledge)": "13%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "27%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "52%",
    "LiveCodeBench(Coding)": "28%",
    "SciCode(Coding)": "23%",
    "IFBench(InstructionFollowing)": "40%",
    "AIME 2025(CompetitionMath)": "8%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 VL 8B",
    "ContextWindow": "256k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "14",
    "ArtificialAnalysisOmniscience Index": "\u221254",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "12%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "2%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "29%",
    "AA-LCR(LongContext Reasoning)": "15%",
    "AA-OmniscienceAccuracy(Knowledge)": "19%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "10%",
    "Humanity's LastExam(Reasoning & Knowledge)": "3%",
    "GPQA Diamond(ScientificReasoning)": "43%",
    "LiveCodeBench(Coding)": "33%",
    "SciCode(Coding)": "17%",
    "IFBench(InstructionFollowing)": "32%",
    "AIME 2025(CompetitionMath)": "27%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "47%",
    "BlendedUSD/1M Tokens": "$0.31",
    "InputPriceUSD/1M Tokens": "$0.18",
    "OutputPriceUSD/1M Tokens": "$0.70",
    "MedianTokens/s": "119",
    "P5Tokens/s": "112",
    "P25Tokens/s": "116",
    "P75Tokens/s": "123",
    "P95Tokens/s": "127",
    "LatencyFirst Answer Chunk (s)": "1.05",
    "FirstAnswerToken (s)": "1.05",
    "P5First Chunk (s)": "0.98",
    "P25First Chunk (s)": "1.01",
    "P75First Chunk (s)": "1.16",
    "P95First Chunk (s)": "1.27",
    "TotalResponse (s)": "5.25",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Olmo 3.1 32B Think",
    "ContextWindow": "66k",
    "Creator": "Allen Institute for AI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "14",
    "ArtificialAnalysisOmniscience Index": "\u221239",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "0%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "14%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "37%",
    "Humanity's LastExam(Reasoning & Knowledge)": "6%",
    "GPQA Diamond(ScientificReasoning)": "59%",
    "LiveCodeBench(Coding)": "70%",
    "SciCode(Coding)": "29%",
    "IFBench(InstructionFollowing)": "66%",
    "AIME 2025(CompetitionMath)": "77%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "101",
    "P5Tokens/s": "82",
    "P25Tokens/s": "93",
    "P75Tokens/s": "107",
    "P95Tokens/s": "109",
    "LatencyFirst Answer Chunk (s)": "0.44",
    "FirstAnswerToken (s)": "20.19",
    "P5First Chunk (s)": "0.34",
    "P25First Chunk (s)": "0.42",
    "P75First Chunk (s)": "0.46",
    "P95First Chunk (s)": "0.52",
    "TotalResponse (s)": "25.13",
    "ReasoningTime (s)": "19.75",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Llama 3.3 70B",
    "ContextWindow": "128k",
    "Creator": "Meta",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "14",
    "ArtificialAnalysisOmniscience Index": "\u221255",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "3%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "27%",
    "AA-LCR(LongContext Reasoning)": "15%",
    "AA-OmniscienceAccuracy(Knowledge)": "18%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "11%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "50%",
    "LiveCodeBench(Coding)": "29%",
    "SciCode(Coding)": "26%",
    "IFBench(InstructionFollowing)": "47%",
    "AIME 2025(CompetitionMath)": "8%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.68",
    "InputPriceUSD/1M Tokens": "$0.58",
    "OutputPriceUSD/1M Tokens": "$0.71",
    "MedianTokens/s": "98",
    "P5Tokens/s": "20",
    "P25Tokens/s": "45",
    "P75Tokens/s": "154",
    "P95Tokens/s": "362",
    "LatencyFirst Answer Chunk (s)": "0.49",
    "FirstAnswerToken (s)": "0.49",
    "P5First Chunk (s)": "0.19",
    "P25First Chunk (s)": "0.34",
    "P75First Chunk (s)": "0.74",
    "P95First Chunk (s)": "1.62",
    "TotalResponse (s)": "5.60",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Llama 3.1 405B",
    "ContextWindow": "128k",
    "Creator": "Meta",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "14",
    "ArtificialAnalysisOmniscience Index": "\u221218",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "7%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "19%",
    "AA-LCR(LongContext Reasoning)": "24%",
    "AA-OmniscienceAccuracy(Knowledge)": "22%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "49%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "52%",
    "LiveCodeBench(Coding)": "31%",
    "SciCode(Coding)": "30%",
    "IFBench(InstructionFollowing)": "39%",
    "AIME 2025(CompetitionMath)": "3%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$4.19",
    "InputPriceUSD/1M Tokens": "$3.75",
    "OutputPriceUSD/1M Tokens": "$6.75",
    "MedianTokens/s": "25",
    "P5Tokens/s": "6",
    "P25Tokens/s": "14",
    "P75Tokens/s": "36",
    "P95Tokens/s": "74",
    "LatencyFirst Answer Chunk (s)": "0.79",
    "FirstAnswerToken (s)": "0.79",
    "P5First Chunk (s)": "0.42",
    "P25First Chunk (s)": "0.46",
    "P75First Chunk (s)": "1.89",
    "P95First Chunk (s)": "3.45",
    "TotalResponse (s)": "20.74",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Ring-flash-2.0",
    "ContextWindow": "128k",
    "Creator": "InclusionAI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "14",
    "ArtificialAnalysisOmniscience Index": "\u221260",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "8%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "0%",
    "AA-LCR(LongContext Reasoning)": "21%",
    "AA-OmniscienceAccuracy(Knowledge)": "16%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "11%",
    "Humanity's LastExam(Reasoning & Knowledge)": "9%",
    "GPQA Diamond(ScientificReasoning)": "73%",
    "LiveCodeBench(Coding)": "63%",
    "SciCode(Coding)": "17%",
    "IFBench(InstructionFollowing)": "43%",
    "AIME 2025(CompetitionMath)": "84%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.25",
    "InputPriceUSD/1M Tokens": "$0.14",
    "OutputPriceUSD/1M Tokens": "$0.57",
    "MedianTokens/s": "81",
    "P5Tokens/s": "70",
    "P25Tokens/s": "77",
    "P75Tokens/s": "85",
    "P95Tokens/s": "87",
    "LatencyFirst Answer Chunk (s)": "1.34",
    "FirstAnswerToken (s)": "26.10",
    "P5First Chunk (s)": "1.20",
    "P25First Chunk (s)": "1.28",
    "P75First Chunk (s)": "1.79",
    "P95First Chunk (s)": "6.65",
    "TotalResponse (s)": "32.29",
    "ReasoningTime (s)": "24.76",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 VL 4B",
    "ContextWindow": "256k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "14",
    "ArtificialAnalysisOmniscience Index": "\u221270",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "14%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "2%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "16%",
    "AA-LCR(LongContext Reasoning)": "21%",
    "AA-OmniscienceAccuracy(Knowledge)": "12%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "8%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "49%",
    "LiveCodeBench(Coding)": "32%",
    "SciCode(Coding)": "17%",
    "IFBench(InstructionFollowing)": "37%",
    "AIME 2025(CompetitionMath)": "26%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "52%",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Hermes 4 70B",
    "ContextWindow": "128k",
    "Creator": "Nous Research",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "14E",
    "ArtificialAnalysisOmniscience Index": "\u221250",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "22%",
    "AA-LCR(LongContext Reasoning)": "2%",
    "AA-OmniscienceAccuracy(Knowledge)": "18%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "17%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "49%",
    "LiveCodeBench(Coding)": "27%",
    "SciCode(Coding)": "28%",
    "IFBench(InstructionFollowing)": "29%",
    "AIME 2025(CompetitionMath)": "11%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.20",
    "InputPriceUSD/1M Tokens": "$0.13",
    "OutputPriceUSD/1M Tokens": "$0.40",
    "MedianTokens/s": "77",
    "P5Tokens/s": "70",
    "P25Tokens/s": "74",
    "P75Tokens/s": "79",
    "P95Tokens/s": "82",
    "LatencyFirst Answer Chunk (s)": "0.60",
    "FirstAnswerToken (s)": "0.60",
    "P5First Chunk (s)": "0.55",
    "P25First Chunk (s)": "0.58",
    "P75First Chunk (s)": "0.72",
    "P95First Chunk (s)": "1.54",
    "TotalResponse (s)": "7.12",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Solar Pro 2",
    "ContextWindow": "66k",
    "Creator": "Upstage",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "14",
    "ArtificialAnalysisOmniscience Index": "\u221263",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "1%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "5%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "32%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "15%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "8%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "56%",
    "LiveCodeBench(Coding)": "42%",
    "SciCode(Coding)": "25%",
    "IFBench(InstructionFollowing)": "34%",
    "AIME 2025(CompetitionMath)": "30%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Llama 4 Scout",
    "ContextWindow": "10m",
    "Creator": "Meta",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "13",
    "ArtificialAnalysisOmniscience Index": "\u221253",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "2%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "16%",
    "AA-LCR(LongContext Reasoning)": "26%",
    "AA-OmniscienceAccuracy(Knowledge)": "14%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "21%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "59%",
    "LiveCodeBench(Coding)": "30%",
    "SciCode(Coding)": "17%",
    "IFBench(InstructionFollowing)": "40%",
    "AIME 2025(CompetitionMath)": "14%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "53%",
    "BlendedUSD/1M Tokens": "$0.29",
    "InputPriceUSD/1M Tokens": "$0.18",
    "OutputPriceUSD/1M Tokens": "$0.63",
    "MedianTokens/s": "154",
    "P5Tokens/s": "47",
    "P25Tokens/s": "95",
    "P75Tokens/s": "194",
    "P95Tokens/s": "422",
    "LatencyFirst Answer Chunk (s)": "0.48",
    "FirstAnswerToken (s)": "0.48",
    "P5First Chunk (s)": "0.15",
    "P25First Chunk (s)": "0.28",
    "P75First Chunk (s)": "0.77",
    "P95First Chunk (s)": "4.09",
    "TotalResponse (s)": "3.72",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Command A",
    "ContextWindow": "256k",
    "Creator": "Cohere",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "13",
    "ArtificialAnalysisOmniscience Index": "\u221250",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "1%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "15%",
    "AA-LCR(LongContext Reasoning)": "18%",
    "AA-OmniscienceAccuracy(Knowledge)": "15%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "24%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "53%",
    "LiveCodeBench(Coding)": "29%",
    "SciCode(Coding)": "28%",
    "IFBench(InstructionFollowing)": "37%",
    "AIME 2025(CompetitionMath)": "13%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$4.38",
    "InputPriceUSD/1M Tokens": "$2.50",
    "OutputPriceUSD/1M Tokens": "$10.00",
    "MedianTokens/s": "57",
    "P5Tokens/s": "41",
    "P25Tokens/s": "50",
    "P75Tokens/s": "64",
    "P95Tokens/s": "72",
    "LatencyFirst Answer Chunk (s)": "0.27",
    "FirstAnswerToken (s)": "0.27",
    "P5First Chunk (s)": "0.23",
    "P25First Chunk (s)": "0.25",
    "P75First Chunk (s)": "0.47",
    "P95First Chunk (s)": "1.40",
    "TotalResponse (s)": "9.06",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Llama 3.1 Nemotron 70B",
    "ContextWindow": "128k",
    "Creator": "NVIDIA",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "13",
    "ArtificialAnalysisOmniscience Index": "\u221241",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "5%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "23%",
    "AA-LCR(LongContext Reasoning)": "7%",
    "AA-OmniscienceAccuracy(Knowledge)": "16%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "31%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "47%",
    "LiveCodeBench(Coding)": "17%",
    "SciCode(Coding)": "23%",
    "IFBench(InstructionFollowing)": "31%",
    "AIME 2025(CompetitionMath)": "11%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$1.20",
    "InputPriceUSD/1M Tokens": "$1.20",
    "OutputPriceUSD/1M Tokens": "$1.20",
    "MedianTokens/s": "30",
    "P5Tokens/s": "25",
    "P25Tokens/s": "29",
    "P75Tokens/s": "31",
    "P95Tokens/s": "31",
    "LatencyFirst Answer Chunk (s)": "0.33",
    "FirstAnswerToken (s)": "0.33",
    "P5First Chunk (s)": "0.29",
    "P25First Chunk (s)": "0.31",
    "P75First Chunk (s)": "0.34",
    "P95First Chunk (s)": "0.37",
    "TotalResponse (s)": "16.89",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "NVIDIA Nemotron 3 Nano",
    "ContextWindow": "1m",
    "Creator": "NVIDIA",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "13",
    "ArtificialAnalysisOmniscience Index": "\u221265",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "12%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "25%",
    "AA-LCR(LongContext Reasoning)": "7%",
    "AA-OmniscienceAccuracy(Knowledge)": "13%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "10%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "40%",
    "LiveCodeBench(Coding)": "36%",
    "SciCode(Coding)": "23%",
    "IFBench(InstructionFollowing)": "38%",
    "AIME 2025(CompetitionMath)": "13%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.10",
    "InputPriceUSD/1M Tokens": "$0.06",
    "OutputPriceUSD/1M Tokens": "$0.22",
    "MedianTokens/s": "164",
    "P5Tokens/s": "66",
    "P25Tokens/s": "110",
    "P75Tokens/s": "222",
    "P95Tokens/s": "403",
    "LatencyFirst Answer Chunk (s)": "0.23",
    "FirstAnswerToken (s)": "0.23",
    "P5First Chunk (s)": "0.17",
    "P25First Chunk (s)": "0.20",
    "P75First Chunk (s)": "9.73",
    "P95First Chunk (s)": "10.76",
    "TotalResponse (s)": "3.28",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 4B 2507",
    "ContextWindow": "262k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "13",
    "ArtificialAnalysisOmniscience Index": "\u221254",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "5%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "27%",
    "AA-LCR(LongContext Reasoning)": "7%",
    "AA-OmniscienceAccuracy(Knowledge)": "12%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "25%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "52%",
    "LiveCodeBench(Coding)": "38%",
    "SciCode(Coding)": "18%",
    "IFBench(InstructionFollowing)": "34%",
    "AIME 2025(CompetitionMath)": "52%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "NVIDIA Nemotron Nano 9B V2",
    "ContextWindow": "131k",
    "Creator": "NVIDIA",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "13",
    "ArtificialAnalysisOmniscience Index": "\u221258",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "1%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "23%",
    "AA-LCR(LongContext Reasoning)": "23%",
    "AA-OmniscienceAccuracy(Knowledge)": "9%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "26%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "56%",
    "LiveCodeBench(Coding)": "70%",
    "SciCode(Coding)": "21%",
    "IFBench(InstructionFollowing)": "27%",
    "AIME 2025(CompetitionMath)": "62%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.10",
    "InputPriceUSD/1M Tokens": "$0.06",
    "OutputPriceUSD/1M Tokens": "$0.23",
    "MedianTokens/s": "122",
    "P5Tokens/s": "72",
    "P25Tokens/s": "110",
    "P75Tokens/s": "165",
    "P95Tokens/s": "171",
    "LatencyFirst Answer Chunk (s)": "0.56",
    "FirstAnswerToken (s)": "0.56",
    "P5First Chunk (s)": "0.18",
    "P25First Chunk (s)": "0.23",
    "P75First Chunk (s)": "0.80",
    "P95First Chunk (s)": "1.19",
    "TotalResponse (s)": "4.65",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Olmo 3.1 32B Instruct",
    "ContextWindow": "66k",
    "Creator": "Allen Institute for AI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "12",
    "ArtificialAnalysisOmniscience Index": "\u221253",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "21%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "11%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "27%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "54%",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "17%",
    "IFBench(InstructionFollowing)": "39%",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.30",
    "InputPriceUSD/1M Tokens": "$0.20",
    "OutputPriceUSD/1M Tokens": "$0.60",
    "MedianTokens/s": "44",
    "P5Tokens/s": "41",
    "P25Tokens/s": "42",
    "P75Tokens/s": "47",
    "P95Tokens/s": "54",
    "LatencyFirst Answer Chunk (s)": "0.22",
    "FirstAnswerToken (s)": "0.22",
    "P5First Chunk (s)": "0.19",
    "P25First Chunk (s)": "0.19",
    "P75First Chunk (s)": "0.24",
    "P95First Chunk (s)": "0.27",
    "TotalResponse (s)": "11.60",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "R1 1776",
    "ContextWindow": "128k",
    "Creator": "Perplexity",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "12E",
    "ArtificialAnalysisOmniscience Index": "--",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "--",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "--",
    "AA-LCR(LongContext Reasoning)": "--",
    "AA-OmniscienceAccuracy(Knowledge)": "--",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "--",
    "Humanity's LastExam(Reasoning & Knowledge)": "--",
    "GPQA Diamond(ScientificReasoning)": "--",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "--",
    "IFBench(InstructionFollowing)": "--",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "--",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Llama 3.2 90B (Vision)",
    "ContextWindow": "128k",
    "Creator": "Meta",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "12E",
    "ArtificialAnalysisOmniscience Index": "--",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "--",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "--",
    "AA-LCR(LongContext Reasoning)": "--",
    "AA-OmniscienceAccuracy(Knowledge)": "--",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "--",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "43%",
    "LiveCodeBench(Coding)": "21%",
    "SciCode(Coding)": "24%",
    "IFBench(InstructionFollowing)": "--",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "--",
    "MMMU Pro(VisualReasoning)": "40%",
    "BlendedUSD/1M Tokens": "$0.72",
    "InputPriceUSD/1M Tokens": "$0.72",
    "OutputPriceUSD/1M Tokens": "$0.72",
    "MedianTokens/s": "48",
    "P5Tokens/s": "32",
    "P25Tokens/s": "35",
    "P75Tokens/s": "51",
    "P95Tokens/s": "72",
    "LatencyFirst Answer Chunk (s)": "0.37",
    "FirstAnswerToken (s)": "0.37",
    "P5First Chunk (s)": "0.27",
    "P25First Chunk (s)": "0.33",
    "P75First Chunk (s)": "0.43",
    "P95First Chunk (s)": "0.53",
    "TotalResponse (s)": "10.83",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "EXAONE 4.0 32B",
    "ContextWindow": "131k",
    "Creator": "LG AI Research",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "12",
    "ArtificialAnalysisOmniscience Index": "\u221264",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "2%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "4%",
    "AA-LCR(LongContext Reasoning)": "8%",
    "AA-OmniscienceAccuracy(Knowledge)": "10%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "18%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "63%",
    "LiveCodeBench(Coding)": "47%",
    "SciCode(Coding)": "25%",
    "IFBench(InstructionFollowing)": "34%",
    "AIME 2025(CompetitionMath)": "39%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.70",
    "InputPriceUSD/1M Tokens": "$0.60",
    "OutputPriceUSD/1M Tokens": "$1.00",
    "MedianTokens/s": "87",
    "P5Tokens/s": "69",
    "P25Tokens/s": "75",
    "P75Tokens/s": "90",
    "P95Tokens/s": "120",
    "LatencyFirst Answer Chunk (s)": "0.30",
    "FirstAnswerToken (s)": "0.30",
    "P5First Chunk (s)": "0.25",
    "P25First Chunk (s)": "0.28",
    "P75First Chunk (s)": "0.34",
    "P95First Chunk (s)": "0.44",
    "TotalResponse (s)": "6.03",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Ministral 3 3B",
    "ContextWindow": "256k",
    "Creator": "Mistral",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "11",
    "ArtificialAnalysisOmniscience Index": "\u221264",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "3%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "25%",
    "AA-LCR(LongContext Reasoning)": "12%",
    "AA-OmniscienceAccuracy(Knowledge)": "8%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "22%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "36%",
    "LiveCodeBench(Coding)": "25%",
    "SciCode(Coding)": "14%",
    "IFBench(InstructionFollowing)": "27%",
    "AIME 2025(CompetitionMath)": "22%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "38%",
    "BlendedUSD/1M Tokens": "$0.10",
    "InputPriceUSD/1M Tokens": "$0.10",
    "OutputPriceUSD/1M Tokens": "$0.10",
    "MedianTokens/s": "301",
    "P5Tokens/s": "259",
    "P25Tokens/s": "288",
    "P75Tokens/s": "307",
    "P95Tokens/s": "313",
    "LatencyFirst Answer Chunk (s)": "0.26",
    "FirstAnswerToken (s)": "0.26",
    "P5First Chunk (s)": "0.24",
    "P25First Chunk (s)": "0.25",
    "P75First Chunk (s)": "0.27",
    "P95First Chunk (s)": "0.34",
    "TotalResponse (s)": "1.92",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Phi-4 Mini",
    "ContextWindow": "128k",
    "Creator": "Microsoft Azure",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "11E",
    "ArtificialAnalysisOmniscience Index": "\u221263",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "8%",
    "AA-LCR(LongContext Reasoning)": "14%",
    "AA-OmniscienceAccuracy(Knowledge)": "8%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "24%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "33%",
    "LiveCodeBench(Coding)": "13%",
    "SciCode(Coding)": "11%",
    "IFBench(InstructionFollowing)": "21%",
    "AIME 2025(CompetitionMath)": "7%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "25",
    "P5Tokens/s": "11",
    "P25Tokens/s": "13",
    "P75Tokens/s": "44",
    "P95Tokens/s": "46",
    "LatencyFirst Answer Chunk (s)": "0.39",
    "FirstAnswerToken (s)": "0.39",
    "P5First Chunk (s)": "0.29",
    "P25First Chunk (s)": "0.31",
    "P75First Chunk (s)": "0.43",
    "P95First Chunk (s)": "8.41",
    "TotalResponse (s)": "20.67",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "DeepHermes 3 - Mistral 24B",
    "ContextWindow": "32k",
    "Creator": "Nous Research",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "11E",
    "ArtificialAnalysisOmniscience Index": "--",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "--",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "--",
    "AA-LCR(LongContext Reasoning)": "--",
    "AA-OmniscienceAccuracy(Knowledge)": "--",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "--",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "38%",
    "LiveCodeBench(Coding)": "20%",
    "SciCode(Coding)": "23%",
    "IFBench(InstructionFollowing)": "--",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "--",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Llama 3.2 11B (Vision)",
    "ContextWindow": "128k",
    "Creator": "Meta",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "11E",
    "ArtificialAnalysisOmniscience Index": "\u221263",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "1%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "15%",
    "AA-LCR(LongContext Reasoning)": "12%",
    "AA-OmniscienceAccuracy(Knowledge)": "10%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "20%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "22%",
    "LiveCodeBench(Coding)": "11%",
    "SciCode(Coding)": "11%",
    "IFBench(InstructionFollowing)": "30%",
    "AIME 2025(CompetitionMath)": "2%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "29%",
    "BlendedUSD/1M Tokens": "$0.16",
    "InputPriceUSD/1M Tokens": "$0.16",
    "OutputPriceUSD/1M Tokens": "$0.16",
    "MedianTokens/s": "70",
    "P5Tokens/s": "4",
    "P25Tokens/s": "32",
    "P75Tokens/s": "135",
    "P95Tokens/s": "158",
    "LatencyFirst Answer Chunk (s)": "0.41",
    "FirstAnswerToken (s)": "0.41",
    "P5First Chunk (s)": "0.29",
    "P25First Chunk (s)": "0.36",
    "P75First Chunk (s)": "0.65",
    "P95First Chunk (s)": "10.74",
    "TotalResponse (s)": "7.58",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Granite 4.0 H Small",
    "ContextWindow": "128k",
    "Creator": "IBM",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "11",
    "ArtificialAnalysisOmniscience Index": "\u221262",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "2%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "17%",
    "AA-LCR(LongContext Reasoning)": "9%",
    "AA-OmniscienceAccuracy(Knowledge)": "13%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "13%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "42%",
    "LiveCodeBench(Coding)": "25%",
    "SciCode(Coding)": "21%",
    "IFBench(InstructionFollowing)": "32%",
    "AIME 2025(CompetitionMath)": "14%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.11",
    "InputPriceUSD/1M Tokens": "$0.06",
    "OutputPriceUSD/1M Tokens": "$0.25",
    "MedianTokens/s": "454",
    "P5Tokens/s": "25",
    "P25Tokens/s": "379",
    "P75Tokens/s": "491",
    "P95Tokens/s": "651",
    "LatencyFirst Answer Chunk (s)": "8.72",
    "FirstAnswerToken (s)": "8.72",
    "P5First Chunk (s)": "8.65",
    "P25First Chunk (s)": "8.66",
    "P75First Chunk (s)": "8.89",
    "P95First Chunk (s)": "9.26",
    "TotalResponse (s)": "9.82",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 Omni 30B A3B",
    "ContextWindow": "66k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "11",
    "ArtificialAnalysisOmniscience Index": "\u221270",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "2%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "16%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "14%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "3%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "62%",
    "LiveCodeBench(Coding)": "42%",
    "SciCode(Coding)": "19%",
    "IFBench(InstructionFollowing)": "31%",
    "AIME 2025(CompetitionMath)": "52%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "56%",
    "BlendedUSD/1M Tokens": "$0.43",
    "InputPriceUSD/1M Tokens": "$0.25",
    "OutputPriceUSD/1M Tokens": "$0.97",
    "MedianTokens/s": "87",
    "P5Tokens/s": "83",
    "P25Tokens/s": "86",
    "P75Tokens/s": "93",
    "P95Tokens/s": "95",
    "LatencyFirst Answer Chunk (s)": "0.95",
    "FirstAnswerToken (s)": "0.95",
    "P5First Chunk (s)": "0.90",
    "P25First Chunk (s)": "0.91",
    "P75First Chunk (s)": "1.06",
    "P95First Chunk (s)": "1.23",
    "TotalResponse (s)": "6.67",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Phi-4",
    "ContextWindow": "16k",
    "Creator": "Microsoft Azure",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "10",
    "ArtificialAnalysisOmniscience Index": "\u221256",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "4%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "0%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "13%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "21%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "57%",
    "LiveCodeBench(Coding)": "23%",
    "SciCode(Coding)": "26%",
    "IFBench(InstructionFollowing)": "24%",
    "AIME 2025(CompetitionMath)": "18%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.22",
    "InputPriceUSD/1M Tokens": "$0.13",
    "OutputPriceUSD/1M Tokens": "$0.50",
    "MedianTokens/s": "9",
    "P5Tokens/s": "5",
    "P25Tokens/s": "6",
    "P75Tokens/s": "11",
    "P95Tokens/s": "13",
    "LatencyFirst Answer Chunk (s)": "0.63",
    "FirstAnswerToken (s)": "0.63",
    "P5First Chunk (s)": "0.45",
    "P25First Chunk (s)": "0.51",
    "P75First Chunk (s)": "1.95",
    "P95First Chunk (s)": "26.14",
    "TotalResponse (s)": "56.71",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Jamba Reasoning 3B",
    "ContextWindow": "262k",
    "Creator": "AI21 Labs",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "10",
    "ArtificialAnalysisOmniscience Index": "\u221263",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "1%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "16%",
    "AA-LCR(LongContext Reasoning)": "7%",
    "AA-OmniscienceAccuracy(Knowledge)": "7%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "26%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "33%",
    "LiveCodeBench(Coding)": "21%",
    "SciCode(Coding)": "6%",
    "IFBench(InstructionFollowing)": "52%",
    "AIME 2025(CompetitionMath)": "11%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Nova Micro",
    "ContextWindow": "130k",
    "Creator": "Amazon",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "10",
    "ArtificialAnalysisOmniscience Index": "\u221249",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "2%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "14%",
    "AA-LCR(LongContext Reasoning)": "10%",
    "AA-OmniscienceAccuracy(Knowledge)": "9%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "35%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "36%",
    "LiveCodeBench(Coding)": "14%",
    "SciCode(Coding)": "9%",
    "IFBench(InstructionFollowing)": "29%",
    "AIME 2025(CompetitionMath)": "6%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.06",
    "InputPriceUSD/1M Tokens": "$0.04",
    "OutputPriceUSD/1M Tokens": "$0.14",
    "MedianTokens/s": "435",
    "P5Tokens/s": "391",
    "P25Tokens/s": "409",
    "P75Tokens/s": "451",
    "P95Tokens/s": "488",
    "LatencyFirst Answer Chunk (s)": "0.35",
    "FirstAnswerToken (s)": "0.35",
    "P5First Chunk (s)": "0.32",
    "P25First Chunk (s)": "0.34",
    "P75First Chunk (s)": "0.36",
    "P95First Chunk (s)": "0.37",
    "TotalResponse (s)": "1.50",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Gemma 3 27B",
    "ContextWindow": "128k",
    "Creator": "Google",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "10",
    "ArtificialAnalysisOmniscience Index": "\u221268",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "4%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "11%",
    "AA-LCR(LongContext Reasoning)": "6%",
    "AA-OmniscienceAccuracy(Knowledge)": "12%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "9%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "43%",
    "LiveCodeBench(Coding)": "14%",
    "SciCode(Coding)": "21%",
    "IFBench(InstructionFollowing)": "32%",
    "AIME 2025(CompetitionMath)": "21%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "48%",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "37",
    "P5Tokens/s": "33",
    "P25Tokens/s": "35",
    "P75Tokens/s": "39",
    "P95Tokens/s": "42",
    "LatencyFirst Answer Chunk (s)": "0.80",
    "FirstAnswerToken (s)": "0.80",
    "P5First Chunk (s)": "0.62",
    "P25First Chunk (s)": "0.68",
    "P75First Chunk (s)": "4.22",
    "P95First Chunk (s)": "8.06",
    "TotalResponse (s)": "14.46",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "NVIDIA Nemotron Nano 12B v2 VL",
    "ContextWindow": "128k",
    "Creator": "NVIDIA",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "10",
    "ArtificialAnalysisOmniscience Index": "\u221273",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "19%",
    "AA-LCR(LongContext Reasoning)": "17%",
    "AA-OmniscienceAccuracy(Knowledge)": "11%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "6%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "44%",
    "LiveCodeBench(Coding)": "35%",
    "SciCode(Coding)": "18%",
    "IFBench(InstructionFollowing)": "26%",
    "AIME 2025(CompetitionMath)": "27%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "45%",
    "BlendedUSD/1M Tokens": "$0.30",
    "InputPriceUSD/1M Tokens": "$0.20",
    "OutputPriceUSD/1M Tokens": "$0.60",
    "MedianTokens/s": "133",
    "P5Tokens/s": "121",
    "P25Tokens/s": "130",
    "P75Tokens/s": "196",
    "P95Tokens/s": "211",
    "LatencyFirst Answer Chunk (s)": "0.56",
    "FirstAnswerToken (s)": "0.56",
    "P5First Chunk (s)": "0.21",
    "P25First Chunk (s)": "0.42",
    "P75First Chunk (s)": "0.60",
    "P95First Chunk (s)": "1.14",
    "TotalResponse (s)": "4.31",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Phi-4 Multimodal",
    "ContextWindow": "128k",
    "Creator": "Microsoft Azure",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "10E",
    "ArtificialAnalysisOmniscience Index": "--",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "--",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "--",
    "AA-LCR(LongContext Reasoning)": "--",
    "AA-OmniscienceAccuracy(Knowledge)": "--",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "--",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "32%",
    "LiveCodeBench(Coding)": "13%",
    "SciCode(Coding)": "11%",
    "IFBench(InstructionFollowing)": "--",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "--",
    "MMMU Pro(VisualReasoning)": "14%",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "17",
    "P5Tokens/s": "17",
    "P25Tokens/s": "17",
    "P75Tokens/s": "17",
    "P95Tokens/s": "18",
    "LatencyFirst Answer Chunk (s)": "0.33",
    "FirstAnswerToken (s)": "0.33",
    "P5First Chunk (s)": "0.30",
    "P25First Chunk (s)": "0.31",
    "P75First Chunk (s)": "0.35",
    "P95First Chunk (s)": "1.33",
    "TotalResponse (s)": "29.23",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Reka Flash 3",
    "ContextWindow": "128k",
    "Creator": "Reka AI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "10",
    "ArtificialAnalysisOmniscience Index": "\u221265",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "0%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "13%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "10%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "53%",
    "LiveCodeBench(Coding)": "44%",
    "SciCode(Coding)": "27%",
    "IFBench(InstructionFollowing)": "30%",
    "AIME 2025(CompetitionMath)": "34%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.35",
    "InputPriceUSD/1M Tokens": "$0.20",
    "OutputPriceUSD/1M Tokens": "$0.80",
    "MedianTokens/s": "51",
    "P5Tokens/s": "48",
    "P25Tokens/s": "50",
    "P75Tokens/s": "52",
    "P95Tokens/s": "54",
    "LatencyFirst Answer Chunk (s)": "1.32",
    "FirstAnswerToken (s)": "40.71",
    "P5First Chunk (s)": "1.05",
    "P25First Chunk (s)": "1.27",
    "P75First Chunk (s)": "1.35",
    "P95First Chunk (s)": "1.45",
    "TotalResponse (s)": "50.55",
    "ReasoningTime (s)": "39.39",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 VL 4B",
    "ContextWindow": "256k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "10",
    "ArtificialAnalysisOmniscience Index": "\u221277",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "23%",
    "AA-LCR(LongContext Reasoning)": "13%",
    "AA-OmniscienceAccuracy(Knowledge)": "10%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "3%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "37%",
    "LiveCodeBench(Coding)": "29%",
    "SciCode(Coding)": "14%",
    "IFBench(InstructionFollowing)": "32%",
    "AIME 2025(CompetitionMath)": "37%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "44%",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Olmo 3 7B Think",
    "ContextWindow": "66k",
    "Creator": "Allen Institute for AI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "9",
    "ArtificialAnalysisOmniscience Index": "\u221274",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "1%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "0%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "10%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "6%",
    "Humanity's LastExam(Reasoning & Knowledge)": "6%",
    "GPQA Diamond(ScientificReasoning)": "52%",
    "LiveCodeBench(Coding)": "62%",
    "SciCode(Coding)": "21%",
    "IFBench(InstructionFollowing)": "42%",
    "AIME 2025(CompetitionMath)": "71%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.14",
    "InputPriceUSD/1M Tokens": "$0.12",
    "OutputPriceUSD/1M Tokens": "$0.20",
    "MedianTokens/s": "167",
    "P5Tokens/s": "142",
    "P25Tokens/s": "165",
    "P75Tokens/s": "171",
    "P95Tokens/s": "181",
    "LatencyFirst Answer Chunk (s)": "0.36",
    "FirstAnswerToken (s)": "12.38",
    "P5First Chunk (s)": "0.28",
    "P25First Chunk (s)": "0.32",
    "P75First Chunk (s)": "0.39",
    "P95First Chunk (s)": "0.57",
    "TotalResponse (s)": "15.38",
    "ReasoningTime (s)": "12.01",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Jamba 1.7 Large",
    "ContextWindow": "256k",
    "Creator": "AI21 Labs",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "9",
    "ArtificialAnalysisOmniscience Index": "0",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "2%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "14%",
    "AA-LCR(LongContext Reasoning)": "17%",
    "AA-OmniscienceAccuracy(Knowledge)": "0%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "0%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "39%",
    "LiveCodeBench(Coding)": "18%",
    "SciCode(Coding)": "19%",
    "IFBench(InstructionFollowing)": "35%",
    "AIME 2025(CompetitionMath)": "2%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$3.50",
    "InputPriceUSD/1M Tokens": "$2.00",
    "OutputPriceUSD/1M Tokens": "$8.00",
    "MedianTokens/s": "48",
    "P5Tokens/s": "43",
    "P25Tokens/s": "45",
    "P75Tokens/s": "51",
    "P95Tokens/s": "57",
    "LatencyFirst Answer Chunk (s)": "0.86",
    "FirstAnswerToken (s)": "0.86",
    "P5First Chunk (s)": "0.63",
    "P25First Chunk (s)": "0.73",
    "P75First Chunk (s)": "0.96",
    "P95First Chunk (s)": "1.05",
    "TotalResponse (s)": "11.27",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Molmo 7B-D",
    "ContextWindow": "4k",
    "Creator": "Allen Institute for AI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "9E",
    "ArtificialAnalysisOmniscience Index": "--",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "0%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "--",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "--",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "24%",
    "LiveCodeBench(Coding)": "4%",
    "SciCode(Coding)": "4%",
    "IFBench(InstructionFollowing)": "20%",
    "AIME 2025(CompetitionMath)": "0%",
    "CritPt(PhysicsReasoning)": "--",
    "MMMU Pro(VisualReasoning)": "25%",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Ling-mini-2.0",
    "ContextWindow": "131k",
    "Creator": "InclusionAI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "9",
    "ArtificialAnalysisOmniscience Index": "0",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "1%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "13%",
    "AA-LCR(LongContext Reasoning)": "7%",
    "AA-OmniscienceAccuracy(Knowledge)": "0%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "7%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "56%",
    "LiveCodeBench(Coding)": "43%",
    "SciCode(Coding)": "14%",
    "IFBench(InstructionFollowing)": "24%",
    "AIME 2025(CompetitionMath)": "49%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.12",
    "InputPriceUSD/1M Tokens": "$0.07",
    "OutputPriceUSD/1M Tokens": "$0.28",
    "MedianTokens/s": "142",
    "P5Tokens/s": "135",
    "P25Tokens/s": "138",
    "P75Tokens/s": "153",
    "P95Tokens/s": "202",
    "LatencyFirst Answer Chunk (s)": "1.42",
    "FirstAnswerToken (s)": "1.42",
    "P5First Chunk (s)": "1.28",
    "P25First Chunk (s)": "1.33",
    "P75First Chunk (s)": "2.13",
    "P95First Chunk (s)": "3.16",
    "TotalResponse (s)": "4.95",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Gemma 3 12B",
    "ContextWindow": "128k",
    "Creator": "Google",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "9",
    "ArtificialAnalysisOmniscience Index": "\u221277",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "1%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "11%",
    "AA-LCR(LongContext Reasoning)": "7%",
    "AA-OmniscienceAccuracy(Knowledge)": "10%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "3%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "35%",
    "LiveCodeBench(Coding)": "14%",
    "SciCode(Coding)": "17%",
    "IFBench(InstructionFollowing)": "37%",
    "AIME 2025(CompetitionMath)": "18%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "38%",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "36",
    "P5Tokens/s": "32",
    "P25Tokens/s": "34",
    "P75Tokens/s": "39",
    "P95Tokens/s": "40",
    "LatencyFirst Answer Chunk (s)": "24.01",
    "FirstAnswerToken (s)": "24.01",
    "P5First Chunk (s)": "1.66",
    "P25First Chunk (s)": "8.14",
    "P75First Chunk (s)": "37.50",
    "P95First Chunk (s)": "48.74",
    "TotalResponse (s)": "37.91",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Gemma 3 270M",
    "ContextWindow": "32k",
    "Creator": "Google",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "8E",
    "ArtificialAnalysisOmniscience Index": "--",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "9%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "--",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "--",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "22%",
    "LiveCodeBench(Coding)": "0%",
    "SciCode(Coding)": "0%",
    "IFBench(InstructionFollowing)": "12%",
    "AIME 2025(CompetitionMath)": "2%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Exaone 4.0 1.2B",
    "ContextWindow": "64k",
    "Creator": "LG AI Research",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "8",
    "ArtificialAnalysisOmniscience Index": "\u221282",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "16%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "6%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "6%",
    "Humanity's LastExam(Reasoning & Knowledge)": "6%",
    "GPQA Diamond(ScientificReasoning)": "52%",
    "LiveCodeBench(Coding)": "52%",
    "SciCode(Coding)": "9%",
    "IFBench(InstructionFollowing)": "23%",
    "AIME 2025(CompetitionMath)": "50%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Olmo 3 7B",
    "ContextWindow": "66k",
    "Creator": "Allen Institute for AI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "8",
    "ArtificialAnalysisOmniscience Index": "\u221278",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "13%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "7%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "8%",
    "Humanity's LastExam(Reasoning & Knowledge)": "6%",
    "GPQA Diamond(ScientificReasoning)": "40%",
    "LiveCodeBench(Coding)": "27%",
    "SciCode(Coding)": "10%",
    "IFBench(InstructionFollowing)": "33%",
    "AIME 2025(CompetitionMath)": "41%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.13",
    "InputPriceUSD/1M Tokens": "$0.10",
    "OutputPriceUSD/1M Tokens": "$0.20",
    "MedianTokens/s": "37",
    "P5Tokens/s": "33",
    "P25Tokens/s": "36",
    "P75Tokens/s": "38",
    "P95Tokens/s": "40",
    "LatencyFirst Answer Chunk (s)": "0.40",
    "FirstAnswerToken (s)": "0.40",
    "P5First Chunk (s)": "0.31",
    "P25First Chunk (s)": "0.37",
    "P75First Chunk (s)": "0.44",
    "P95First Chunk (s)": "0.52",
    "TotalResponse (s)": "13.93",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "LFM2.5-1.2B-Thinking",
    "ContextWindow": "32k",
    "Creator": "Liquid AI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "8",
    "ArtificialAnalysisOmniscience Index": "\u221283",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "20%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "7%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "4%",
    "Humanity's LastExam(Reasoning & Knowledge)": "6%",
    "GPQA Diamond(ScientificReasoning)": "34%",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "4%",
    "IFBench(InstructionFollowing)": "42%",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Exaone 4.0 1.2B",
    "ContextWindow": "64k",
    "Creator": "LG AI Research",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "8",
    "ArtificialAnalysisOmniscience Index": "\u221283",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "21%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "5%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "8%",
    "Humanity's LastExam(Reasoning & Knowledge)": "6%",
    "GPQA Diamond(ScientificReasoning)": "42%",
    "LiveCodeBench(Coding)": "29%",
    "SciCode(Coding)": "7%",
    "IFBench(InstructionFollowing)": "25%",
    "AIME 2025(CompetitionMath)": "24%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Granite 4.0 H 1B",
    "ContextWindow": "128k",
    "Creator": "IBM",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "8",
    "ArtificialAnalysisOmniscience Index": "\u221274",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "20%",
    "AA-LCR(LongContext Reasoning)": "6%",
    "AA-OmniscienceAccuracy(Knowledge)": "5%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "16%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "26%",
    "LiveCodeBench(Coding)": "12%",
    "SciCode(Coding)": "8%",
    "IFBench(InstructionFollowing)": "26%",
    "AIME 2025(CompetitionMath)": "6%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "LFM2.5-1.2B-Instruct",
    "ContextWindow": "32k",
    "Creator": "Liquid AI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "8",
    "ArtificialAnalysisOmniscience Index": "\u221275",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "11%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "6%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "13%",
    "Humanity's LastExam(Reasoning & Knowledge)": "7%",
    "GPQA Diamond(ScientificReasoning)": "33%",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "2%",
    "IFBench(InstructionFollowing)": "44%",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 1.7B",
    "ContextWindow": "32k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "8",
    "ArtificialAnalysisOmniscience Index": "\u221278",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "26%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "8%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "5%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "36%",
    "LiveCodeBench(Coding)": "31%",
    "SciCode(Coding)": "4%",
    "IFBench(InstructionFollowing)": "27%",
    "AIME 2025(CompetitionMath)": "39%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.40",
    "InputPriceUSD/1M Tokens": "$0.11",
    "OutputPriceUSD/1M Tokens": "$1.26",
    "MedianTokens/s": "125",
    "P5Tokens/s": "120",
    "P25Tokens/s": "123",
    "P75Tokens/s": "126",
    "P95Tokens/s": "128",
    "LatencyFirst Answer Chunk (s)": "0.99",
    "FirstAnswerToken (s)": "17.02",
    "P5First Chunk (s)": "0.92",
    "P25First Chunk (s)": "0.94",
    "P75First Chunk (s)": "1.11",
    "P95First Chunk (s)": "1.32",
    "TotalResponse (s)": "21.03",
    "ReasoningTime (s)": "16.03",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "LFM2 2.6B",
    "ContextWindow": "33k",
    "Creator": "Liquid AI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "8",
    "ArtificialAnalysisOmniscience Index": "\u221255",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "1%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "14%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "5%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "37%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "31%",
    "LiveCodeBench(Coding)": "8%",
    "SciCode(Coding)": "3%",
    "IFBench(InstructionFollowing)": "20%",
    "AIME 2025(CompetitionMath)": "8%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Granite 4.0 Micro",
    "ContextWindow": "128k",
    "Creator": "IBM",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "8",
    "ArtificialAnalysisOmniscience Index": "\u221278",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "2%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "13%",
    "AA-LCR(LongContext Reasoning)": "4%",
    "AA-OmniscienceAccuracy(Knowledge)": "9%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "5%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "34%",
    "LiveCodeBench(Coding)": "18%",
    "SciCode(Coding)": "12%",
    "IFBench(InstructionFollowing)": "25%",
    "AIME 2025(CompetitionMath)": "6%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "DeepHermes 3 - Llama-3.1 8B",
    "ContextWindow": "128k",
    "Creator": "Nous Research",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "8E",
    "ArtificialAnalysisOmniscience Index": "--",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "--",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "--",
    "AA-LCR(LongContext Reasoning)": "--",
    "AA-OmniscienceAccuracy(Knowledge)": "--",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "--",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "27%",
    "LiveCodeBench(Coding)": "9%",
    "SciCode(Coding)": "9%",
    "IFBench(InstructionFollowing)": "--",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "--",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Jamba 1.7 Mini",
    "ContextWindow": "258k",
    "Creator": "AI21 Labs",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "7",
    "ArtificialAnalysisOmniscience Index": "\u22121",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "13%",
    "AA-LCR(LongContext Reasoning)": "13%",
    "AA-OmniscienceAccuracy(Knowledge)": "0%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "3%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "32%",
    "LiveCodeBench(Coding)": "6%",
    "SciCode(Coding)": "9%",
    "IFBench(InstructionFollowing)": "31%",
    "AIME 2025(CompetitionMath)": "0%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.25",
    "InputPriceUSD/1M Tokens": "$0.20",
    "OutputPriceUSD/1M Tokens": "$0.40",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Granite 4.0 1B",
    "ContextWindow": "128k",
    "Creator": "IBM",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "7",
    "ArtificialAnalysisOmniscience Index": "\u221283",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "23%",
    "AA-LCR(LongContext Reasoning)": "4%",
    "AA-OmniscienceAccuracy(Knowledge)": "6%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "6%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "28%",
    "LiveCodeBench(Coding)": "5%",
    "SciCode(Coding)": "9%",
    "IFBench(InstructionFollowing)": "21%",
    "AIME 2025(CompetitionMath)": "6%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "LFM2 8B A1B",
    "ContextWindow": "33k",
    "Creator": "Liquid AI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "7",
    "ArtificialAnalysisOmniscience Index": "\u221278",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "11%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "7%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "9%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "34%",
    "LiveCodeBench(Coding)": "15%",
    "SciCode(Coding)": "7%",
    "IFBench(InstructionFollowing)": "26%",
    "AIME 2025(CompetitionMath)": "25%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 1.7B",
    "ContextWindow": "32k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "7",
    "ArtificialAnalysisOmniscience Index": "\u221282",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "22%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "7%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "3%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "28%",
    "LiveCodeBench(Coding)": "13%",
    "SciCode(Coding)": "7%",
    "IFBench(InstructionFollowing)": "21%",
    "AIME 2025(CompetitionMath)": "7%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.19",
    "InputPriceUSD/1M Tokens": "$0.11",
    "OutputPriceUSD/1M Tokens": "$0.42",
    "MedianTokens/s": "116",
    "P5Tokens/s": "112",
    "P25Tokens/s": "113",
    "P75Tokens/s": "121",
    "P95Tokens/s": "123",
    "LatencyFirst Answer Chunk (s)": "1.01",
    "FirstAnswerToken (s)": "1.01",
    "P5First Chunk (s)": "0.91",
    "P25First Chunk (s)": "0.92",
    "P75First Chunk (s)": "1.17",
    "P95First Chunk (s)": "1.40",
    "TotalResponse (s)": "5.30",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Granite 4.0 350M",
    "ContextWindow": "33k",
    "Creator": "IBM",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "7",
    "ArtificialAnalysisOmniscience Index": "\u221264",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "13%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "4%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "30%",
    "Humanity's LastExam(Reasoning & Knowledge)": "6%",
    "GPQA Diamond(ScientificReasoning)": "26%",
    "LiveCodeBench(Coding)": "2%",
    "SciCode(Coding)": "1%",
    "IFBench(InstructionFollowing)": "16%",
    "AIME 2025(CompetitionMath)": "0%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 0.6B",
    "ContextWindow": "32k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "6",
    "ArtificialAnalysisOmniscience Index": "\u221282",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "21%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "6%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "7%",
    "Humanity's LastExam(Reasoning & Knowledge)": "6%",
    "GPQA Diamond(ScientificReasoning)": "24%",
    "LiveCodeBench(Coding)": "12%",
    "SciCode(Coding)": "3%",
    "IFBench(InstructionFollowing)": "23%",
    "AIME 2025(CompetitionMath)": "18%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.40",
    "InputPriceUSD/1M Tokens": "$0.11",
    "OutputPriceUSD/1M Tokens": "$1.26",
    "MedianTokens/s": "201",
    "P5Tokens/s": "189",
    "P25Tokens/s": "196",
    "P75Tokens/s": "204",
    "P95Tokens/s": "207",
    "LatencyFirst Answer Chunk (s)": "0.93",
    "FirstAnswerToken (s)": "10.87",
    "P5First Chunk (s)": "0.89",
    "P25First Chunk (s)": "0.91",
    "P75First Chunk (s)": "0.99",
    "P95First Chunk (s)": "1.26",
    "TotalResponse (s)": "13.36",
    "ReasoningTime (s)": "9.94",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Gemma 3 4B",
    "ContextWindow": "128k",
    "Creator": "Google",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "6",
    "ArtificialAnalysisOmniscience Index": "\u221284",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "1%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "5%",
    "AA-LCR(LongContext Reasoning)": "6%",
    "AA-OmniscienceAccuracy(Knowledge)": "7%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "2%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "29%",
    "LiveCodeBench(Coding)": "11%",
    "SciCode(Coding)": "7%",
    "IFBench(InstructionFollowing)": "28%",
    "AIME 2025(CompetitionMath)": "13%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "30%",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "35",
    "P5Tokens/s": "32",
    "P25Tokens/s": "34",
    "P75Tokens/s": "38",
    "P95Tokens/s": "42",
    "LatencyFirst Answer Chunk (s)": "0.98",
    "FirstAnswerToken (s)": "0.98",
    "P5First Chunk (s)": "0.79",
    "P25First Chunk (s)": "0.92",
    "P75First Chunk (s)": "1.04",
    "P95First Chunk (s)": "1.17",
    "TotalResponse (s)": "15.39",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Gemma 3n E4B",
    "ContextWindow": "32k",
    "Creator": "Google",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "6",
    "ArtificialAnalysisOmniscience Index": "\u221282",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "2%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "5%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "7%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "3%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "30%",
    "LiveCodeBench(Coding)": "15%",
    "SciCode(Coding)": "8%",
    "IFBench(InstructionFollowing)": "28%",
    "AIME 2025(CompetitionMath)": "14%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "26%",
    "BlendedUSD/1M Tokens": "$0.03",
    "InputPriceUSD/1M Tokens": "$0.02",
    "OutputPriceUSD/1M Tokens": "$0.04",
    "MedianTokens/s": "44",
    "P5Tokens/s": "33",
    "P25Tokens/s": "37",
    "P75Tokens/s": "48",
    "P95Tokens/s": "56",
    "LatencyFirst Answer Chunk (s)": "0.38",
    "FirstAnswerToken (s)": "0.38",
    "P5First Chunk (s)": "0.31",
    "P25First Chunk (s)": "0.34",
    "P75First Chunk (s)": "0.44",
    "P95First Chunk (s)": "0.86",
    "TotalResponse (s)": "11.81",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "LFM2.5-VL-1.6B",
    "ContextWindow": "32k",
    "Creator": "Liquid AI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "6",
    "ArtificialAnalysisOmniscience Index": "\u221286",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "9%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "5%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "4%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "29%",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "3%",
    "IFBench(InstructionFollowing)": "33%",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "27%",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Qwen3 0.6B",
    "ContextWindow": "32k",
    "Creator": "Alibaba",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "6",
    "ArtificialAnalysisOmniscience Index": "\u221287",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "15%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "4%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "5%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "23%",
    "LiveCodeBench(Coding)": "7%",
    "SciCode(Coding)": "4%",
    "IFBench(InstructionFollowing)": "22%",
    "AIME 2025(CompetitionMath)": "10%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.19",
    "InputPriceUSD/1M Tokens": "$0.11",
    "OutputPriceUSD/1M Tokens": "$0.42",
    "MedianTokens/s": "192",
    "P5Tokens/s": "176",
    "P25Tokens/s": "181",
    "P75Tokens/s": "197",
    "P95Tokens/s": "216",
    "LatencyFirst Answer Chunk (s)": "0.97",
    "FirstAnswerToken (s)": "0.97",
    "P5First Chunk (s)": "0.90",
    "P25First Chunk (s)": "0.91",
    "P75First Chunk (s)": "1.11",
    "P95First Chunk (s)": "1.24",
    "TotalResponse (s)": "3.57",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Gemma 3 1B",
    "ContextWindow": "32k",
    "Creator": "Google",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "5",
    "ArtificialAnalysisOmniscience Index": "\u221280",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "11%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "3%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "13%",
    "Humanity's LastExam(Reasoning & Knowledge)": "5%",
    "GPQA Diamond(ScientificReasoning)": "24%",
    "LiveCodeBench(Coding)": "2%",
    "SciCode(Coding)": "1%",
    "IFBench(InstructionFollowing)": "20%",
    "AIME 2025(CompetitionMath)": "3%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "37",
    "P5Tokens/s": "30",
    "P25Tokens/s": "34",
    "P75Tokens/s": "44",
    "P95Tokens/s": "60",
    "LatencyFirst Answer Chunk (s)": "0.51",
    "FirstAnswerToken (s)": "0.51",
    "P5First Chunk (s)": "0.46",
    "P25First Chunk (s)": "0.48",
    "P75First Chunk (s)": "0.53",
    "P95First Chunk (s)": "0.55",
    "TotalResponse (s)": "14.15",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Granite 4.0 H 350M",
    "ContextWindow": "33k",
    "Creator": "IBM",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "5",
    "ArtificialAnalysisOmniscience Index": "\u221289",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "15%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "3%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "4%",
    "Humanity's LastExam(Reasoning & Knowledge)": "6%",
    "GPQA Diamond(ScientificReasoning)": "26%",
    "LiveCodeBench(Coding)": "2%",
    "SciCode(Coding)": "2%",
    "IFBench(InstructionFollowing)": "18%",
    "AIME 2025(CompetitionMath)": "1%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Gemma 3n E2B",
    "ContextWindow": "32k",
    "Creator": "Google",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "5",
    "ArtificialAnalysisOmniscience Index": "\u221281",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "1%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "0%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "6%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "7%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "23%",
    "LiveCodeBench(Coding)": "10%",
    "SciCode(Coding)": "5%",
    "IFBench(InstructionFollowing)": "22%",
    "AIME 2025(CompetitionMath)": "10%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "38",
    "P5Tokens/s": "30",
    "P25Tokens/s": "34",
    "P75Tokens/s": "50",
    "P95Tokens/s": "61",
    "LatencyFirst Answer Chunk (s)": "0.39",
    "FirstAnswerToken (s)": "0.39",
    "P5First Chunk (s)": "0.23",
    "P25First Chunk (s)": "0.27",
    "P75First Chunk (s)": "0.48",
    "P95First Chunk (s)": "0.51",
    "TotalResponse (s)": "13.55",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "DeepSeek-OCR",
    "ContextWindow": "8k",
    "Creator": "DeepSeek",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "--",
    "ArtificialAnalysisOmniscience Index": "--",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "--",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "--",
    "AA-LCR(LongContext Reasoning)": "--",
    "AA-OmniscienceAccuracy(Knowledge)": "--",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "--",
    "Humanity's LastExam(Reasoning & Knowledge)": "--",
    "GPQA Diamond(ScientificReasoning)": "--",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "--",
    "IFBench(InstructionFollowing)": "--",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "--",
    "MMMU Pro(VisualReasoning)": "5%",
    "BlendedUSD/1M Tokens": "$0.05",
    "InputPriceUSD/1M Tokens": "$0.03",
    "OutputPriceUSD/1M Tokens": "$0.10",
    "MedianTokens/s": "327",
    "P5Tokens/s": "278",
    "P25Tokens/s": "309",
    "P75Tokens/s": "350",
    "P95Tokens/s": "368",
    "LatencyFirst Answer Chunk (s)": "0.18",
    "FirstAnswerToken (s)": "0.18",
    "P5First Chunk (s)": "0.12",
    "P25First Chunk (s)": "0.14",
    "P75First Chunk (s)": "0.69",
    "P95First Chunk (s)": "1.09",
    "TotalResponse (s)": "1.71",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Grok Voice Agent",
    "ContextWindow": "32k",
    "Creator": "xAI",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "--",
    "ArtificialAnalysisOmniscience Index": "--",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "--",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "--",
    "AA-LCR(LongContext Reasoning)": "--",
    "AA-OmniscienceAccuracy(Knowledge)": "--",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "--",
    "Humanity's LastExam(Reasoning & Knowledge)": "--",
    "GPQA Diamond(ScientificReasoning)": "--",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "--",
    "IFBench(InstructionFollowing)": "--",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "--",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Molmo2-8B",
    "ContextWindow": "37k",
    "Creator": "Allen Institute for AI",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "--",
    "ArtificialAnalysisOmniscience Index": "\u221269",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "0%",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "0%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "0%",
    "AA-LCR(LongContext Reasoning)": "0%",
    "AA-OmniscienceAccuracy(Knowledge)": "11%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "10%",
    "Humanity's LastExam(Reasoning & Knowledge)": "4%",
    "GPQA Diamond(ScientificReasoning)": "43%",
    "LiveCodeBench(Coding)": "--",
    "SciCode(Coding)": "13%",
    "IFBench(InstructionFollowing)": "27%",
    "AIME 2025(CompetitionMath)": "--",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "38%",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "216",
    "P5Tokens/s": "206",
    "P25Tokens/s": "210",
    "P75Tokens/s": "222",
    "P95Tokens/s": "236",
    "LatencyFirst Answer Chunk (s)": "0.38",
    "FirstAnswerToken (s)": "0.38",
    "P5First Chunk (s)": "0.27",
    "P25First Chunk (s)": "0.33",
    "P75First Chunk (s)": "0.45",
    "P95First Chunk (s)": "0.96",
    "TotalResponse (s)": "2.70",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Cogito v2.1",
    "ContextWindow": "128k",
    "Creator": "Deep Cogito",
    "License": "Open",
    "ArtificialAnalysisIntelligence Index": "--",
    "ArtificialAnalysisOmniscience Index": "\u221227",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "17%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "--",
    "AA-LCR(LongContext Reasoning)": "22%",
    "AA-OmniscienceAccuracy(Knowledge)": "29%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "21%",
    "Humanity's LastExam(Reasoning & Knowledge)": "11%",
    "GPQA Diamond(ScientificReasoning)": "77%",
    "LiveCodeBench(Coding)": "69%",
    "SciCode(Coding)": "41%",
    "IFBench(InstructionFollowing)": "46%",
    "AIME 2025(CompetitionMath)": "73%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$1.25",
    "InputPriceUSD/1M Tokens": "$1.25",
    "OutputPriceUSD/1M Tokens": "$1.25",
    "MedianTokens/s": "73",
    "P5Tokens/s": "72",
    "P25Tokens/s": "72",
    "P75Tokens/s": "75",
    "P95Tokens/s": "77",
    "LatencyFirst Answer Chunk (s)": "0.37",
    "FirstAnswerToken (s)": "27.67",
    "P5First Chunk (s)": "0.29",
    "P25First Chunk (s)": "0.32",
    "P75First Chunk (s)": "0.50",
    "P95First Chunk (s)": "0.58",
    "TotalResponse (s)": "34.50",
    "ReasoningTime (s)": "27.31",
    "FurtherAnalysis": "ModelProviders"
  },
  {
    "Model": "Mi:dm K 2.5 Pro Preview",
    "ContextWindow": "128k",
    "Creator": "Korea Telecom",
    "License": "Proprietary",
    "ArtificialAnalysisIntelligence Index": "--",
    "ArtificialAnalysisOmniscience Index": "\u221256",
    "GDPval-AA (AgenticReal-World WorkTasks, (ELO-500)/2000)": "--",
    "Terminal-BenchHard (AgenticCoding & Terminal Use)": "3%",
    "\ud835\udf0f\u00b2-BenchTelecom(Agentic Tool Use)": "49%",
    "AA-LCR(LongContext Reasoning)": "11%",
    "AA-OmniscienceAccuracy(Knowledge)": "19%",
    "AA-OmniscienceNon-Hallucination Rate(1 - Hallucination Rate)": "9%",
    "Humanity's LastExam(Reasoning & Knowledge)": "9%",
    "GPQA Diamond(ScientificReasoning)": "72%",
    "LiveCodeBench(Coding)": "58%",
    "SciCode(Coding)": "30%",
    "IFBench(InstructionFollowing)": "46%",
    "AIME 2025(CompetitionMath)": "79%",
    "CritPt(PhysicsReasoning)": "0%",
    "MMMU Pro(VisualReasoning)": "--",
    "BlendedUSD/1M Tokens": "$0.00",
    "InputPriceUSD/1M Tokens": "$0.00",
    "OutputPriceUSD/1M Tokens": "$0.00",
    "MedianTokens/s": "0",
    "P5Tokens/s": "0",
    "P25Tokens/s": "0",
    "P75Tokens/s": "0",
    "P95Tokens/s": "0",
    "LatencyFirst Answer Chunk (s)": "0.00",
    "FirstAnswerToken (s)": "0.00",
    "P5First Chunk (s)": "0.00",
    "P25First Chunk (s)": "0.00",
    "P75First Chunk (s)": "0.00",
    "P95First Chunk (s)": "0.00",
    "TotalResponse (s)": "0.00",
    "ReasoningTime (s)": "0.00",
    "FurtherAnalysis": "ModelProviders"
  }
]